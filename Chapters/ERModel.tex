\section{Different approaches of the same space}
As said in the title of the section there are different ways to approach the Erdos-Renyi model that we may call paradigms as they will give us the same kind of results but depending on the context, one might be much more convenient to use than the others.
\newline
Historically the first paper published on random graphs was from Erdos and Renyi in 1959 \cite{erdos59}, in which they give the following construction :
\begin{definition}
We call a random graph $\mathcal{G}_{n, M}$ having $n$ labelled vertices and $M$ edges. That is we choose at random ( with equal probability ) one of the $\binom{\binom{n}{2}}{M}$ possible graphs.
\end{definition}
One may observe that some changes in notations are made between this paraphrasing of the article of Erdos and Renyi, they are made in order to be more adapted with the modern study of random graphs. We will also adopt for the following $N = \binom{n}{2}$ to denote the total number of edges possible on $n$ labelled vertices.
\newline
We then arrive to our main model that has been the most extensively studied in the literature of random graphs, that is $\mathcal{G}_{n, p}$ on which the coin tosses are no longer fair, but the probability of drawing an edge is now $p$. And the coin tosses are still independent. Now if we denote by $e_G$ the number of edges of a graph $G$ on the vertex set $[n]$. We have :
\begin{align}
	\mathbb{P}(G) = p^{e_G}(1-p)^{N-e_G}
\end{align}
This model is called the binomial model. It is easily seen that this model is asymptotically equivalent to the first one if $Np$ is close to $M$ on several aspects.
\newline
The third model that we will investigate is on the form of a Markov process, see in Annex for a discussion on properties used here from Markov chains. 
At time 0 there is no edge and an edge is selected at random among all of the possible edges. 
At time $t$, the edge is chosen among all the edges not already present in the graph. We denote this process by $\{\mathcal{G}_{n, t}\}_t$, with $t$ the number of edges added. 
It is clear that this model is perfectly equivalent to the first model presented in the case $t = M$. This model was also introduced in 1959 by Erdos and Renyi and is usually refered to as the random graph process.
The advantage of this model is that it allows one to study properties on the verge of their realisations.
For instance, using this model Bollob\'as proved that a graph is fully connected, when the last connection made is between an isolated vertice and the giant component. But we will study this in the following.




\section{Connectivity}

One of the most fundamental structure of a graph will be it's number of components. Hence, the first question we will try to ask is how often a random graph in the Erdos-Renyi model is connected. It is essential to answer this question as many other questions might not make sense on a graph that is not connected ( for instance the diameter, the existence of hamiltonian paths or the stability number of the graph ). 
\newline
It is also an interesting first topic to have an insight of the kind of elegant results that arise from the study of random graphs. The main aim of this section will be to prove the following theorem in a didactic way as it is the first random graphs proof that we will study.
\begin{theorem}\label{th:connect}
Let $p = p(n) = \frac{log(n) + c}{n}$
\newline
Then $\lim_{n \to \infty} P(G \in \mathcal{G}_{n, p}\text{ is connected }) = e^{-e^{-c}}$ 
\end{theorem}
The proof of this theorem will be in two parts, first we will show that a graph will be connected if and only if there are no isolated vertices and then we will estimate the distribution that follows the number of isolated vertices.
\begin{theorem}\label{th:isolcomp}
	With $p = \frac{log(n) + c}{n}$
	 Almost every $G \in \mathcal{G}_{n, p}$ consists of a giant component and isolated vertices. 
\end{theorem}
\begin{proof}
This proof can be found in \cite{Spencer14} or \cite{Bollob01}.
\newline
During this proof we will consider the random value $X_k$ that counts the number of vertices that are in components of size $k$.
So, let's estimate the probability $P(X_2 > 0) = P(X_2 \geq 1 )$. In order to do so we will use the method of first moment.
\begin{align}
	\mathbb{P}(X_2 \geq 1) \leq \mathbb{E}(X_2) 	&= \binom{n}{2}\mathbb{P}(\text{"drawing an isolated edge"}) \\
						    	&= \binom{n}{2}p((1-p)^{n-1})^2 \\
							&\leq (\frac{ne}{2})^2p(e^{-p})^{2(n-2)} \\
						    	&= \mathcal{O}\left(n^2p \frac{ e^2 e^{-2p(n+1)} }{4}\right) \\
							&= \mathcal{O}(n^2p)
\end{align}
So it's sufficient that $p = o(n^{-2})$ in order to have almost surely no edges in G. This is clearly satisfied by the $p$ we use in the theorem.
\newline
However, this is not sufficient to prove that there is no isolated other than vertices. We will observe that there can't have any component of size larger than $\lceil \frac{n}{2} \rceil$ that is not the largest component in the graph.
Hence, we will study, the probability that there is any component of intermediary size that is not connected to the greatest component.
\begin{align}
	\mathbb{P}(X_k \geq 1) &\leq \mathbb{E}(X_k)\quad, \forall k \geq 3 \\
				&\leq \binom{n}{k} k^{k-2} q_k\\
				&\leq \binom{n}{k} k^{k-2} p^{k-1} ((1-p)^{n-k})^k
\end{align}
In the above, $q_k$ represents the probability that a spanning tree on $k$ vertices doesn't connect to the greatest connected components. A tree on $k$ vertices having $k-1$ edges, this means in terms of probability that it must have $k-1$ "success" and on each of the $k$ vertices $n-k$ failures. Which leads to the following line.
\newline 
Now we will try to have an upper bound of the RHS such that the sum on $k$ will converges to a $o(n^{-\delta})$ for some $\delta >0$.
\begin{align}
	\mathbb{P}(X_k \geq 1) 	
	            &\leq k^{-2}p^{-1}(\frac{ne}{k})^k k^{k} p^{k} e^{-p\frac{n}{2}(n-k))}\\
	            &\leq k^{-2}p^{-1}((\frac{ne}{k}) k p e^{-p\frac{n}{2}})^k\\
	            &\leq p^{-1}(nep e^{-p\frac{n}{2}})^k
\end{align}
If we denote the bracketed term by $A$, then
\begin{equation}
    A = \mathcal{O}(\log(n) n^{-\frac{1}{2}})
\end{equation}
Hence, $A$ goes to 0 without any condition on $k$, so we obtain
\begin{equation}
    \sum_{k=3}^{\lceil n/2\rceil} \mathbb{P}(X_k \geq 1)\leq p^{-1}\sum_{k=3}^{\lceil n/2\rceil} A^k = o(1)
\end{equation}
Which gives the fact that whether $\mathcal{G}_{n,p}$ is connected doesn't depend on the existence of isolated connected components of size 2 or more. So it only depends on the existence of isolated vertices.
An intuitive point of view on that is some isolated component emerges it is really likely to be eaten by the giant component, then the only components that have a chance of not being absorbed by the giant component are the isolated vertex.
So we have proved \ref{th:isolcomp}.
\end{proof}
The proof of \ref{th:connect} will be given in the part on branching processes.
\section{Existence of thresholds}\footnote{ The proofs and results from this section are from \cite{JLR} and \cite{Bollob01}}
One of the most surprising features on random graphs, which seems to have motivated Erd\H{o}s to publish his results from 1959 ( TODO : FIND THE REF THAT SAID THAT...), is the existence of thresholds. That is, for many graph properties, with a small variation on the number of edges ( in the ER model ) or on $p(n)$, the limiting probability would jump between 0 and 1.
This zone which produces great difference in limiting probability will be called a threshold.
It has been shown by Bollob\'as and Thomason \cite{Bollob87} that this is in fact not exclusive to random graphs, but true for all monotone properties on random subsets.
\begin{definition}
We will call a graph property a family of graphs that is closed under isomorphism.
\end{definition}
This means that a graph property is independent of the labelling and of the drawing of the graph.
We can refine properties in the following definition.
\begin{definition}
	A property is monotone\footnote{ A propery is monotone if it is either increasing or decreasing} increasing (resp. decreasing) if it's stable under the the addition (resp. removal) of an edge.
	A graph property $\mathcal{Q}$ is convex if when $ A,C \in \mathcal{Q}$ and $A\subseteq B\subseteq C$ then $B \in \mathcal{Q}$. 
\end{definition}

For instance, being connected or containing a specific subgraph are monotone increasing properties where as being planar or containing an isolated vertice are monotone decreasing. 
As an example of property that is neither monotone increasing or decreasing, we can think of being $k$-regular for some $k$ ( this means that all vertices are of degree $k$).
Having exactly $k$ isolated vertices is an example of a convex not monotone property.
\newline
Here is a theorem showing that monotone increasing properties make probability distributions on these properties also monotone increasing.
\begin{theorem}
	Suppose $\mathcal{Q}$ is a monotone increasing property and $0 \leq M_1 \leq M_2 \leq N$ and $0 \leq p_1 \leq p_2 \leq 1$.
	\newline
	Then
	$$\mathbb{P}_{M_1}(\mathcal{Q}) \leq  \mathbb{P}_{M_2}(\mathcal{Q}) \text{  and  } \mathbb{P}_{p_1}(\mathcal{Q}) \leq \mathbb{P}_{p_2}(\mathcal{Q})$$
\end{theorem}
\begin{proof}
	The first inequality is clear, as the only difference between the two spaces on which we evaluate the property $\mathcal{Q}$ is that on the RHS edges have been added, hence, the probability of realising a monotone increasing has been increased.
	\newline
	For the second inequality, let $p = \frac{p_2 - p_1}{1 - p_1}$. Let $G_1 \in \mathcal{G}_{n, p_1}, G_2 \in \mathcal{G}_{n, p}$
	\newline
	So if $G_2 = G_1 \cup G$ it's edges are chosen with probability $p_1 + p - p_1 p = p_2$. So $G_1$ is in $G_2$, the property being monotone increasing, we have $ \mathbb{P}_{p_1}(\mathcal{Q}) \leq \mathbb{P}_{p_2}(\mathcal{Q})$
\
\end{proof}
The following result follows from definition with $\mathcal{Q}$ a monotone increasing property
\begin{equation}
	\mathbb{P}(\mathcal{Q}) = \sum_{A \in \mathcal{Q}} p^{|A|}(1-p)^{N-|A|}
\end{equation}
However this result requires to know all of the elements in $\mathcal{Q}$ and as we are often interested with properties for very large $n$ this result won't be magical... However from the following lemmas it is very useful to obtain some results on the links between $\mathcal{G}_{n, p}$ mentionned in the introduction $\mathcal{G}_{n, M}$.
Indeed the following theorem shows that if we know quite accurately $\mathbb{P}_M(\mathcal{Q})$ for every $M$ close to $pN$ then we know $\mathbb{P}_p(\mathcal{Q})$ with a comparable accuracy. The converse being clearly false, for instance the property of containing M edges.

\begin{theorem}\label{th:linkMP}
	Suppose $\mathcal{Q}$ is any property and $0 < p = M/N< 1$ 
	\newline
	Then $\mathbb{P}_M(\mathcal{Q}) \leq 3 \sqrt{M}\mathbb{P}_p(\mathcal{Q})$
\end{theorem}
\begin{proof}
	Let $\mathcal{Q}$ be any property, then we will write $\mathcal{Q}$ as a partition based on the number of edges in each graph contained in $\mathcal{Q}$.
	\newline
	So we have
	$$\mathcal{Q} = \bigsqcup_{m=0}^{N} \mathcal{Q}_m \quad, \text{ with } \forall G \in \mathcal{Q}_m, e(G) = m$$
	$\mathbb{P}_m(\mathcal{Q}) = |\mathcal{Q}_m| \binom{N}{M}^{-1}$
	From this we can obtain, with $q = 1 - p$
	\begin{align*}
		\mathbb{P}_p(\mathcal{Q})	&= \sum_{A \in \mathcal{Q}} p^{|A|}q^{N-|A|}\\
						&= \sum_{m=0}^{N}\sum_{A \in \mathcal{Q}_m} p^{|A|}q^{N-|A|}\\
						&= \sum_{m=0}^{N}\sum_{A \in \mathcal{Q}_m} p^{m}q^{N-m}\\
						&= \sum_{m=0}^{N}|\mathcal{Q}_m|p^{m}q^{N-m}\\
						&= \sum_{m=0}^N p^mq^{N-m}\binom{N}{M}\mathbb{P}_m(\mathcal{Q}) \\
						&\geq \binom{N}{M}p^Mq^{N-M}\mathbb{P}_M(\mathcal{Q}) \\
						&\geq \mathbb{P}_M(\mathcal{Q})(e^{\frac{1}{6M}}\sqrt{2\pi p q N})^{-1}\\
	\end{align*}
	So we have (TODO : PROVE LAST INEQ IN ANNEX OR ADD REF )
	\begin{equation}
		\mathbb{P}_M(\mathcal{Q}) \leq  \mathbb{P}_p(\mathcal{Q})e^{\frac{1}{6M}}\sqrt{2\pi q M}
	\end{equation}
	Observing that $q \leq 1$ and $\sqrt{2\pi}e^{\frac{1}{6}} \approx 2.961... < 3$ the proof is complete.
\end{proof}
The previous section was about connectivity in $\mathcal{G}_{n,p}$, in this section we have seen that connectivity can be characterized as a monotone increasing property.
Also it was observed that the function $p$ was somehow best possible, by that we mean that modifying it slightly would impply to only have a zero-one law. 
We call such a function $p$ a threshold ( in that case for the connectivity ). 
\newline

More formally, let $\mathcal{Q}$ a monotone increasing property,  in $\mathcal{G}_{n, p}$, we call $\hat{p} = \hat{p}(n)$ a threshold if
\begin{align}
	\mathbb{P}(\mathcal{G}_{n,p} \in \mathcal{Q}) \rightarrow \left\{\begin{array}{rl}
										0 & \text{if } p \ll \hat{p},\\
										1 & \text{if } p \gg \hat{p}.
									 \end{array}
									\right.
\end{align}

Analogously, in $\mathcal{G}_{n, M}$, we call $\hat{M} = \hat{M}(n)$ a threshold if
\begin{align}
	\mathbb{P}(\mathcal{G}_{n,M} \in \mathcal{Q}) \rightarrow \left\{\begin{array}{rl}
										0 & \text{if } M \ll \hat{M},\\
										1 & \text{if } M \gg \hat{M}.
									 \end{array}
									\right.
\end{align}
In fact, thresholds are unique with respect to the multiplication by a scalar. So for the following, we should denote a threshold for a property as the threshold.


\section{The stability number}\footnote{This section is from \cite{Bondy08}}
Another property of graphs that one might be interested to study is the stability number. The stability number of a graph is the size of the largest set of vertices we can choose in a graph such that no two vertices are adjacent. 
One of the reasons that makes this an interesting property to study is that it is linked to one of the most fundamental property of graphs, $\chi (G)$, the chromatic number. Indeed, the chromatic number being the smallest number of colours that make a proper colouring of a graph. 
That means a colouring on which there are no two adjacent vertices of the same color. 
We are then led to a similar question than finding the stability number of a graph.
If we denote by $\alpha(G)$ the stability number of a graph, we have the simple lower bound
\begin{equation}
	\chi(G) \geq \frac{n}{\alpha(H)}
\end{equation}
Before giving a lower bound on the stability number of a graph it might be interesting to denote that the notion of stable set is dual to the notion of clique and is analogous to the notion of perfect matching that concerns the edges.
Although the following theorem will give a bound that is quite tight in $\mathcal{G}_{n, p}$ the problem of finding the actual maximum stable set of a graph is a $NP$-hard problem.
\begin{theorem}
	The stability number of a graph in $\mathcal{G}_{n, p}$, is at most $\lceil 2p^{-1}\log(n)\rceil$.
\end{theorem}
\begin{proof}
	Let $G \in \mathcal{G}_{n,p}$ and $S \subseteq V$ such that $V$ contains $k+1$ vertices.
	Then we have 
	\begin{equation}
		\mathcal{P}(\text{"S is a stable set""}) = (1-p)^{\binom{k+1}{2}  }
	\end{equation}
	as none of the $\binom{k+1}{2}$ possible edges must be selected.
	\newline
	Let's define our random values as follow, $X_S = \mathbb{1}(\text{" $S$ is a stable set "})$ and
	\begin{equation}
		X_{k+1} = \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} X_S
	\end{equation}
	the random variable counting the number of stable sets of size $k+1$, so what we are investigating here is the rank $\alpha$ such that $X_k = 0, \forall k > \alpha$. Such an $\alpha$ would then be maximal stability number.
	\begin{align}
		\mathbb{E}X_{k+1} &= \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} \mathbb{E} X_S 	
				  = \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} \mathbb{P}(X_S = 1)	\\		
		&= \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} (1-p)^{\binom{k+1}{2}  }		
		= (1-p)^{\binom{k+1}{2}} \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} 1 	\\	
		&= \binom{n}{k+1} (1-p)^{\binom{k+1}{2}} 
	\end{align}
	Now we will use the common inequalities $  \binom{n}{k+1} \leq \frac{n^{k+1}}{(k+1)!} $ and $(1-p) \leq e^{-p}$. 
	And we have
	\begin{align}
		\mathbb{E}X_{k+1} 	&\leq  \frac{n^{k+1}}{(k+1)!} e^{-p \binom{k+1}{2}} = \frac{n^{k+1}}{(k+1)!} e^{-p \frac{k(k+1)}{2}}\\
									     &\leq  \frac{(ne^{-p\frac{k}{2}})^{k+1}}{(k+1)!}
	\end{align}
	So, if we consider $k = \lceil 2p^{-1}\log(n)\rceil \leq 2p^{-1}\log(n)$ we have that $ ne^{-p\frac{k}{2}} \leq 1$.
	We finally obtain 
	\begin{equation}
		\mathbb{E} X_{k+1} \longrightarrow_{n \to \infty} 0
	\end{equation}
	And then $X_{k+1} = 0$ almost surely which proves the theorem.

\end{proof}

\section{The diameter}\footnote{This section uses \cite{Bollob01}}

\begin{definition}
	The diameter of a graph is the greatest distance between any pair of vertice. We denote it by diam($G$) and say it is equal to $\infty$ if the graph is not connected.
\end{definition}
It is quite easy to understand that the diameter is a value that is a great importance particularly in applied systems. For instance, the small world phenomena is quite notorious ( and will be discussed later in this report ) but we can also think of optimization problems in which the fact that two points are far appart might be of great consequences.
This section won't be focused on real world applications of the diameter because as we will see we are not studying the right model for this. 
Hence we will first discuss some graph theoretic problems and results on the diameter and after giving the main theorem on the diameter we will demonstrate it through several technical lemmas, some of which will be admitted. Finally some corollary will be obtained from the theorem.
\newline
One of the challenging questions in graph theory is estimating the following function
\begin{equation}
	n(D, \Delta) = max\{ |G|, \text{diam}(G) \leq D, \Delta(G) \leq \Delta\}
\end{equation}
$n$ is the function that for a fixed diameter and a fixed maximal degree, gives the graph with a maximal number of verifies both conditions.
\newline
For instance if we take $\Delta = 2$ we obtain easily by construction that a graph that maximizes the number of vertices with a diameter $D$ is a $(2D+1)$-cycle. Hence, 
\begin{equation}
	n(D, 2) = 2D + 1, \forall D \in \mathbb{N}
\end{equation}
But it is in fact very hard to obtain such a formula for other values of $D$ or $\Delta$.
\newline
If we take a graph of max degree $\Delta$ we observe that there are at most $\Delta (\Delta - 1)^{k-1}$ vertices at distance $k$ from a chosen vertex. It is easily to be convinced of this simply by drawing such a graph. 
From this very simple construction we can obtain the following upped bound
\begin{equation}
	n(D, \Delta) \leq 1 + \Delta \sum_{j=1}^{D}(\Delta - 1)^{j-1} = \frac{\Delta(\Delta - 1)^D - 2}{\Delta - 2} = n_0(D, \Delta)
\end{equation}
$n_0$ is called the Moore bound and a graph for which the Moore bound is best possible is called a Moore graph. 
As Donald Knuth would have suspected, the Petersen's graph is such a graph with parameter $D= 2$ and $\Delta = 3$.
\newline
Now let's give some functions that will allow us to make a study of the diameter in random graphs. If we choose $x$ a vertex in a graph, then we define $\Gamma_k(x)$ as the set of vertices at distance $k$ from $x$. And from this we will define $N_k(x)$ as the set of vertices of distance less than or equal to $k$. 
Formally, we have
\begin{equation}
	\Gamma_k(x) = \{v : d(x, v) = k \}
\end{equation}
\begin{equation}
	N_k(x) = \bigcup_{i=1}^k \Gamma_i(x)
\end{equation}
And we can link those with the diameter using, diam$(G) \leq d $ if and only if $N_d(x) = V(G), \forall x$.
\newline
Similarly diam$(G) \geq d $ if and only if $\exists y, N_{d-1}(y) \neq V(G)$.
\newline
Now we will define the probabilty function that we will use in the following of this section as 
\begin{equation} \label{eq:pdiam}
	p^d n^{d-1} = \log(\frac{n^2}{c}) \quad, \text{  for some } c >0
\end{equation}
and as a teaser the following abbreviated version of the theorem we will properly give and prove later.
\begin{theorem}
	$\mathbb{P}(d \leq \text{diam}(\mathcal{G}_{n, p}) \leq d + 1) \longrightarrow_{n \to \infty} 1$
\end{theorem}
This (inexact )theorem that is very strong states that all random graphs in $\mathcal{G}_{n, p}$ have (nearly exactly) the same diameter.
In the actual theorem that we will prove we will be able to characterize exactly the probability of obtaining either $d$ or $d+1$ as a function of the parameter $c$ in \label{eq:diam}
\newline
Before proving such a theorem we will need some technical lemmas and assumptions. So we will give here some equations for reference later.
So we will assume that $0 < p < 1$ and $d=d(n)$ is a natural number.
Also 
\begin{equation}
	p \frac{\log(n)}{n}^{-1} \longrightarrow_{n \to \infty} \infty
\end{equation}
that should not be too surprising as the threshold for connectivity is $\frac{\log(n) + c}{n}$ as shown before.
We may also consider that $p = o(n^{-\frac{1}{2} +\epsilon}), \forall \epsilon > 0$ as it can be shown that otherwise the diameter of $\mathcal{G}_{n,p}$ is lower or equal than 2 ( TODO : CHECK THIS )
One might be interested in what $p(n, d, c)$ or $d(n , p, c)$ look like so here they are
\begin{equation}
	p=n^{\frac{1}{d} - 1}(\log(\frac{n^2}{c})^\frac{1}{d}
\end{equation}
\begin{align}
	d &= \frac{1}{\log(pn)}(\log(n) + \log \log (n) + log(2) + \mathcal{O}(\frac{1}{\log(n)}))\\
	  &= \mathcal{O}((1+o(1))\frac{\log(n)}{\log\log(n)})
\end{align}
And finally
\begin{equation}
	p(pn)^{d-2} = o(1)
\end{equation}
The first lemma that we present here gives a tail inequality of $\Gamma_k(x)$ conditionnaly on some space that we will now define. $\Omega_k \subseteq \mathcal{G}_{n, p}$ is a set of graphs with $a = |\Gamma_{k-1}(x)|$ and $b = |N_{k-1}(x)|$ that satisfy
\begin{align}
 \left\{\begin{array}{rl}
		 \frac{1}{2}(pn)^{k-1} \leq a &\leq \frac{1}{2}(pn)^{k-1} \\
		 			    b &\leq 2(pn)^{k-1}
	 \end{array}
	\right.
\end{align}
\begin{lemma}
	Let $x$ be a fixed vertex.
	\newline
	$1 \leq k = k(n)  \leq d-1$
	\newline
	And $ K = K(n)$ that satisfy $ 6 \leq K \leq \frac{1}{12} \sqrt{pn\frac{1}{\log[n}}$
	We also define
	\begin{equation}
		\alpha_k = K\sqrt{\frac{\log{n}}{(pn)^k}} , \quad \beta_k = p(pn)^{k-1}, \quad \gamma_k = 2\frac{(pn)^{k-1}}{n} = \frac{2\beta_k}{pn}
	\end{equation}

	Then
	\begin{equation}
		\mathbb{P}(|\gamma_k(x) - apn| \geq (\alpha_k + \beta_k + \gamma_k)apn \quad|\quad \Omega_k) \leq n^{-\frac{K^2}{9}}
	\end{equation}
\end{lemma}

\begin{proof}
	Knowing that we are in the space $\Omega_k$, then the probability that some vertex $y$ is at distance $k$, that is the probability that $y$ is not in $N_{k-1}(x)$ but is connected to $\Gamma_{k-1}(x)$ is 
	\begin{equation}
		p^a = 1 - (1-p)^a
	\end{equation}
	Hence, the random value $|\Gamma_k(x)|$ follows a binomial on $n-b$ elements, with a propability $p_a$.
	In the following inequalities we will assume that $n$ is large enough, this will allow us to do certain inequalities that are not really precise but asymptotically satisfying. Also to make it more easy for the reader we consider that all of these inequalities take place in $\Omega_k$ without writing it explicitly.
	\begin{align}
		|\gamma_k(x) - apn| &\geq (\alpha_k + \beta_k + \gamma_k)apn \\
		 &\geq (\alpha_k + \beta_k)apn + \gamma_k apn \\
		 &\geq (\alpha_k + \beta_k)apn + ap(n-n_k) \\ 
		 &\geq (\alpha_k + \beta_k + 1 - \frac{n_k}{n})apn \\ 
		 &\geq (\alpha_k + \beta_k)apn \geq (\alpha_k + \beta_k)apn_k\\ 
	\end{align}
	From this first sequence of inequalities we managed to remove $\gamma_k$ and we used some $n_k$ that is less than $n$.
	The point in evaluating these inequalities is that we now have
	\begin{align}
		\mathbb{P}(|\gamma_k(x) - apn| \geq (\alpha_k &+ \beta_k + \gamma_k)apn \quad|\quad \Omega_k) \\&\leq \mathbb{P}(|\gamma_k(x) - apn_k| \geq (\alpha_k + \beta_k )apn_k \quad|\quad \Omega_k)
	\end{align}
	Now using $ap -p_a \leq \beta_k ap$ and the triangular inequality (TODO : CHECK TRI INEQ ) we have
	\begin{align}
		&\leq \mathbb{P}(|\gamma_k(x) - p_an_k| \geq \alpha_k apn_k \quad|\quad \Omega_k)\\
		&\leq \mathbb{P}(|\gamma_k(x) - p_an_k| \geq \alpha_k p_an_k \quad|\quad \Omega_k)\\
	\end{align}
	And using Theorem 1.7 from Bollob\'as we have
	\begin{align}
		&\leq \frac{1}{\sqrt{\alpha_k^2p_an_k}}\exp(-\frac{1}{3}\alpha_k^2p_an_k) \\
		&\leq \exp(-\frac{1}{3}\alpha_k^2p_an_k)
	\end{align}
	And using $p_a \geq pa(1-\frac{pa}{2})$ , $a \geq \frac{1}{2}(pn)^{k-1}$ and using $n_k = n-b$ we obtain $p_an_k > \frac{(pn)^k}{3}$ that we insert in the previous inequality that will give us the result expected.
	\begin{align}
		&\leq \exp(-\alpha_k^2\frac{(pn)^k}{9}) = n^{-\frac{K^2}{9}}
	\end{align}
\end{proof}
We will now prove another lemma
\begin{lemma}
	Let $K > 12$ a constant and $\alpha_k, \beta_k, \gamma_k, k=1,...,d-1$ as before.
	\newline
	Set 
	\begin{equation}
		\delta_k = \exp(2\sum_{l=1}^{k}(\alpha_l + \beta_l +\gamma_l)) - 1
	\end{equation}
	Then if $n$ is sufficiently large, with probability at least $1-n^{-K-2}$ for every vertex $x$ and every natural number $k, 1\leq k \leq d-1$ we have
	\begin{equation}
		||\Gamma_k(x)| - (pn)^k| \leq \delta_k(pn)^k
	\end{equation}
\end{lemma}

\begin{proof}
	As $\delta_{d-1} \longrightarrow_{n\to\infty} 0$ we may assume that $\delta_{d-1} < \frac{1}{4}$
	For a fixed vertex $x$, we denote by $\Omega_k^*$ the set of graph for which 
	\begin{equation}
		||\Gamma_l(x) - (pn)^l| \leq \delta_l(pn)^l \quad , 0 \leq l \leq k
	\end{equation}
	And it is easy to verify that it is decreasing. Anf if one replaces $|\Gamma_l(x)|$ by $a$ it is clear that we have
	\begin{equation}
		\Omega_k^* \subseteq \Omega_{k-1}^* \subseteq \Omega_k
	\end{equation}
	Now, simply applying Bayes formula ( for the complementary ) we have
	\begin{equation}\label{eq:bayes}
		1 - \mathbb{P}(\Omega_k^*)= 1-\mathbb{P}(\Omega_{k-1}^*)
		+ \mathbb{P}(\Omega_{k-1}^*)\mathbb{P}(||\Gamma_k(x)| - (pn)^k| \leq \delta_k(pn)^k | \Omega_{k-1}^*)
	\end{equation}
	If $G \in \Omega_{k-1}^*$ and $|a| = |\Gamma_{k-1}(x)|$ and by applying the definition of belonging to $\Omega_{k-1}^*$ and multiplying both sides by $pn$ we have 
	\begin{equation}
		|(pn)^k - apn| \leq \delta_{k-1}(pn)^k
	\end{equation}
	And we obtain using the second triangular inequality
	\begin{align}
		\mathbb{P}(\not\Omega_k^* | \Omega_{k-1}^*) 
		&\leq \mathbb{P}(\Omega_{k-1}^*)^{-1}\mathbb{P}(||\gamma_k(x)| - apn| \geq (\delta_k - \delta_{k-1})(pn)^k | \Omega_k)\\
		&\leq (1 - 2(k-1)n^{-\frac{K^2}{9}})^{-1}\mathbb{P}(||\Gamma_k(x)| - apn| \geq 2(\alpha_k + \beta_k + \gamma_k)(pn)^k | \Omega_k)
	\end{align}
	The last inequality being obtained from the hypothesis of induction and using $(1+x) \leq \exp(x)$. Now using the fact that $apn \leq \frac{3}{2} (pn)^k$ we have
	\begin{align}
		&\leq 2\mathbb{P}(||\Gamma_k(x)| - apn| \geq (\alpha_k + \beta_k + \gamma_k)apn | \Omega_k)\\
		&\leq 2n^{-\frac{K^2}{9}}
	\end{align}
	If we combine the last inequality that is obtained applying the previous lemma with \ref{eq:bayes} then we have the result.
\end{proof}
\begin{theorem}
	Using all previous definitions on $p$ and $d$, in $\mathcal{G}_{n, p}$ we have
	\begin{equation}
		\mathbb{P}(\text{diam}(G) = d) \longrightarrow e^{-\frac{c}{2}}
	\end{equation}	
	\begin{equation}
		\mathbb{P}(\text{diam}(G) = d + 1) \longrightarrow 1 - e^{-\frac{c}{2}}
	\end{equation}
\end{theorem}
\begin{definition}
	For $x$ and $y$ two vertices of $G$, we say that $x$ and $y$ are remote if $y \not\in N_d(x)$.
\end{definition}
\begin{proof}
	TODO : CHECK THE FOLLOWING EQ
	\begin{equation}
		\mathbb{P}(|N_{d-1}(x)| < \frac{5}{6}(pn)^{d-1}) < n^{-4}
	\end{equation}
	Now we estimate the probability that a vertex $y$ is joined to no vertex in a set $W$ with $|W| \geq \frac{5}{6}(pn)^{d-1}$
	\begin{equation}
	(1-p)^{|W|} \leq \exp(-|W|) \leq \exp(-p|W|) = \exp(-\frac{5}{6}\log(\frac{n^2}{c})) = c^{\frac{5}{6}}n^{-\frac{5}{3}}
	\end{equation}

	Hence, if $x, y, z$ are vertices we have
	\begin{align}
		\mathbb{P}(x&\text{ is remote from $y$ and $z$ }) 						\\	
		&\leq \mathbb{P}(|N_{d-1}(x)|\leq \frac{5}{6} (pn)^{d-1}) 					\\
		&+\mathbb{P}(\{y, z\} \cap N_{d-1}(x) = \emptyset | |N_{d-1}(x)| \geq \frac{5}{6}(pn)^{d-1} )	\\
		&\leq \mathbb{P}(|N_{d-1}(x)|\leq \frac{5}{6} (pn)^{d-1}) 					\\				
		&+(\mathbb{P}(\text{$y$ is not joined to } W = N_{d-1}(x) | W \geq \frac{5}{6} (pn)^{d-1}))^2	\\
		&\leq n^{-4} + c^{\frac{5}{3}}n^{-\frac{10}{3}}							\\
		&\leq n^{-3}n^{-\frac{1}{4}}
	\end{align}
	So, we obtain
	\begin{align}
		\mathbb{P}(\mathcal{G}_{n, p} &\text{ contains two remote vertices pairs sharing a vertex })	\\
			&\leq \sum_x\sum_{(y, z)}\mathbb{P}(\text{ $x$ is remote from $y$ and $z$ })		\\
			&\leq \sum_x \binom{n}{2}n^{-3-\frac{1}{4}} = n\binom{n}{2}n^{-3-\frac{1}{4}}		\\
			&\leq n^{-\frac{1}{4}}
	\end{align}
	The following lemma can be obtained quite easily simply by construction.
	\begin{lemma}
		From $r$ disjoint pair of vertices, there are $2^r$ $r$-tuples of vertices meeting each pair.
	\end{lemma}
	TODO : CHECK FOLLOWING LEMMA
	\begin{lemma}
		The $r$-th factorial moment of a random value is within $o(1)$ of the expected number of ordered $r$-tuples of disjoint remote pairs.
	\end{lemma}
	If we denote by $\mathbb{F}_r$ the probability that a fixed $r$-tuple consists of vertices remote from each other.
	Then with $X$ the random value that counts the number of remote pair of vertices.
	\begin{equation}
		\mathbb{E}_r(X) = \frac{n!}{(n-r)!}2\mathbb{F}_r(1+o(1)) + o(1)
	\end{equation}
	In order to shorten the proof, we admit the following lemma that requires a quite long and technical proof. The proof can be found in \ref{Bollob81}.
	\begin{lemma}
		With probability at least $1-n^{-K}$
		\begin{equation}
			(1-n^{-K})Q_r \leq \mathbb{F}_r \leq (1-n^{-K}) Q_r + n^{-K}
		\end{equation}
		With $Q_r = (\frac{c}{n})^r(1+o(1))$
	\end{lemma}

	
	\begin{equation}
		\mathbb{F}_r = (\frac{c}{n})^r(1+o(1))
	\end{equation}
	And we can obtain the asymptotical estimate
	\begin{equation}
		\mathbb{E}_r(X) = n^r2^{-r}(\frac{c}{n})^r(1+o(1)) + o(1)
	\end{equation}
	Using the following theorem ( TODO : PROVE IT SOMEWHERE )
	\begin{theorem}
		$\lim_{n\to\infty} \mathbb{E}_r(X_n) - \lambda^r = 0$
			\newline
			$\implies X \longrightarrow_d \mathcal{P}_{\lambda}$
	\end{theorem}
	Then we have that $X$ converges in distribution to Poisson law of parameter $\frac{c}{2}$.
	\newline
	We can then obtain 
	\begin{equation}\label{eq:diamcv}
		\mathbb{P}(X=0) = \mathbb{P}(\text{diam}(G) \leq d) \longrightarrow e^{-\frac{c}{2}}
	\end{equation}
	that proves the first part of the theorem. We can also prove the case with $d=1$ that gives
	\begin{equation}
		\mathbb{P}(\text{diam}(G) \leq 1) = \mathbb{P}(G = K_n) = p^{\binom{n}{2}} \longrightarrow 0
	\end{equation}
	And to finish the proof we need to consider the two following cases
	\begin{equation}
		p^{d-1}n^{d-2} = \log(\frac{n^2}{c_1(n)})
	\end{equation}
	And in that case we have $c_1 \longrightarrow \infty$.
	\begin{equation}
		\mathbb{P}(\text{diam}(G) \leq d+1) = e^{-\frac{c_1}{2}} \longrightarrow 0
	\end{equation}
	If we now consider
	\begin{equation}
		p^{d+1}n^d = \log(\frac{n^2}{c_2(n)})
	\end{equation}
	we obtain $c_2 \longrightarrow 0$ which gives
	\begin{equation}
		\mathbb{P}(\text{diam}(G) \leq d+1) = e^{-\frac{c_2}{2}} \longrightarrow 1
	\end{equation}
	Finally, if we combine the last result with \ref{eq:diamcv} it is clear that the proof if complete.
\end{proof}
	To end this section we present two theorems that are much easier to prove with a similar flavor to the previous one.
	\begin{theorem}\label{th:diam2}
	Suppose 
	\begin{equation}
		i) \quad p^2n - 2\log(n) \longrightarrow \infty
	\end{equation}
	\begin{equation}
		ii) \quad n^2(1-p) \longrightarrow \infty
	\end{equation}
	Then almost every graph in $\mathcal{G}_p$ has diameter 2.
\end{theorem}

\begin{proof}
	\begin{equation}
		\mathbb{P}(\text{dist}(x, y) > 2) = (1-p^2)^{n-2}
	\end{equation}
	as it is the probability that two vertices $x$ and $y$ are not connected by a path of length two.
	\newline
	Then using the the first moment method.
	\begin{align}
		\mathbb{E}&(\text{ pairs not connected by a path of length 2})	\\
				&= \binom{n}{2}(1-p^2)^{n-2} \geq \mathbb{P}(\text{diam}(G) > 2)
	\end{align}
	As from $i)$ the LHS converges to zero, we are almost sure that no graph is of diameter more than 2 and then we just have to prove that there is almost no graph of diameter 1.
	A graph of diameter 1 being a complete graph we have
	\begin{align}
		\mathbb{P}(\text{diam}(G) = 1) = \mathbb{P}(G = K_n) 	
				= p^{\binom{n}{2}} &\longrightarrow 0				\\
		\text{iff  } \log p^{\binom{n}{2}} &\longrightarrow -\infty			\\
			   \text{iff  } n^2 \log p &\longrightarrow -\infty			\\
			    \text{iff  } n^2 (1-p) &\longrightarrow +\infty			
	\end{align}	
\end{proof}
And simply if we remember a previous corollary linking $\mathcal{G}_{n,p}$ and $\mathcal{G}_{n,M}$ we obtain the following corollary

\begin{corollary}
	If $M=M(n) < \binom{n}{2}$ satisfies 
	\begin{equation}
		\frac{2M^2}{n^3} - \log(n) \longrightarrow \infty
	\end{equation}
	Then almost every graph in $\mathcal{G}_{n, M}$ is of diameter 2.
\end{corollary}
\begin{proof}
	The first condition makes that we are not studying a complete graph then the diameter is not 1.
	And we have the proof simply using \ref{th:linkMP} with $pN = M$ and combining it with the previous theorem \ref{th:diam2} we have the result.
\end{proof}

