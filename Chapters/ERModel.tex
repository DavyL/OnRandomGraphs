\section{Different approaches of the same space}
As said in the title of the section there are different ways to approach the Erd\H{o}s-Renyi model that we may call paradigms as they will give us the same kind of results but depending on the context, one might be much more convenient to use than the others.
\newline
The first paper published on random graphs that gained notoriety was from Erd\H{o}s and Renyi in 1959 \cite{erdos59}, in which they give the following construction:
\begin{definition}
We define by $\mathcal{G}_{n, M}$ a uniformly distributed graph among the graphs with $n$ given vertices and exactly $M$ edges.
\end{definition}
One may observe that changes in notations are made compared to the articles from Erd\"os and R\'enyi in order to be more adapted with the modern study of random graphs.
We will also adopt for the following $N = \binom{n}{2}$ to denote the total number of possible edges on $n$ labelled vertices.
From the previous definition, the probability of obtaining a chosen graph in with $n$ vertices and $M$ edges in $\mathcal{G}_{n, M}$ is $1/\binom{N}{M}$.
\newline
We then arrive at our main model that has been the most extensively studied in the literature of random graphs, that is $\mathcal{G}_{n, p}$ on which each of the $N$ possible edges is taken with probability $p \in [0, 1]$, independently of the others.
This model was first introduced by Gilbert in 1959, see \cite{Gilbert59}\footnote{It is interesting to note that Gilbert's paper who was at the time working at Bell's laboratories formulated the question of connectivity based on real life phone networks.}.
Now if we denote by $e_G$ the number of edges of a graph $G$ on the vertex set $[n]$, we have :
\begin{align}
	\mathbb{P}(G) = p^{e_G}(1-p)^{N-e_G}
\end{align}
This model is called the \emph{binomial model}. It is expected that this model is close to the first one if $Np$ is close to $M$.
\newline
The third model that we will investigate is on the form of a Markov process, see in Appendix for a discussion on properties used here from Markov chains. 
At time 0 there is no edge in the graph and an edge is selected at random among all of the possible edges. 
At time $t$, an edge is chosen among all the edges not already present in the graph. We denote this process by $\{\mathcal{G}_{n, t}\}_t$, with $t$ the number of edges added. 
It is clear that this model is perfectly equivalent to the first model presented if we fix $t = M$. 
This model was also introduced in 1959 by Erd\H{o}s and Renyi and is usually referred to as the \emph{random graph process}.
The advantage of this model is that it allows one to study properties on the verge of their realisations.
For instance, using this model Bollob\'as and Thomason \cite{Bollob85} proved that a graph is fully connected, when the last connection made is between an isolated vertex and the giant component. Equivalently, at the first time when $\delta(G) > 1$.
\newline
The three models introduced above ( $\mathcal{G}_{n, M}$, $\mathcal{G}_{n, p}$ and $\{\mathcal{G}_{n, t}\}_t $) are usually designated in the literature as Erd\H{o}s-R\'enyi model.
However, even if the first two are quite similar (static random graphs) the last one is of a different nature as it is dynamic.
\newline
In this report we will mainly focus on $\mathcal{G}_{n, p}$ which is in fact easier to study than $\mathcal{G}_{n,M}$.
\footnote{There is usually no confusion possible on the model being studied.}
We might also make use of the notation $\mathbb{P}_{p}$ (resp. $\mathbb{P}_{M}$) to designate the probability law associated with $\mathcal{G}_{n, p}$ (resp. $\mathcal{G}_{n,M}$).
We will see in \eqref{th:linkMP} that there is a strong relation between those two models.

\section{Connectivity}

One of the most fundamental structure of a graph is its number of connected components. 
Hence, the first question we will try to ask is with which probability a random graph following the Erd\H{o}s-Renyi model is connected. 
It is essential to answer this question as many other questions might not make sense or have an obvious answer on a graph that is not connected ( for instance the diameter, the existence of Hamiltonian paths or the stability number of the graph ). 
\newline
It is also an interesting first topic to have an insight of the kind of elegant results that arise from the study of random graphs.
The main aim of this section will be to prove the following theorem in a detailed way as it is the first random graphs proof that we will study.
\begin{theorem}\label{th:connect}
	Let $p = p(n) = \frac{\log(n) + c}{n}$ , $c \in \mathbb{R}$ independent of $n$.
\newline
Then $\lim_{n \to \infty} P(G \in \mathcal{G}_{n, p}\text{ is connected }) = e^{-e^{-c}}$ 
\end{theorem}
The proof of this theorem will be in two parts, first we will show that a graph is connected with high probability if there are no isolated vertices and then we will estimate the distribution of the number of isolated vertices.
We designate by giant component a component with a size larger than a constant times $n$.
\begin{theorem}\label{th:isolcomp}
	With $p = \frac{\log(n) + c}{n}$
	 A graph following $\mathcal{G}_{n, p}$ consists in a single giant component and isolated vertices with probability going to 1 when $n \to \infty$. 
\end{theorem}
\begin{proof}
This proof can be found in \cite{Spencer14} or \cite{Bollob01}.
\newline
During this proof we will consider the random value $X_k$ that counts the number of connected components of size $k$.
So, let's estimate the probability $\mathbb{P}(X_2 > 0) = \mathbb{P}(X_2 \geq 1 )$. In order to do so we will use the method of first moment.
\begin{align}
	\mathbb{P}(X_2 \geq 1) \leq \mathbb{E}(X_2) 	&= \binom{n}{2}\mathbb{P}(\text{"drawing an isolated edge"}) \\
						    	&= \binom{n}{2}p((1-p)^{n-1})^2 \\
							&\leq (\frac{ne}{2})^2p(e^{-p})^{2(n-2)} \\
						    	&= \mathcal{O}\left(n^2p e^{-2pn}\right) \\
							    &= \mathcal{O}(n^2pe^{-2(\log(n) + c}) = \mathcal{O}(p)
\end{align}
However, this is not sufficient to prove that there is no isolated component other than isolated vertices. We will observe that there can't have any component of size larger than $\lceil \frac{n}{2} \rceil$ that is not the largest component in the graph.
Hence, we will study the probability that there is any component of intermediary size that is not the greatest component.
\begin{align}
	\mathbb{P}(X_k \geq 1) &\leq \mathbb{E}(X_k)\quad, \forall k \geq 3 \\
				&\leq \binom{n}{k} k^{k-2} q_k\\
				&\leq \binom{n}{k} k^{k-2} p^{k-1} ((1-p)^{n-k})^k
\end{align}
In the above, the "$n$ choose $k$" term represents the number of possible subsets for the $k$ vertices of a connected component. 
The term $k^{k-2}$ is the number of possible spanning trees for each of these subset of $k$ vertices. 
The $q_k$ is the probability that a set of $k$ vertices is not connected to any other of the $n-k$ vertices.
Hence the term $p^{k-1}$ is the probability that the edges of the spanning tree are selected and the $((1-p)^{n-k})k$ is the probability that every vertex of the spanning tree are not connected to any of the other $n-k$ vertices.
\newline 
Now we will try to have an upper bound of the RHS such that the sum on $k$ will converges to a $o(n^{-\delta})$ for some $\delta >0$.
\begin{align}
	\mathbb{P}(X_k \geq 1) 	
	            &\leq k^{-2}p^{-1}(\frac{ne}{k})^k k^{k} p^{k} e^{-pk(n-k))}\\
			   &\leq k^{-2}p^{-1}((\frac{ne}{k}) k p e^{-p(n-k)})^k\\
			   &\leq p^{-1}(nep e^{-p(n-k)})^k
\end{align}
If we denote the term in the parentheses by $A$, then, if $k \leq \frac{n}{2}$
\begin{equation}
    A = \mathcal{O}(\log(n) n^{-\frac{1}{2}})
\end{equation}
Hence, we obtain
\begin{equation}
	\sum_{k=2}^{\lfloor n/2\rfloor} \mathbb{P}(X_k \geq 1)\leq o(1) + p^{-1}\sum_{k=3}^{\lfloor n/2\rfloor} A^k = o(1)
\end{equation}
Hence, the graph has, with probability tending to one, no component of size between 2 to $\lfloor \frac{n}{2} \rfloor$.
Hence it has only isolated vertices and components of size at least $\lceil \frac{n}{2} \rceil$.
Necessarily, there is at most one component of size at least $\lceil \frac{n}{2} \rceil$.
\newline
On the other hand, such component exists with probability going to 1. Otherwise, the graph would have only isolated vertices and then no edges, which occurs with probability $(1-p(n))^N \longrightarrow_{n\to \infty} 0$.

This proves theorem \eqref{th:isolcomp}.
\end{proof}

Theorem \eqref{th:connect} is proved in a more general setting later in this report in \ref{}, proofs restricted to the model $\mathcal{G}_{n,p}$ can be found in \cite{JLR}.


\section{Existence of thresholds}\footnote{ The proofs and results from this section are from \cite{JLR} and \cite{Bollob01}}
One of the most surprising features on random graphs, which seems to have motivated Erd\H{o}s to publish his results from 1959 ( TODO : FIND THE REF THAT SAID THAT...), is the existence of thresholds.
Indeed the property of appearance of certain graph properties will be either close to 0 or close to 1 for a great range of functions $p$.
\newline
For instance from the previous theorem we observe that $\forall \epsilon > 0$  if $p = (1+\epsilon)\frac{\log(n)}{n}$ 
or if $p = \frac{\log(n) + \omega(n)}{n}$ with $\lim_{n\to \infty} \omega(n) = \infty$ then $\lim_{n\to\infty}\mathbb{P}(G \in \mathcal{G}_{n,p} \text{ is connected}) = 1$.
We would say that for such $p$, $\mathcal{G}_{n,p}$ is asymptotically almost surely connected.
We also observe that if in the previous definitions of $p$ we changed the $+$ signs into $-$ signs, we would find that $\mathcal{G}_{n,p}$ is asymptotically almost surely disconnected.
\newline
Hence, the function $p$ from \eqref{th:connect} has some very peculiar behaviour on the property of connectivity.
We will call such a function a \emph{threshold} (here, for connectivity).
\newline
The aim of this section is to detail formally what we mean by a graph property and a threshold function.
We will also show a formal relation between $\mathcal{G}_{n,p}$ and $\mathcal{G}_{n,M}$ and a proof of the existence of thresholds on a family of graph properties.
\newline
It has been shown by Bollob\'as and Thomason \cite{Bollob87} that this is in fact not exclusive to random graphs, but true for all monotone properties on random subsets.
\begin{definition}
	We will call a \emph{graph property} a family of graphs that is closed under isomorphism.
\end{definition}
This means that a graph property is independent of the labelling and of the drawing of the graph.
We can refine properties in the following definition.
\begin{definition}
	A property is \emph{monotone }\footnote{ A property is monotone if it is either increasing or decreasing} \emph{increasing} (resp. \emph{decreasing}) if it's stable under the the addition (resp. removal) of an edge.
	A graph property $\mathcal{Q}$ is \emph{convex} if when $ A,C \in \mathcal{Q}$ and $A\subseteq B\subseteq C$\footnote{ $\subseteq$ is the inclusion of the edges on the same set of vertices} then $B \in \mathcal{Q}$. 
\end{definition}

For instance, being connected or containing a specific subgraph are monotone increasing properties whereas being planar or containing an isolated vertex are monotone decreasing. 
As an example of property that is neither monotone increasing or decreasing, we can think of being $k$-regular for some $k$ ( this means that all vertices are of degree $k$).
Having exactly $k$ isolated vertices is an example of a convex not monotone property.
\newline
Here is a theorem showing that monotone increasing properties make probability distributions on these properties also monotone increasing.
\begin{theorem}\label{th:QIncr}
	Suppose $\mathcal{Q}$ is a monotone increasing property, $0 \leq M_1 \leq M_2 \leq N$ and $0 \leq p_1 \leq p_2 \leq 1$.
	\newline
	Then
	\begin{equation}
		\mathbb{P}_{M_1}(\mathcal{Q}) \leq  \mathbb{P}_{M_2}(\mathcal{Q}) \text{  and  } \mathbb{P}_{p_1}(\mathcal{Q}) \leq \mathbb{P}_{p_2}(\mathcal{Q})
	\end{equation}
\end{theorem}
\begin{proof}
	The first inequality is clear, as the only difference between the two spaces on which we evaluate the property $\mathcal{Q}$ is that on the RHS edges have been added, hence, the probability of realising a monotone increasing property has been increased.	
	\newline
	For the second inequality, let $p = \frac{p_2 - p_1}{1 - p_1}$. Let $G_1 \in \mathcal{G}_{n, p_1}, G \in \mathcal{G}_{n, p}$ independent to each other.
	\newline
	So if $G_2 = G_1 \cup G$ it's edges are chosen with probability $p_1 + p - p_1 p = p_2$. 
	So $G_2$ follows $\mathcal{G}_{n,p_2}$ and contains $G_1$, the property being monotone increasing, we have $ \mathbb{P}_{p_1}(\mathcal{Q}) \leq \mathbb{P}_{p_2}(\mathcal{Q})$
\
\end{proof}
The following result follows from definition, when $\mathcal{Q}$ is a monotone increasing property\footnote{It is in fact true for any property}:
\begin{equation}\label{eq:QProp}
	\mathbb{P}(\mathcal{Q}) = \sum_{A \in \mathcal{Q}} p^{|A|}(1-p)^{N-|A|}
\end{equation}
However this result requires to know all of the elements in $\mathcal{Q}$ and as we are often interested with properties for very large $n$ this result won't be magical... 
\newline
The following theorem shows that if we know quite accurately $\mathbb{P}_M(\mathcal{Q})$ for every $M$ close to $pN$ then we know $\mathbb{P}_p(\mathcal{Q})$ with a comparable accuracy. The converse being clearly false, for instance the property of containing M edges.

\begin{theorem}\label{th:linkMP}
	Suppose $\mathcal{Q}$ is any property and $0 < p = M/N< 1$. 
	\newline
	Then $\mathbb{P}_M(\mathcal{Q}) \leq 3 \sqrt{M}\mathbb{P}_p(\mathcal{Q})$
\end{theorem}
\begin{proof}
	Let $\mathcal{Q}$ be any property, then we will write $\mathcal{Q}$ as a partition based on the number of edges in each graph contained in $\mathcal{Q}$.
	\newline
	So we have
	$$\mathcal{Q} = \bigsqcup_{m=0}^{N} \mathcal{Q}_m \quad, \text{ with } \forall G \in \mathcal{Q}_m, e(G) = m$$
	We have $\mathbb{P}_m(\mathcal{Q}) = |\mathcal{Q}_m| \binom{N}{m}^{-1}$
	From this we can obtain, with $q = 1 - p$
	\begin{align*}
		\mathbb{P}_p(\mathcal{Q})	&= \sum_{A \in \mathcal{Q}} p^{|A|}q^{N-|A|}
						= \sum_{m=0}^{N}\sum_{A \in \mathcal{Q}_m} p^{|A|}q^{N-|A|}\\
						&= \sum_{m=0}^{N}\sum_{A \in \mathcal{Q}_m} p^{m}q^{N-m}
						= \sum_{m=0}^{N}|\mathcal{Q}_m|p^{m}q^{N-m}\\
						&= \sum_{m=0}^N p^mq^{N-m}\binom{N}{m}\mathbb{P}_m(\mathcal{Q}) 
						\geq \binom{N}{M}p^Mq^{N-M}\mathbb{P}_M(\mathcal{Q}) \\
						&\geq \mathbb{P}_M(\mathcal{Q})(e^{\frac{1}{6M}}\sqrt{2\pi p q N})^{-1}\\
	\end{align*}
	The last inequality coming from inequality (1.5) of chapter 1 in \cite{Bollob01}. The proof of the inequality uses the sharp estimates of Stirling's $n! \sim (\frac{n}{e})^n\sqrt{2\pi n}e^{\alpha_n}$ proved in Robinson ( TODO: ADD REF). 
	The proof being rather lengthy and out of the topic of this report we will admit it.
	\begin{equation}
		\mathbb{P}_M(\mathcal{Q}) \leq  \mathbb{P}_p(\mathcal{Q})e^{\frac{1}{6M}}\sqrt{2\pi pq N}
	\end{equation}
	Observing that $q \leq 1$ and $\sqrt{2\pi}e^{\frac{1}{6}} \approx 2.961... < 3$ the proof is complete.
\end{proof}
The previous section was about connectivity in $\mathcal{G}_{n,p}$, in this section we have seen that connectivity can be characterised as a monotone increasing property.
Also it was observed that the function $p$ was somehow best possible, by that we mean that modifying it slightly would imply to only have a zero-one law. 
We call such a function $p$ a threshold ( in that case for the connectivity ). 
\newline

More formally, let $\mathcal{Q}$ a monotone increasing property,  in $\mathcal{G}_{n, p}$, we call $\hat{p} = \hat{p}(n)$ a threshold if
\begin{align}
	\mathbb{P}(\mathcal{G}_{n,p} \in \mathcal{Q}) \rightarrow \left\{\begin{array}{rl}
										0 & \text{if } p / \hat{p} \to 0,\\
										1 & \text{if } p / \hat{p} \to \infty.
									 \end{array}
									\right.
\end{align}

Analogously, in $\mathcal{G}_{n, M}$, we call $\hat{M} = \hat{M}(n)$ a threshold if
\begin{align}
	\mathbb{P}(\mathcal{G}_{n,M} \in \mathcal{Q}) \rightarrow \left\{\begin{array}{rl}
										0 & \text{if } M / \hat{M} \to 0,\\
										1 & \text{if } M / \hat{M} \to \infty.
									 \end{array}
									\right.
\end{align}
In fact, thresholds are unique with respect to the multiplication by a positive constant. So for the following, we should denote a threshold for a property as \emph{the} threshold.
\newline
As said in the introduction of this section, the fact that thresholds could be found for many of the properties that where investigated is one of the main reason behind the study of random graphs.
In fact, the following theorem from Bollob\'as and Thomason (TODO : ADD REF )confirms that we can always expect the existence of a threshold if the property investigated is non trivial
\footnote{A property being trivial if it is always or never satisfied, for instance, having a specified number of vertices or the empty-set propery.}.
\begin{theorem}
	Every non-trivial monotone graph property has a threshold
\end{theorem}
\begin{proof}
	We consider without loss of generality that $\mathcal{P}$ is a non-trivial monotone increasing graph property.
	Given $0 < \epsilon < 1$ we define $p:[0,1] \to [0,1]$ such that :
	\begin{equation}
		\mathbb{P}(\mathcal{G}_{n, p} \in \mathcal{P}) = \epsilon
	\end{equation}
	The existence of $p$ is guaranteed from \eqref{eq:QProp} because it is an increasing polynomial in $p$, from 0 to 1.
	Indeed, we know that it is increasing from \eqref{th:QIncr}.
	\newline
	We will show that $p^* = p(\frac{1}{2})$ is a threshold for $\mathcal{P}$ through a coupling argument.
	We take $G_1, G_2, ..., G_k$ independent random variables following $\mathcal{G}_{n,p}$.
	Then we claim that $G_1 \cup G_2 \cup ... \cup G_k$ is distributed like $\mathcal{G}_{n, 1 - (1-p)^k}$.
	This is clear by induction on $k$ since we observe the following equivalence for $p_1 < p$ and $p_2$.
	\begin{equation}
		1 - p = (1 - p_1)(1-p_2) \iff p = p_1 + p_2 - p_1p_2
	\end{equation}
	The previous equation being satisfied with $p = 1 -(1-p)^k$, $p_1 = 1 - (1-p)^{k-1}$ and $p_2 = p$.
	\newline
	We may now use Bernoulli's inequality \eqref{bernoulli}, $1 - (1-p)^k \leq kp$ to obtain that we can couple the graphs in such a way that:
	\begin{equation}
		\mathbb{G}_{n,1 - (1-p)^k} \subseteq \mathbb{G}_{n, kp}
	\end{equation}
	and so $\mathbb{G}_{n, kp} \not\in \mathcal{P}$ implies $G_1, G_2, ..., G_k \not\in \mathcal{P}$. 
	We obtain
	\begin{equation}
		\mathbb{P}(\mathbb{G}_{n, kp} \not\in \mathcal{P}) \leq (\mathbb{P}(\mathbb{G}_{n,p}\not\in\mathcal{P}))^k
	\end{equation}
	Let $\omega$ be a function of $n$ growing arbitrarily slowly such that $\lim_{n\to\infty}\omega(n) = \infty$. 
	Suppose also $p^* = p(\frac{1}{2})$ and $k=\omega$, then
	\begin{equation}
		\mathbb{P}(\mathbb{G}_{n, \omega p^*} \not\in \mathcal{P}) \leq 2^{-\omega} = o(1)
	\end{equation}
	On the other hand,
	\begin{equation}
		\frac{1}{2} = \mathbb{P}(\mathbb{G}_{n,p^*} \not\in \mathcal{P}) \leq (\mathbb{P}(\mathbb{G}_{n, \frac{p^*}{\omega}} \not\in \mathcal{P}))^{\omega}
	\end{equation}
	Finally, 
	\begin{equation}
		\mathbb{P}(\mathbb{G}_{n, \frac{p^*}{\omega}} \not \in \mathcal{P}) \geq 2^{-\frac{1}{\omega}} = 1 -o(1)
	\end{equation}
	This proves that $p^*$ is a threshold for $\mathcal{P}$.
\end{proof}

\section{The stability number}\footnote{This section is from \cite{Bondy08}}
Another property of graphs that one might be interested to study is the stability number. The \emph{stability number} of a graph is the size of the largest set of vertices we can choose in a graph such that no two vertices are adjacent. 
One of the reasons that makes this an interesting property to study is that it is linked to one of the most fundamental property of graphs, $\chi (G)$, the \emph{chromatic number}. 
Indeed, the chromatic number is the smallest number of colours\footnote{A colouring of a graph is just assigning to each edge a colour. Colours can be thought as "red, green, blue, ..." or as numbers.} that makes a \emph{proper colouring} of a graph, 
that means a colouring on which there are no two adjacent vertices of the same colour. 
If we denote by $\alpha(G)$ the stability number of a graph, it is not difficult to get \footnote{ We use this link with the chromatic as a simple motivation for studying the stability number. It is a number which is in fact of great importance in graph theory but on matters that are out of the scope of this report. See for instance Chapter 12 in \cite{Bondy08}.}
\begin{equation}
	\chi(G) \geq \frac{n}{\alpha(G)}
\end{equation}
Before giving a lower bound on the stability number of a graph it might be interesting to notion that the notion of stable set is dual to the notion of clique and is analogous to the notion of perfect matching that concerns the edges.
Although the following theorem will give a bound that is quite tight in $\mathcal{G}_{n, p}$ the problem of finding the actual maximum stable set of a graph is a $NP$-hard problem.
\begin{theorem}
	The stability number of a graph in $\mathcal{G}_{n, p}$, is at most $\lceil 2p^{-1}\log(n)\rceil$, with probability going to 1 when $n \longrightarrow \infty$.
\end{theorem}
\begin{proof}
	Let $G \in \mathcal{G}_{n,p}$ and $S \subseteq V$ such that $S$ contains $k+1$ vertices.
	Then we have 
	\begin{equation}
		\mathcal{P}(\text{"S is a stable set"}) = (1-p)^{\binom{k+1}{2}  }
	\end{equation}
	as none of the $\binom{k+1}{2}$ possible edges must be selected.
	\newline
	Let's define our random values as follow, $X_S = \mathbbm{1}(\text{" $S$ is a stable set "})$ and
	\begin{equation}
		X_{k+1} = \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} X_S
	\end{equation}
	the random variable counting the number of stable sets of size $k+1$, so what we are investigating here is the smallest $\alpha$ such that $X_k = 0, \forall k > \alpha$. Such an $\alpha$ would then be the stability number.
	\begin{align}
		\mathbb{E}X_{k+1} &= \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} \mathbb{E} X_S 	
				  = \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} \mathbb{P}(X_S = 1)	\\		
		&= \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} (1-p)^{\binom{k+1}{2}  }		
		= (1-p)^{\binom{k+1}{2}} \sum_{\substack{ S \subseteq V \\ |S| = k + 1}} 1 	\\	
		&= \binom{n}{k+1} (1-p)^{\binom{k+1}{2}} 
	\end{align}
	Now we will use the inequalities $  \binom{n}{k+1} \leq \frac{n^{k+1}}{(k+1)!} $ and $(1-p) \leq e^{-p}$ (see \ref{th:NChooseK}). 
	And we have
	\begin{align}
		\mathbb{E}X_{k+1} 	&\leq  \frac{n^{k+1}}{(k+1)!} e^{-p \binom{k+1}{2}} = \frac{n^{k+1}}{(k+1)!} e^{-p \frac{k(k+1)}{2}}\\
									     &\leq  \frac{(ne^{-p\frac{k}{2}})^{k+1}}{(k+1)!}
	\end{align}
	So, if we consider $k = \lceil 2p^{-1}\log(n)\rceil \leq 2p^{-1}\log(n)$ we have that $ ne^{-p\frac{k}{2}} \leq 1$.
	We finally obtain 
	\begin{equation}
		\mathbb{E} X_{k+1} \longrightarrow_{n \to \infty} 0
	\end{equation}
	And then $X_{k+1} = 0$ with probability tending to 1 which proves the theorem.

\end{proof}

\section{The diameter}\footnote{This section uses \cite{Bollob01}}

\begin{definition}
	The \emph{diameter} of a graph is the greatest distance between any pair of vertices. We denote it by diam($G$) and say it is equal to $\infty$ if the graph is not connected.
\end{definition}
It is quite easy to understand that the diameter is a value that is a great importance particularly in applied systems. For instance, the small world phenomena is quite notorious ( and will be discussed later in this report ) but we can also think of optimisation problems in which the fact that two points are far apart might be of great consequences.
This section won't be focused on real world applications of the diameter because as we will see we are studying a model which is not realistic for that. 
Hence we will first discuss some graph theoretic problems and results on the diameter and after giving the main theorem on the diameter we will prove it through several technical lemmas, some of which will be admitted. Finally some corollary will be obtained from the theorem.
\newline
One of the challenging questions in graph theory is estimating the following function
\begin{equation}\label{eq:DDeltaProb}
	n(D, \Delta) = max\{ |G|, \text{diam}(G) \leq D, \Delta(G) \leq \Delta\}
\end{equation}
$n$ is the function that for a fixed diameter $D$ and a fixed maximal degree $\Delta$, gives the maximal number of vertices of a graph that verifies both conditions. This is the kind of problem that is part of \emph{extremal graph theory}.
\newline
For instance if we take $\Delta = 2$ we obtain easily by construction that a graph that maximises the number of vertices with a diameter $D$ is a $(2D+1)$-cycle. Hence, 
\begin{equation}
	n(D, 2) = 2D + 1, \forall D \in \mathbb{N}
\end{equation}
But it is in fact very hard to obtain such a formula for other values of $D$ or $\Delta$.
\newline
If we take a graph of max degree $\Delta$ we observe that there are at most $\Delta (\Delta - 1)^{k-1}$ vertices at distance $k$ from a chosen vertex. It is easily to be convinced of this simply by drawing such a graph. 
From this very simple construction we can obtain the following upper bound
\begin{equation}
	n(D, \Delta) \leq 1 + \Delta \sum_{j=1}^{D}(\Delta - 1)^{j-1} = \frac{\Delta(\Delta - 1)^D - 2}{\Delta - 2} = n_0(D, \Delta)
\end{equation}
$n_0$ is called the Moore bound and a graph for which the Moore bound is best possible is called a Moore graph. 
The \emph{Petersen's graph} is such a graph with parameter $D= 2$ and $\Delta = 3$, see Figure \ref{fig:petG}.
\begin{figure}
	\centering
	\begin{tikzpicture}
			\tikzstyle{LabelStyle}=[fill=white,sloped]
			\tikzstyle{EdgeStyle}=[]
			\SetVertexMath
			\grPetersen[form=1,RA=3.6,RB=1.8]
	\end{tikzpicture}
    \caption{A representation of Petersen's graph, $D=2$, $\Delta =3$}
    \label{fig:petG}
\end{figure}
When $D=2$ explicit constructions of Moore graphs have been found with $\Delta = 2$ (pentagon) and $\Delta = 3$ (Petersen's graph) as we have seen above.
There also exists an explicit construction with $\Delta = 7$, the Hoffman-Singleton graph on 50 vertices.
There might exist a graph with $\Delta = 57$ on 3250 vertices but its existence is still an open-question.
However, it is proved that no other graph of diameter 2 can be a Moore graph, see \cite{Hoffman60}.
\newline
In the end of this section we will give a quite good lower bound on $n$, in order to do so, we will see that we can select a probability function $p$ such that the value of the diameter is highly concentrated.
\newline
In the following we will consider that $d = d(n) \geq 2$ is a natural number (representing the diameter) and $c$ is a positive real number.
\newline
Now let's give some functions that will allow us to make a study of the diameter in random graphs. If we choose $x$ a vertex in a graph, then we define $\Gamma_k(x)$ as the set of vertices at distance $k$ from $x$. And from this we will define $N_k(x)$ as the set of vertices of distance less than or equal to $k$. 
Formally, we have
\begin{equation}
	\Gamma_k(x) = \{v : d(x, v) = k \}
\end{equation}
\begin{equation}
	N_k(x) = \bigcup_{i=1}^k \Gamma_i(x)
\end{equation}
And we can link those with the diameter using, diam$(G) \leq d $ if and only if $N_d(x) = V(G), \forall x$.
\newline
Similarly diam$(G) \geq d $ if and only if $\exists y, N_{d-1}(y) \neq V(G)$.
\newline
Now we will define the probability function that we will use in the following of this section as 
\begin{equation} \label{eq:pdiam}
	p^d n^{d-1} = \log(\frac{n^2}{c}) \quad, \text{  for some } c >0
\end{equation}
Moreover, we will assume
\begin{equation}\label{eq:logtrois}
	\frac{pn}{(\log(n))^3} \longrightarrow \infty
\end{equation}
Note that this condition is automatically satisfied if $d(n)$ is uniformly bounded.
The aim of this section will be to prove the following theorem.
\begin{theorem}\label{th:diam2}
	Using all previous definitions and conditions on $p$, $d$ and $c$, we have,
	\begin{equation}
		\mathbb{P}(\text{diam}(\mathbb{G}_{n, p}) = d) \longrightarrow e^{-\frac{c}{2}}
	\end{equation}	
	\begin{equation}
		\mathbb{P}(\text{diam}(\mathbb{G}_{n,p}) = d + 1) \longrightarrow 1 - e^{-\frac{c}{2}}
	\end{equation}
\end{theorem}
This theorem states that in $\mathcal{G}_{n,p}$ with $p$ defined as in \eqref{eq:pdiam} the diameter is spread on only two values.
As a corollary we clearly have
\begin{corollary}
	Using all previous definitions on $p$, $d$ and $c$. We have,
	\begin{equation}
		\mathbb{P}(\text{diam}(\mathbb{G}_{n,p} \in \{d, d+1\}) \longrightarrow 1
	\end{equation}
\end{corollary}

In fact the number of values that the diameter can take has been fully resolved by Chung and Lu in \cite{ChungLu01}.
\newline
Before proving such a theorem we will need some technical lemmas and assumptions. So we will give here some equations for reference later.
Also from \eqref{eq:logtrois},
\begin{equation}
	p (\frac{\log(n)}{n})^{-1} \longrightarrow_{n \to \infty} \infty
\end{equation}
that should not be too surprising as the threshold for connectivity is $\frac{\log(n) + c}{n}$ as shown before.
We may also observe, since $d(n) \geq 2$ that $p = o(n^{-\frac{1}{2} +\epsilon}), \forall \epsilon > 0$.
We observe that $p$ and $d$ are connected as follows:
\begin{equation}
	p=n^{\frac{1}{d} - 1}(\log(\frac{n^2}{c}))^\frac{1}{d}
\end{equation}
\begin{align}\label{eq:pnd}
	d &= \frac{1}{\log(pn)}(\log(n) + \log \log (n) + \log(2) + \mathcal{O}(\frac{1}{\log(n)}))\\
	  &= \mathcal{O}((1+o(1))\frac{\log(n)}{\log\log(n)})
\end{align}
And finally
\begin{equation}
	p(pn)^{d-2} = o(1)
\end{equation}
The first lemma that we present here gives a tail inequality of $\Gamma_k(x)$ conditionally on some space that we will now define. 
In this section, $\Omega_k$ is $\mathcal{G}_{n, p}$ conditionned on $a = |\Gamma_{k-1}(x)|$ and $b = |N_{k-1}(x)|$ that satisfy
\begin{align}\label{eq:CondAB}
 \left\{\begin{array}{rl}
		 \frac{1}{2}(pn)^{k-1} \leq a &\leq \frac{3}{2}(pn)^{k-1} \\
		 			    b &\leq 2(pn)^{k-1}
	 \end{array}
	\right.
\end{align}
\begin{lemma}
	Let $x$ be a fixed vertex.
	\begin{equation}
		1 \leq k = k(n)  \leq d-1
	\end{equation}
	And $ K = K(n)$ that satisfy $ 6 \leq K \leq \frac{1}{12} \sqrt{pn\frac{1}{\log(n)}}$
	We also define
	\begin{equation}
		\alpha_k = K\sqrt{\frac{\log{n}}{(pn)^k}} , \quad \beta_k = p(pn)^{k-1}, \quad \gamma_k = 2\frac{(pn)^{k-1}}{n} = \frac{2\beta_k}{pn}
	\end{equation}

	Then
	\begin{equation}
		\mathbb{P}(|\Gamma_k(x)| - apn| \geq (\alpha_k + \beta_k + \gamma_k)apn \quad|\quad \Omega_k) \leq n^{-\frac{K^2}{9}}
	\end{equation}
\end{lemma}

\begin{proof}
	Conditionally on $|\Gamma_{k-1}(x)| = a$ and $|N_{k-1}(x)| = b$, $|\Gamma_k(x)|$ follows a binomial distribution of parameters $n_k = n-b$ and
	\begin{equation}
		p_a = 1 - (1-p)^a
	\end{equation}
	Indeed $\Gamma_k(x)$ is the set of vertices not in $N_{k-1}(x)$ and connected to at least one element of $\Gamma_{k-1}(x)$.
	\newline
	In the following inequalities we will assume that $n$ is large enough, this will allow us to do certain inequalities that are only asymptotically satisfying. Also to make it more easy for the reader we consider that all of these inequalities take place in $\Omega_k$ without writing it explicitly.
	Under the event we consider, we have:
	\begin{align}
		|\Gamma_k(x)| - apn| &\geq (\alpha_k + \beta_k + \gamma_k)apn \\
		 &\geq (\alpha_k + \beta_k + \gamma_k)apn + ap(n -n_k) \\
		 &= (\alpha_k + \beta_k + \gamma_k + 1 - \frac{n_k}{n})apn \\ 
		 &= (\alpha_k + \beta_k + \gamma_k - \frac{b}{n})apn \\  
	\end{align}
	Using, \eqref{eq:CondAB}, we obtain the following inequality:
	\begin{equation}
		\gamma_k - \frac{b}{n} = \frac{1}{n}(\frac{2\beta_k}{p} - b) = \frac{1}{n}(2(pn)^{k-1} -b) > 0
	\end{equation}
	From which we get:
	\begin{equation}
		 (\alpha_k + \beta_k + \gamma_k - \frac{b}{n})apn \geq (\alpha_k + \beta_k)apn \geq (\alpha_k + \beta_k)apn_k  
	\end{equation}
	From this first sequence of inequalities we managed to remove $\gamma_k$ and we used some $n_k$ that is less than $n$.
	The point in evaluating these inequalities is that we now have
	\begin{align}
		\mathbb{P}(|\Gamma_k(x)| - apn| \geq (\alpha_k &+ \beta_k + \gamma_k)apn \quad|\quad \Omega_k) \\&\leq \mathbb{P}(|\Gamma_k(x)| - apn_k| \geq (\alpha_k + \beta_k )apn_k \quad|\quad \Omega_k)
	\end{align}
	Now using $ap -p_a \leq \beta_k ap$ and the triangular inequality we have
	\begin{align}
		&\leq \mathbb{P}(|\Gamma_k(x)| - p_an_k| \geq \alpha_k apn_k \quad|\quad \Omega_k)\\
		&\leq \mathbb{P}(|\Gamma_k(x)| - p_an_k| \geq \alpha_k p_an_k \quad|\quad \Omega_k)\\
	\end{align}
	And using the tail inequality of Theorem 1.7 from Bollob\'as \cite{Bollob01} which we admit, we have,
	\begin{align}
		&\leq \frac{1}{\sqrt{\alpha_k^2p_an_k}}\exp(-\frac{1}{3}\alpha_k^2p_an_k) \\
		&\leq \exp(-\frac{1}{3}\alpha_k^2p_an_k)
	\end{align}
	And using $p_a \geq pa(1-\frac{pa}{2})$ , $a \geq \frac{1}{2}(pn)^{k-1}$ and using $n_k = n-b$ we obtain $p_an_k > \frac{(pn)^k}{3}$ that we insert in the previous inequality that will give us the result expected.
	\begin{align}
		&\leq \exp(-\alpha_k^2\frac{(pn)^k}{9}) = n^{-\frac{K^2}{9}}
	\end{align}
\end{proof}
We will now prove another lemma
\begin{lemma}\label{th:KLemma}
	Let $K > 12$ a constant and $\alpha_k, \beta_k, \gamma_k, k=1,...,d-1$ as before.
	\newline
	Set 
	\begin{equation}
		\delta_k = \exp(2\sum_{l=1}^{k}(\alpha_l + \beta_l +\gamma_l)) - 1
	\end{equation}
	Then if $n$ is sufficiently large, with probability at least $1-n^{-K-2}$ for every vertex $x$ and every natural number $k, 1\leq k \leq d-1$ we have
	\begin{equation}
		||\Gamma_k(x)| - (pn)^k| \leq \delta_k(pn)^k
	\end{equation}
\end{lemma}

\begin{proof}
	As $\delta_{d-1} \longrightarrow_{n\to\infty} 0$ we may assume that $\delta_{d-1} < \frac{1}{4}$
	For a fixed vertex $x$, we denote by $\Omega_k^*$ the set of graph for which 
	\begin{equation}
		||\Gamma_l(x)| - (pn)^l| \leq \delta_l(pn)^l \quad , 0 \leq l \leq k
	\end{equation}
	And it is easy to verify that it is decreasing. Anf if one replaces $|\Gamma_l(x)|$ by $a$ it is clear that we have
	\begin{equation}
		\Omega_k^* \subseteq \Omega_{k-1}^* \subseteq \Omega_k
	\end{equation}
	We shall now prove by induction that:
	\begin{equation}\label{eq:InductionHyp}
		1 - \mathbb{P}(\Omega_k^*)\leq 2kn^{-\frac{K^2}{9}}
	\end{equation}
	Now, simply applying Bayes formula (for the complementary) we have
	\begin{equation}\label{eq:bayes}
		1 - \mathbb{P}(\Omega_k^*)= 1-\mathbb{P}(\Omega_{k-1}^*)
		+ \mathbb{P}(\Omega_{k-1}^*)\mathbb{P}(||\Gamma_k(x)| - (pn)^k| > \delta_k(pn)^k | \Omega_{k-1}^*)
	\end{equation}
	If $G \in \Omega_{k-1}^*$ and $|a| = |\Gamma_{k-1}(x)|$, then by applying the definition of belonging to $\Omega_{k-1}^*$ and multiplying both sides by $pn$ we have 
	\begin{equation}
		|(pn)^k - apn| \leq \delta_{k-1}(pn)^k
	\end{equation}
	And we obtain using the second triangular inequality
	\begin{align}
		\mathbb{P}(\neg\Omega_k^* | \Omega_{k-1}^*) 
		&\leq \mathbb{P}(\Omega_{k-1}^*)^{-1}\mathbb{P}(||\Gamma_k(x)| - apn| \geq (\delta_k - \delta_{k-1})(pn)^k | \Omega_k)\\
		&\leq (1 - 2(k-1)n^{-\frac{K^2}{9}})^{-1}\mathbb{P}(||\Gamma_k(x)| - apn| \geq 2(\alpha_k + \beta_k + \gamma_k)(pn)^k | \Omega_k)
	\end{align}
	The last inequality being obtained from the hypothesis of induction and using $(1+x) \leq \exp(x)$. Now using the fact that $apn \leq \frac{3}{2} (pn)^k$ we have
	\begin{align}
		&\leq 2\mathbb{P}(||\Gamma_k(x)| - apn| \geq (\alpha_k + \beta_k + \gamma_k)apn | \Omega_k)\\
		&\leq 2n^{-\frac{K^2}{9}}
	\end{align}
	which proves \eqref{eq:InductionHyp}.
	If we combine the last inequality that is obtained applying the previous lemma with \eqref{eq:bayes} then we have the result.
\end{proof}
\begin{definition}
	For $x$ and $y$ two vertices of $G$, we say that $x$ and $y$ are remote if $y \not\in N_d(x)$.
\end{definition}
	We shall now prove that with high probability, two remote pairs of vertices in $\mathcal{G}_{n,p}$ do not share a vertex.
	From \ref{th:KLemma} we can obtain the following equation.
	\begin{equation}
		\mathbb{P}(|N_{d-1}(x)| < \frac{5}{6}(pn)^{d-1}) < n^{-4}
	\end{equation}
	Now we estimate the probability that a vertex $y$ is joined to no vertex in a set $W$ with $|W| \geq \frac{5}{6}(pn)^{d-1}$
	\begin{equation}
	(1-p)^{|W|} \leq \exp(-|W|) \leq \exp(-p|W|) = \exp(-\frac{5}{6}\log(\frac{n^2}{c})) = c^{\frac{5}{6}}n^{-\frac{5}{3}}
	\end{equation}

	Hence, if $x, y, z$ are distinct vertices we have
	\begin{align}
		\mathbb{P}(x&\text{ is remote from $y$ and $z$ }) 						\\	
		&\leq \mathbb{P}(|N_{d-1}(x)|\leq \frac{5}{6} (pn)^{d-1}) 					\\
		&+\mathbb{P}(\{y, z\} \cap N_{d-1}(x) = \emptyset | |N_{d-1}(x)| \geq \frac{5}{6}(pn)^{d-1} )	\\
		&\leq \mathbb{P}(|N_{d-1}(x)|\leq \frac{5}{6} (pn)^{d-1}) 					\\				
		&+(\mathbb{P}(\text{$y$ is not joined to } W = N_{d-1}(x) | W \geq \frac{5}{6} (pn)^{d-1}))^2	\\
		&\leq n^{-4} + c^{\frac{5}{3}}n^{-\frac{10}{3}}							\\
		&\leq n^{-3}n^{-\frac{1}{4}}
	\end{align}
	So, we obtain
	\begin{align}
		\mathbb{P}(\mathcal{G}_{n, p} &\text{ contains two remote vertices pairs sharing a vertex })	\\
			&\leq \sum_x\sum_{(y \neq z)}\mathbb{P}(\text{ $x$ is remote from $y$ and $z$ })		\\
			&\leq \sum_x \binom{n}{2}n^{-3-\frac{1}{4}} = n\binom{n}{2}n^{-3-\frac{1}{4}}		\\
			&\leq n^{-\frac{1}{4}}
	\end{align}
	The following lemma can be obtained quite easily simply by construction.
	\begin{lemma}
		From $r$ disjoint pair of vertices, there are $2^r$ $r$-tuples of vertices meeting each pair.
	\end{lemma}
	As we have seen above, the number of remote pairs is within $o(1)$ the number of remote disjoint pairs, which gives the following lemma.
	\begin{lemma}
		The $r$-th factorial moment of $X$, $X$ being the number of remote pairs of vertices, is within $o(1)$ of the expected number of ordered $r$-tuples of disjoint remote pairs.
	\end{lemma}
	If we denote by $\mathbb{F}_r$ the probability that a fixed $r$-tuple consists of vertices remote from each other.
	Then,
	\begin{equation}
		\mathbb{E}_r(X) = \frac{n!}{(n-r)!}2\mathbb{F}_r(1+o(1)) + o(1)
	\end{equation}
	In order to shorten the proof, we admit the following lemma that requires a quite long and technical proof. The proof can be found in \cite{Bollob81}.
	\begin{lemma}
		Let $K= \max\{r+2, e^7\}$ and using all previous definitions on $p, d$ and $c$, in particular \eqref{eq:logtrois}.
		With probability at least $1-n^{-K}$
		\begin{equation}
			(1-n^{-K})Q_r \leq \mathbb{F}_r \leq (1-n^{-K}) Q_r + n^{-K}
		\end{equation}
		with $Q_r = (\frac{c}{n})^r(1+o(1))$.
		\newline
		In particular:
		\begin{equation}
			\mathbb{F}_r = (\frac{c}{n})^r(1+o(1))
		\end{equation}

		and we can obtain the asymptotical estimate
		\begin{equation}
			\mathbb{E}_r(X) = n^r2^{-r}(\frac{c}{n})^r(1+o(1)) + o(1)
		\end{equation}
	\end{lemma}
	Using the following theorem 
	\begin{theorem}\footnote{See \eqref{th:factPois} for a proof}
		Let $X_1, X_2, \ldots$ be non-negative integer valued random variables and $X$ a random variable following a Poisson distribution of parameter $\lambda$.
		Suppose
		\begin{equation}
			\lim_{n\to\infty} \mathbb{E}_r(X_n) =\lambda^r , \quad r=0,1,\ldots
		\end{equation}
		and 
		\begin{equation}
			\lim_{r \to \infty} \frac{E_r(X) r^m}{r!} = 0,\quad m = 0, 1, \ldots
		\end{equation}
		Then 
		\begin{equation}
			X_n \longrightarrow_d X
		\end{equation}
	\end{theorem}
	Then we have that $X$ converges in distribution to Poisson law of parameter $\frac{c}{2}$.
	\newline
	We can then obtain 
	\begin{equation}\label{eq:diamcv}
		\mathbb{P}(X=0) = \mathbb{P}(\text{diam}(G) \leq d) \longrightarrow e^{-\frac{c}{2}}
	\end{equation}
	that proves the first part of the theorem. 
	For each fixed $c$, we have $p\geq p'$ where $(p')^{d+1}n^d = \log(\frac{n^2}{c})$, when $n$ is large enough.
	Hence, by the previous estimates, with $d$ replaced by $d+1$, we get,
	\begin{equation}
		\liminf_{n \to \infty} \mathbb{P}(\text{diam}(G) \leq d+1) \geq e^{-\frac{c}{2}}
	\end{equation}
	and since $c$ is arbitrary
	\begin{equation}
		\mathbb{P}(\text{diam}(G) \leq d+1) \longrightarrow_{n\to\infty} 1
	\end{equation}
	Finally, if we combine the last result with \eqref{eq:diamcv} it is clear that the proof if complete.
	\newline
	To end this section we present two theorems that are much easier to prove with a similar flavor to the previous one.
	\begin{theorem}\label{th:diam2}
	Suppose 
	\begin{equation}
		i) \quad p^2n - 2\log(n) \longrightarrow \infty
	\end{equation}
	\begin{equation}
		ii) \quad n^2(1-p) \longrightarrow \infty
	\end{equation}
	Then almost every graph in $\mathcal{G}_{n, p}$ has diameter 2.
\end{theorem}

\begin{proof}
	for two distinct vertices $x$ and $y$,
	\begin{equation}
		\mathbb{P}(\text{dist}(x, y) > 2) = (1-p^2)^{n-2}(1-p)
	\end{equation}
	as it is the probability that two vertices $x$ and $y$ are not connected by a path of length one or two.
	\newline
	Then using the the first moment method.
	\begin{align}
		\mathbb{P}(\text{diam}(\mathbb{G}_{n,p}) > 2) 	&\leq \mathbb{E}(\text{number of pairs of vertices with distance} > 2) \\
	&\leq \binom{n}{2}(1-p)(1-p^2)^{n-2} = \mathcal{O}(n^2e^{-np^2})
	\end{align}
	Which tends to 0 by $i)$. Hence, with high proability, the diameter of $\mathbb{G}_{n,p}$ is less than 2. 
	In order to finish to prove the theorem we need to show that the diameter of $\mathbb{G}_{n,p}$ is not 1 with high probability.
	A graph of diameter 1 being a complete graph we have
	\begin{align}
		\mathbb{P}(\text{diam}(\mathbb{G}_{n,p}) = 1) = \mathbb{P}(\mathbb{G}_{n,p} = K_n) 	
				= p^{\binom{n}{2}} &\longrightarrow 0				\\
		\text{iff  } \log p^{\binom{n}{2}} &\longrightarrow -\infty			\\
			   \text{iff  } n^2 \log p &\longrightarrow -\infty			\\
	\text{iff  } n^2 (1-p) &\longrightarrow +\infty			
	\end{align}	
\end{proof}
And simply if we remember the previous theorem \eqref{th:linkMP} linking $\mathcal{G}_{n,p}$ and $\mathcal{G}_{n,M}$ we obtain the following corollary:
\begin{corollary}
	If $M=M(n) < \binom{n}{2}$ satisfies 
	\begin{equation}
		\frac{2M^2}{n^3} - \log(n) \longrightarrow \infty
	\end{equation}
	Then with high probability $\mathbb{G}_{n, M}$ is of diameter 2.
\end{corollary}
\begin{proof}
	The first condition makes that we are not studying a complete graph then the diameter is not 1.
	And we have the proof simply using \eqref{th:linkMP} with $pN = M$ and combining it with the previous theorem \eqref{th:diam2} we have the result.
\end{proof}

Now that we know that the diameter is spread on two values we are interested in restricting it on only one value. 
We may observe that theorem \eqref{th:diam2} gives us enough information to do so.
Indeed, we see that if we forced $c \to 0$ then the diameter would be $d$. This would translate in \eqref{eq:pdiam} as
\begin{equation}
	p^dn^{d-1} - 2\log(n) \longrightarrow \infty
\end{equation}
But this restriction is not sufficient because the values of the diameter might be smaller than $d$ then we will need to make sure that the probability that the diameter is $d-1$ goes to 0 which we could do forcing $c \to \infty$.
In the same fashion as previously this would translate as
\begin{equation}
	p^{d-1}n^{d-2} - 2 \log(n) \longrightarrow -\infty
\end{equation}

This informal reasoning gives motivation for the following corollary of \eqref{th:diam2} that we will now formally prove. 
\footnote{In fact it is a immediate consequence of the last part of the proof of \eqref{th:diam2} but we will prove it only assuming the theorem itself.}
\begin{corollary}\label{th:aediam}
	Suppose $d = d(n) \geq 3$ and $p = p(n)$ satisfy
	\begin{equation}
		\frac{\log(n)}{d} - 3\log\log n \to \infty
	\end{equation}
	\begin{equation}
		p^dn^{d-1} - 2\log(n) \longrightarrow \infty \quad \text{and} \quad
		p^{d-1}n^{d-2} - 2 \log(n) \longrightarrow -\infty
	\end{equation}
	Then almost every $\mathbb{G}_{n,p}$ has diameter $d$.
\end{corollary}
\begin{proof}
	Let $K_1$ and $K_2$ be positive constants and define $0<p_1<p_2<1$ by
	\begin{equation}
		p_2^{d-1}n^{d-2} = \log(\frac{n^2}{K_2}) \quad \text{and} \quad p_1^{d}n^{d-1} = \log(\frac{n^2}{K_2})	
	\end{equation}
	Then 
	\begin{equation}
		\limsup_{n \to \infty} \mathbb{P}(\text{diam}\mathbb{G}_{n,p} \geq d+1 )
		\leq  \limsup_{n \to \infty} \mathbb{P}(\text{diam}\mathbb{G}_{n,p_1} \geq d+1 ) = 1 - e^{-\frac{K_1}{2}}
	\end{equation}
	\begin{equation}
		\limsup_{n \to \infty} \mathbb{P}(\text{diam}\mathbb{G}_{n,p} \leq d-1 )
		\leq  \limsup_{n \to \infty} \mathbb{P}(\text{diam}\mathbb{G}_{n,p_2} \leq d-1 ) = e^{-\frac{K_2}{2}}
	\end{equation}
	As we can choose an arbitrary small $K_1$ and an arbitrarily large $K_2$ the assertion follows.
\end{proof}


Now, going back to the degree-diameter problem defined in \eqref{eq:DDeltaProb}, which is of finding the largest graph with fixed diameter and maximal degree.
As we already have the Moore upper bound, we are interested in finding a lower bound, and maybe hope that it would be close from Moore's bound in order to obtain precise estimate of $n(D, \Delta)$.
To obtain such a bound here we will make use of the probabilistic method by sampling on an appropriate graph space (with known diameter and max degree),
we will show that a specified property appears with positive probability, which ensures the existence of at least one graph satisfying the property.
\newline
We admit the following result from \cite{Bollob01}.
\begin{theorem}\label{th:deltap}
	If $\frac{pn}{\log n} \to \infty$, then with high probability, $\mathbb{G}_{n,p}$ satisfies 
	\begin{equation}
		\Delta(\mathbb{G}_{n,p}) = (1+o(1))pn
	\end{equation}
\end{theorem}
We can obtain the following result :
\begin{theorem}
	Suppose $0<\epsilon <1$ and the sequence $(D_k), (\Delta_k)$ are such that 
	\begin{equation}
		D_k^4 \leq \Delta_k \quad \text{and} \quad D_k \to \infty
	\end{equation}
	Then if $k$ is sufficiently large,
	\begin{equation}
		n(D_k, \Delta_k) \geq \frac{((1-\epsilon)\Delta_k)^{D_k}}{2D_k\log \Delta_k}
	\end{equation}
\end{theorem}
\begin{proof}
	We consider a random graph with $\mathcal{G}_{n_k, p_k}$, with:
	\begin{equation}
		n_k = \lceil \frac{((1-\epsilon)\Delta_k)^{D_k}}{2D_k\log \Delta_k} \rceil
	\end{equation}
	\begin{equation}
		p_k = n_k^{\frac{1}{D_k} -1 }(2\log(n_k) + \log\log(n_k))^{1/D_k}
	\end{equation}
	From the previous corollary \eqref{th:aediam}, $\mathbb{G}_{n_k, p_k}$ has diameter at most $D_k$ with probability going to 1.
	By Theorem \eqref{th:deltap}, $\mathbb{G}_{n_k,p_k}$ has with high probability maximal degree $(1+o(1))p_kn_k$.
	Which gives:
	\begin{align}
		\Delta(\mathbb{G}_{n_k,p_k}) 	&=(1+o(1))n_kp_k \\
						&= (1+o(1))n_k^{1/D_k}(2\log n_k + \log\log n_k)^{1/D_k}	\\
						&= (1+o(1))\lceil{\frac{(1-\epsilon)\Delta_k}{(2D_k\log(\Delta_k))^{1/D_k}}}\rceil(2\log n_k +  \log\log n_k)^{1/D_k}\\	
						&\leq \frac{(1+o(1))(1-\epsilon) \Delta_k}{(2D_k \log\Delta_k)^{1/D_k}} (2D_k\log((1-\epsilon)\Delta_k))^{1/D_k} \\   
						&\leq (1+o(1))(1-\epsilon)\Delta_k \leq (1-\epsilon)\Delta_k \\
						&< \Delta_k	
	\end{align}	
	This provides the existence of a graph with $n_k$ vertices, diameter at most $D_k$, degree at most $\Delta_k$.
	Hence,
	\begin{equation}
		n(D_k, \Delta_k)\geq n_k \geq \frac{((1-\epsilon)\Delta_k)^{D_k}}{2D_k\log \Delta_k} 
	\end{equation}
\end{proof}
As a strengthening of this result, Bollob\'as \cite{BollobExtr} conjectured that for each $\epsilon > 0$, it should be the case that $$n(D, \Delta) > (1-\epsilon)\Delta^D$$ if $\Delta$ and $D$ are sufficiently large.
\newline
A quite recent survey on the degree-diameter problem can be found in \cite{degreeDiam}.

