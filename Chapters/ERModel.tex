\section{Different approaches of the same space}
As said in the title of the section there are different ways to approach the Erdos-Renyi model that we may call paradigms as they will give us the same kind of results but depending on the context, one might be much more convenient to use than the others.
\newline
Historically the first paper published on random graphs was from Erdos and Renyi in 1959, in which they give the following construction :
\begin{definition}
We call a random graph $\mathcal{G}_{n, M}$ having $n$ labelled vertices and $M$ edges. That is we choose at random ( with equal probability ) one of the $\binom{\binom{n}{2}}{M}$ possible graphs.
\end{definition}
One may observe that some changes in notations are made between this paraphrasing of the article of Erdos and Renyi, they are made in order to be more adapted with the modern study of random graphs. We will also adopt for the following $N = \binom{n}{2}$ to denote the total number of edges possible on $n$ labelled vertices.
\newline
We then arrive to our main model that has been the most extensively studied in the literature of random graphs, that is $\mathcal{G}_{n, p}$ on which the coin tosses are no longer fair, but the probability of drawing an edge is now $p$. And the coin tosses are still independent. Now if we denote by $e_G$ the number of edges of a graph $G$ on the vertex set $[n]$. We have :
\begin{align}
	\mathbb{P}(G) = p^{e_G}(1-p)^{N-e_G}
\end{align}
This model is called the binomial model. It is easily seen that this model is asymptotically equivalent to the first one if $Np$ is close to $M$ on several aspects.
\newline
The third model that we will investigate is on the form of a Markov process, see in Annex for a discussion on properties used here from Markov chains. 
At time 0 there is no edge and an edge is selected at random among all of the possible edges. 
At time $t$, the edge is chosen among all the edges not already present in the graph. We denote this process by $\{\mathcal{G}_{n, t}\}_t$, with $t$ the number of edges added. 
It is clear that this model is perfectly equivalent to the first model presented in the case $t = M$. This model was also introduced in 1959 by Erdos and Renyi and is usually refered to as the random graph process.
The advantage of this model is that it allows one to study properties on the verge of their realisations.
For instance, using this model Bollobas proved that a graph is fully connected, when the last connection made is between an isolated vertice and the giant component. But we will study this in the following.




\section{Connectivity}

One of the most fundamental structure of a graph will be it's number of components. Hence, the first question we will try to ask is how often a random graph in the Erdos-Renyi model is connected. It is essential to answer this question as many other questions might not make sense on a graph that is not connected ( for instance the diameter, the existence of hamiltonian paths or the stability number of the graph ). 
\newline
It is also an interesting first topic to have an insight of the kind of elegant results that arise from the study of random graphs. The main aim of this section will be to prove the following theorem in a didactic way as it is the first random graphs proof that we will study.
\begin{theorem}
Let $p = p(n) = \frac{log(n) + c}{n}$
\newline
Then $\lim_{n \to \infty} P(G \in \mathcal{G}_{n, p}\text{ is connected }) = e^{-e^{-c}}$ 
\end{theorem}
The proof of this theorem will be in two parts, first we will show that a graph will be connected if and only if there are no isolated vertices and then we will estimate the distribution that follows the number of isolated vertices.
\begin{theorem}
	With $p = \frac{log(n) + c}{n}$
	 Almost every $G \in \mathcal{G}_{n, p}$ consists of a giant component and isolated vertices. 
\end{theorem}
\begin{proof}
During this proof we will consider the random value $X_k$ that counts the number of isolated vertices of order k.
So, let's estimate the probability $P(X_2 > 0) = P(X_2 \geq 1 )$. In order to do so we will use the method of first moment.
\begin{align}
	\mathbb{P}(X_2 \geq 1) \leq \mathbb{E}(X_2) 	&= \binom{n}{2}\mathbb{P}(\text{"drawing an isolated edge"}) \\
						    	&= \binom{n}{2}p((1-p)^{n-1})^2 \\
							&\leq (\frac{ne}{2})^2p(e^{-p})^{2(n-2)} \\
						    	&= \mathcal{O}\left(n^2p \frac{ e^2 e^{-2p(n+1)} }{4}\right) \\
							&= \mathcal{O}(n^2p)
\end{align}
So it's sufficient that $p = o^{n{-2}}$ in order to have almost surely no edges in G. This is clearly satisfied by the $p$ we use in the theorem.
\newline
However, this is not sufficient to prove that there is no isolated other than vertices. We will observe that there can't have any component of size larger than $\lceil \frac{n}{2} \rceil$ that is not the largest component in the graph.
Hence, we will study, the probability that there is any component of intermediary size that is not connected to the greatest component.
\begin{align}
	\mathbb{P}(X_k \geq 1) &\leq \mathbb{E}(X_k)\quad, \forall k \geq 3 \\
				&\leq \binom{n}{k} k^{k-2} q_k\\
				&\leq \binom{n}{k} k^{k-2} p^{k-1} ((1-p)^{n-k})^k
\end{align}
In the above, $q_k$ represents the probability that a spanning tree on $k$ vertices doesn't connect to the greatest connected components. A tree on $k$ vertices having $k-1$ edges, this means in terms of probability that it must have $k-1$ "success" and on each of the $k$ vertices $n-k$ failures. Which leads to the following line.
\newline 
Now we will try to have an upper bound of the RHS such that the sum on $k$ will converges to a $\mathcal{0}(n^{-\delta})$ for some $\delta >0$.
\begin{align}
	\mathbb{P}(X_k \geq 1) 	&\leq (\frac{ne}{k})^k k^{k-2} p^{k-1} e^{-pk(n-k))}\\
				&\leq n^ke^kk^{-2} p^{k-1}e^{-(\log(n) + c)\frac{k(n-k)}{n}}\\
				&\leq n^\frac{k^2}{n} p^{k-1}e^{-c\frac{k(n-k)}{n}}\\
				&\leq s
\end{align}

\end{proof}
\section{Existence of thresholds}
One of the most surprising features on random graphs, which seems to have motivated Erdos to publish results from 1959, is the existence of thresholds. That is, for many graph properties, with a small variation on the number of edges ( in the ER model ) or on $p(n)$, the limiting probability would jump between 0 and 1.
This zone which produces great difference in limiting probability will be called a threshold.
It has been shown by Bollobas and Thomason that this is in fact not exclusive to random graphs, but true for all monotone properties on random subsets.
\begin{definition}
We will call a graph property a family of graphs that is closed under isomorphism.
\end{definition}
This means that a graph property is independent of the labelling and of the drawing of the graph.
We can refine properties in the following definition.
\begin{definition}
	A property is monotone increasing (resp. decreasing) if it's stable under the the addition (resp. removal) of an edge.
	A graph property $\mathcal{Q}$ is convex if when $ A,C \in \mathcal{Q}$ and $A\subseteq B\subseteq C$ then $B \in \mathcal{Q}$. 
\end{definition}
For instance, being connected or containing a specific subgraph are monotone increasing properties where as being planar or containing an isolated vertice are monotone decreasing. 
As an example of property that is neither monotone increasing or decreasing, we can think of being $k$-regular for some $k$ ( this means that all vertices are of degree $k$).
Having exactly $k$ isolated vertices is an example of a convex not monotone property.
\newline
Here is a theorem showing that monotone increasing properties make probability distributions on these properties also monotone increasing.
\begin{theorem}
	Suppose $\mathcal{Q}$ is a monotone increasing property and $0 \leq M_1 \leq M_2 \leq N$ and $0 \leq p_1 \leq p_2 \leq 1$.
	\newline
	Then
	$$\mathbb{P}_{M_1}(\mathcal{Q}) \leq  \mathbb{P}_{M_2}(\mathcal{Q}) \text{  and  } \mathbb{P}_{p_1}(\mathcal{Q}) \leq \mathbb{P}_{p_2}(\mathcal{Q})$$
\end{theorem}
\begin{proof}
	The first inequality is clear, as the only difference between the two spaces on which we evaluate the property $\mathcal{Q}$ is that on the RHS edges have been added, hence, the probability of realising a monotone increasing has been increased.
	\newline
	For the second inequality, let $p = \frac{p_2 - p_1}{1 - p_1}$. Let $G_1 \in \mathcal{G}_{n, p_1}, G_2 \in \mathcal{G}_{n, p}$
	\newline
	So if $G_2 = G_1 \cup G$ it's edges are chosen with probability $p_1 + p - p_1 p = p_2$. So $G_1$ is in $G_2$, the property being monotone increasing, we have $ \mathbb{P}_{p_1}(\mathcal{Q}) \leq \mathbb{P}_{p_2}(\mathcal{Q})$
\
\end{proof}
The following result follows from definition with $\mathcal{Q}$ a monotone increasing property
\begin{equation}
	\mathbb{P}(\mathcal{Q}) = \sum_{A \in \mathcal{Q}} p^{|A|}(1-p)^{N-|A|}
\end{equation}
However this result requires to know all of the elements in $\mathcal{Q}$ and as we are often interested with properties for very large $n$ this result won't be magical... However from the following lemmas it is very useful to obtain some results on the links between $\mathcal{G}_{n, p}$ mentionned in the introduction $\mathcal{G}_{n, M}$.
Indeed the following theorem shows that if we know quite accurately $\mathbb{P}_M(\mathcal{Q})$ for every $M$ close to $pN$ then we know $\mathbb{P}_p(\mathcal{Q})$ with a comparable accuracy. The converse being clearly false, for instance the property of containing M edges.

\begin{theorem}
	Suppose $\mathcal{Q}$ is any property and $0 < p = M/N< 1$ 
	\newline
	Then $\mathbb{P}_M(\mathcal{Q}) \leq 3 \sqrt{M}\mathbb{P}_p(\mathcal{Q})$
\end{theorem}
\begin{proof}
	Let $\mathcal{Q}$ be any property, then we will write $\mathcal{Q}$ as a partition based on the number of edges in each graph contained in $\mathcal{Q}$.
	\newline
	So we have
	$$\mathcal{Q} = \bigsqcup_{m=0}^{N} \mathcal{Q}_m \quad, \text{ with } \forall G \in \mathcal{Q}_m, e(G) = m$$
	$\mathbb{P}_m(\mathcal{Q}) = |\mathcal{Q}_m| \binom{N}{M}^{-1}$
	From this we can obtain, with $q = 1 - p$
	\begin{align*}
		\mathbb{P}_p(\mathcal{Q})	&= \sum_{A \in \mathcal{Q}} p^{|A|}q^{N-|A|}\\
						&= \sum_{m=0}^{N}\sum_{A \in \mathcal{Q}_m} p^{|A|}q^{N-|A|}\\
						&= \sum_{m=0}^{N}\sum_{A \in \mathcal{Q}_m} p^{m}q^{N-m}\\
						&= \sum_{m=0}^{N}|\mathcal{Q}_m|p^{m}q^{N-m}\\
						&= \sum_{m=0}^N p^mq^{N-m}\binom{N}{M}\mathbb{P}_m(\mathcal{Q}) \\
						&\geq \binom{N}{M}p^Mq^{N-M}\mathbb{P}_M(\mathcal{Q}) \\
						&\geq \mathbb{P}_M(\mathcal{Q})(e^{\frac{1}{6M}}\sqrt{2\pi p q N})^{-1}
	\end{align*}
	So we have
	\begin{equation}
		\mathbb{P}_M(\mathcal{Q}) \leq  \mathbb{P}_p(\mathcal{Q})e^{\frac{1}{6M}}\sqrt{2\pi q M}
	\end{equation}
	Observing that $q \leq 1$ and $\sqrt{2\pi}e^{\frac{1}{6}} \approx 2.961... < 3$ the proof is complete.
\end{proof}
The previous section was about connectivity in $\mathcal{G}_{n,p}$, in this section we have seen that connectivity can be characterized as a monotone increasing property.
Also it was observed that the function $p$ was somehow best possible, by that we mean that modifying it slightly would impply to only have a zero-one law. 
We call such a function $p$ a threshold ( in that case for the connectivity ). 
\newline

More formally, let $\mathcal{Q}$ a monotone increasing property,  in $\mathcal{G}_{n, p}$, we call $\hat{p} = \hat{p}(n)$ a threshold if
\begin{align}
	\mathbb{P}(\mathcal{G}_{n,p} \in \mathcal{Q}) \rightarrow \left\{\begin{array}{rl}
										0 & \text{if } p \ll \hat{p},\\
										1 & \text{if } p \gg \hat{p}.
									 \end{array}
									\right.
\end{align}

Analogously, in $\mathcal{G}_{n, M}$, we call $\hat{M} = \hat{M}(n)$ a threshold if
\begin{align}
	\mathbb{P}(\mathcal{G}_{n,M} \in \mathcal{Q}) \rightarrow \left\{\begin{array}{rl}
										0 & \text{if } M \ll \hat{M},\\
										1 & \text{if } M \gg \hat{M}.
									 \end{array}
									\right.
\end{align}
In fact, thresholds are unique with respect to the multiplication by a scalar. So for the following, we should denote a threshold for a property as the threshold.
\section{The stability number}
\section{The diameter}
