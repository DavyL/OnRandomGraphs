\section{Galton-Watson trees}
A branching process is the simplest model that can be used to describe the evolution of a population over time.
Typically in a branching process we start with one individual, consider it will create a number of individuals through his lifetime. 
This number following a distribution, we call it the offspring distribution and denote it by $\{p_i\}_0^{\infty}$ such as
\begin{equation}
	p_i = \mathbb{P}(\text{ having i child })
\end{equation}
When we say that a branching process is a "something" branching process, that means that the offspring distribution follows the "something" law (typically a Poisson branching process).
And we also denote by $Z_n$ the number of individuals in the n-th generation. 
Then if we consider that the offspring distribution doesn't depend on the generation of the individual considered we have
\begin{equation}
	Z_n = \sum_{i=1}^{Z_{n-1}}X_{n, i} \quad, \text{ with } X = \{X_{n,i}\}_{n,i} \text{,  i.i.d.}
\end{equation}
Observing this distribution, we observe that if for some generation $k_0$ we have $Z_{k_0} = 0$, then $Z_{k_0 + k} = 0$ for any $k$. We would say that that the population dies out at $k_0$ and one might be interested to study under which condition a population will die out.
It was in fact this question that was studied by Galton and Watson (TODO : CHECK AND ADD HISTORICAL DETAILS ) that led to the study of branching processes. 
Hence, we might refer to these branching processes as Galton-Watson processes or trees (GW).
We can obtain the following theorem
\begin{theorem}
	If $\mathbb{E}X \leq 1$ then the population dies out almost surely.
\end{theorem}
\begin{proof}
	TODO : ADD PROOF
\end{proof}
\begin{theorem}
	If $\mathbb{E}X > 1$ then the population survives with a non-zero probability.
\end{theorem}
\begin{proof}
	TODO : ADD PROOF
\end{proof}
We will now define an exploration process of such a branching process. We will use the model and notations from Bollob\'as and Riordan \cite{BollobRiordan12}, which is in fact an extension of the process introduced by Karp in \cite{Karp90}.
In this model we consider a graph with $n$ vertices and the exploration will take $n$ steps. 
For now we will consider the case where we are exploring a connected component.
With this process we think of vertices in three different positions, a vertex can be active, that means the algorithm knows the existence of the vertex and is evaluating it.
A vertex can be explored, in that case we can consider that the vertex has been completely evaluated and sorted, in some fashion we can forget about it.
Otherwise a vertex can be unseen, meaning that we still have no idea of what it is.
So in terms of set, we can consider that at time (step) $t$ we have :
\begin{align}
	A_t&: \text{ the set of active vertices at time $t$ } \\
	E_t&: \text{ the set of explored vertices at time $t$ } \\
	U_t&: \text{ the set of unseen vertices at time $t$ } 
\end{align}
The process starts as follows, at $t = 0$, no vertex has been seen yet, so $U_0 = V$ and the process has not started yet then $A_0 = \emptyset, E_0=\emptyset$.
In the case of a branching process we consider a rooted tree and we denote the root as $r$, so at $t = 1$, $A_1 = r, U_1 = V\backslash\{r\}, E_1 = r$.
The two-steps initialisation might seem redundant but we will use it in the future when we will extend this process to study any kind of graph. 
\newline
For the following steps, at time $t > 1$, the process is as follows, a vertex $v_t$ is picked \footnote{We use \emph{picked} in the sense that if an element $x$ is picked in  $X_{t}$ then $x \not\in X_{t+1}$.} at random in $A_{t-1}$. 
For convenience we will add the variable $\eta_t = |N_(v_t) \cap U_{t-1}|$ the number of vertices not yet seen that are neighbours from $v_t$.
And then $A_t = (N(v_t)\cap U_{t-1} ) \cup A_{t-1} \backslash \{v_t\}$
Finally, we move $v_t$ to $E_t$ and the process stops when all vertices are explored $|E_t|$ = n, equivalently $t=n$, equivalently $|U_t| = 0$.
\newline
Connecting this to a Galton-Watson tree, $\eta_t$ is the direct progeny of $v_t$ so $\eta_t$ follows the offspring distribution defined by $X_{1, 1}$ and we consider that the population dies out only if the algorithm finishes.
\begin{align}\label{activatedV}
	\left\{\begin{array}{rl}
			|A_0| &= 1\\
			|A_i| &= |A_{i-1}| + \eta_i - 1 = \eta_1 + ... + \eta_i - ( i - 1)
	 \end{array}
	\right.
\end{align}
With all the $\eta_i$ independent and identically distributed random variables.
We can then define $T$ as the instant the population dies out.
\begin{equation}
	T := \min\{t: A_t = 0\}
\end{equation}
If $T = \infty$ then we say that the population survives. 
\newline
We have that $\{A_t\}_t$ is a random walk on the tree and we also get the following evolution equation 
\begin{equation}\label{eq:St}
	|S_t| = |S_{t-1}| + \eta_t - 1
\end{equation}
\newline
When we are considering the Erd\H{o}s Renyi model, we consider that in this random walk each vertex $v$ has a probability $p$ of turning active. 
Studying the random walk in that case we observe that the number of vertices to which we can connect the vertex $v$ is
\begin{equation}
	|U_t| = n - |E_{t-1}| - |A_{t-1}| = n - (t-1) - |A_{t-1}|
\end{equation}
So we have
\begin{equation}
	\eta_t \sim \text{Bin}( n - (t-1) - |A_{t-1}|, p)
\end{equation}
and we observe comparing it to \ref{activatedV} that our random values $\eta_i$ are no longer independently distributed. 
However we notice that if $A_{t-1}$ is "small enough" and $n$ "large enough" the r.v. are almost independently distributed.
We also denote that the random walk we defined explores only a connected component so intuitively we want to say that if the connected components are small enough and sparse enough, then, they follow a Galton Watson process, hence our random graph would be made of Galton Watson trees.
This will be the point of this chapter, studying links between random graphs and branching process, so we will consider that our probability $p$ has to be small enough such that there is not a single connected component. From \ref{th:isolcomp} we will consider that $p=\frac{\lambda}{n}$.
\newline
As the Poisson law is more convenient to work with than the binomial, and also being it's limit in distribution, let's observe through a few theorems the connecting between Poisson and binomial branching process.
\begin{theorem}\label{th:poibin}
For a branching process with a binomial offspring distribution with parameter $n$ and $p$ and a branching process with a Poisson offspring distribution with parameter $\lambda = np$
	\begin{equation}
		\mathbb{P}_{n,p}(T \geq k) = \mathbb{P}_{\lambda}^*(T^* \geq k) + e_k(n)\quad, \forall k \geq 1
	\end{equation}
	With $T$ (resp. $T^*$) the size of the binomial (resp. Poisson) resulting branching process.
	And 
	\begin{equation}
		|e_k(n)| \leq \frac{2\lambda ^2}{n}\sum_{s=1}^{k-1} \mathbb{P}_{\lambda}^*(T^* \geq s) \leq \frac{2\lambda ^2k}{n}
	\end{equation}
\end{theorem}
\begin{proof}
	The proof makes use of couplings, see Annex (TODO : WRITE ON COUPLINGS IN THE ANNEX), they are a way to define distinct random values on a common probability space giving information on the inter-twinning of both.
	\begin{equation}
		X_i\sim\text{Bin}(n, \frac{\lambda}{n})\quad,\quad X_i^* \sim\text{Poi}(\lambda)
	\end{equation}
	If we denote by $\mathbb{P}$ the joint probability distribution of $X_i$ and $X_i^*$n then according to the theorem (TODO : ADD THE THEOREM IN ANNEX + PROVE IT) then
	\begin{equation}
		\mathbb{P}(X_i \neq X_i^*) \leq \frac{\lambda^2}{n}
	\end{equation}
	Also, 
	\begin{align}
		\mathbb{P}_{n,p}(t\leq k) &= \mathbb{P}(T\geq k, T^* \geq k) +\mathbb{P}(T\geq k, T^* < k)\\
		\mathbb{P}_{\lambda}^*(t\leq k) &= \mathbb{P}(T\geq k, T^* \geq k) +\mathbb{P}(T< k, T^* \geq k)
	\end{align}
	Which gives,
	\begin{align}
		|\mathbb{P}_{n, p}&(T \geq k) -\mathbb{P}_{\lambda}^*(T^* \geq k)|\\
				  &\leq  \mathbb{P}(T\geq k, T^* < k) +\mathbb{P}(T< k, T^* \geq k)
	\end{align}
	The following part, until the end of the proof, is valid if we exchange $T$ by $T^*$ and $X$ by $X^*$.
	\newline
	By construction of $T$, the event $\{T\geq k\}$ is only defined by the events $X_1, ...., X_{k-1}$
	Then we have $T\geq k$ and $T^*<k$ if there exists some $s$ such as $X_s \neq X_s^*$.
	Hence, 
	\begin{equation}
		\mathbb{P}(T\geq k, T^*<k) \leq \sum_{s=1}^{k-1}\mathbb{P}(T \geq K, X_i \neq X_i^*, \forall i \leq s-1, X_s \neq X_s^*)
	\end{equation}
	If we are in the event,$T \geq K, X_i \neq X_i^*, \forall i \leq s-1$ then
	\begin{equation}
		X_1^*+...+X_i^* \geq i, \forall i \leq s-1
	\end{equation}
	In particular,
	\begin{equation}
		X_1^*+....+X_s^* = T^* - 1 \geq s-1
	\end{equation}
	Then the event $\{T^* \geq s\}$ depends only on the $X_i^*, i\leq s-1$ thus it is independent of the event $X_s \neq X_s^*$.
	Combining these elements we obtain,
	\begin{align}
		\mathbb{P}(T\geq k, T^* < k) &\leq \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s, X_s \neq X_s^*) 	\\
					     &\leq  \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s)(X_s \neq X_s^*) 	\\
					     &\leq  \frac{\lambda ^2}{n} \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s) 
	\end{align}
	The last inequality being obtained by the theorem on couplings (TODO : ADD REFERENCE ).
	Using the remark on the fact that this portion of the proof is valid for both the binomial and the Poisson case we obtain the following inequality that finishes the proof.
	\begin{equation}
		e_k(n) = |\mathbb{P}_{n, p}(T \geq k) -\mathbb{P}_{\lambda}^*(T^* \geq k)| \leq \frac{2\lambda ^2}{n}\sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s)
	\end{equation}
\end{proof}
The last theorem gives us some kind of "point wise" convergence between Poisson and binomial branching for trees to be larger than some fixed constant.
\newline
Now we want to investigate the typical size of a connected component in $\mathcal{G}_{n,p}$. We denote the connected comported of a vertex $v$ by $\mathcal{C}(v)$ and as a typical component we take $\mathcal{C}(1)$.
\begin{theorem}
	For all $k\geq1$,
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)| \geq k) \leq \mathbb{P}_{n,p}(T\geq k)
	\end{equation}
\end{theorem}
\begin{proof}
	We consider $|U_i| = n - i - |A_i|$ the number of vertices at step $t = i$ and the random variable $X_i$ that is the offspring distribution on vertices that make the branching process stay a tree.
	And $Y_i$ the converse of $X_i$ that is the offspring distribution for elements that have already appeared in our branching process. We then have
	\begin{equation}
		X_i \sim \text{Bin}(|U_{i-1}|, p) \quad, \quad Y_i \sim \text{Bin}(n-|U_{i-1}|, p)
	\end{equation}
	And we construct the random variable $X_i^{\geq} = X_i + Y_i$ Then we have that $X_i{\geq} ~ \text{Bin}(n, p)$ if $X_i$ and $Y_i$ are independent, so it is true if we consider $X_i{\geq}$ conditionally on $\{X_j\}_{j=1}^{i-1}$. Which gives that they are i.i.d. variables.
	\newline
	Also, as $Y_i \geq 0$, then $X_j{\geq} \geq X_i$.
	And if we denote by 
	\begin{equation}
		A_i^{\geq} = X_1 + ... + X_i^{\geq} - (i-1)
	\end{equation}
	Then, 
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)|\geq k) = \mathbb{P}(|A_t| > 0, \forall t \leq k-1) 
		\leq \mathbb{P}(|A_t^{\geq}| > 0, \forall t \leq k-1) = \mathbb{P}(T\geq k)
	\end{equation}
\end{proof}
Another similar theorem ( TODO : PROVE IT CORRECTLY  ?? ) is the following one that gives a lower bound on the size of a typical connected component although it is less useful than the previous because it is less fit for asymptotic evaluations.

\begin{theorem}\label{th:lowbin}
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)| \geq k) \geq \mathbb{P}_{n-k,p}(T\geq k)
	\end{equation}
	Here $T$ is the total progeny of a branching process with parameter $n-k$ and $p$.
\end{theorem}
Here is a sketch of the proof, the following changes shall just be plugged in the previous proof and the result will appear.
\begin{proof}
	The proof is very similar to the previous one but the following random variables are used
	\begin{equation}
		Y_i \sim \text{Bin}(N_{i-1} - (n-k), p)
	\end{equation}
	then 
	\begin{equation}
		X_i = X_i^{\leq}+Y_i
	\end{equation}
\end{proof}
Before finishing this section, here is a theorem that gives the probability law of $|A_t|$ in a random graph branching process.
\begin{theorem}\label{th:Atlaw}
	\begin{equation}
		|A_t| + (t-1) \sim \text{Bin}(n-1, 1 - (1-p)^t)
	\end{equation}
\end{theorem}
\begin{proof}
	Let's first observe by symmetry that 
	\begin{equation}
		X \sim \text{Bin}(m, p) \iff Y = m-X \sim \text{Bin}(m, 1-p)
	\end{equation}
	So to prove the theorem we will prove the equivalent statement
	\begin{equation}
		n-t-|A_t| = |U_t| \sim \text{Bin}(n-1, (1-p)^t)
	\end{equation}
	Indeed, 
	\begin{align}
		|U_t| &= n - t - |A_t| = n - t - |A_{t-1}| - |\eta_t| + 1 \\
		      &= n - (t-1) - A_{t-1} -\eta_t \\
		      &= n - (t-1) - A_{t-1} - \text{Bin}(n-(t-1)-|A_{t-1}|, p)\\
		      &=|U_{t-1}| - \text{Bin}(|U_{t-1}|, p) = \text{Bin}(|U_{t-1}|, 1-p)
	\end{align}
	Simply applying a recursion on the last result gives the expected result.
\end{proof}
Now we will apply this result to prove the following theorem.
\begin{theorem}\label{th:uplambda}
	$\mathbb{P}_{\lambda}(|\mathcal{C}(1)| >t) \leq e^{-I_{\lambda}t}$
\end{theorem}
In the previous theorem, $I_{\lambda}$ stands for the large deviation rate function and is defined as follows.
\begin{equation}
	I_{\lambda} = \lambda - 1 - \log(\lambda)
\end{equation}
It is interesting to note that $I_\lambda$ is positive if $\lambda \neq 0$
\begin{proof}
	This proof uses the fact that $|A_t| = 0$ means that the whole connected component has been explored after $t$ steps, so the connected component is of size less than $t$.
	Using \ref{th:Atlaw} we obtain that $|A_t| \sim \text{Bin}(n-1, 1-(1-p)^t) - (t-1)$, so 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq \mathbb{P}(|A_t|>0) \leq \mathbb{P}(\text{Bin}(n-1, 1-(1-p)^t) \geq t)
	\end{equation}
	Using Bernoulli's inequality \ref{bernoulli} $ 1- (1-p)^t \leq tp$ and observing that for all $s$ positive the following is true
	\begin{equation}
		\mathbb{P}(\text{Bin}(n-1, 1-(1-p)^t) \geq t) \leq \mathbb{P}(e^{s\text{Bin}(n-1, tp)} \geq e^{st}) 
	\end{equation}
	Then we can apply Markov inequality which gives, $\forall s \geq 0$ 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq e^{-st}\mathbb{E}(e^{s\text{Bin}(n, \frac{t\lambda}{n})})
	\end{equation}
	Replacing the moment generating function of the binomial with it's value \ref{binMGF}, we obtain 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq e^{-st}(1 - \frac{t\lambda}{n} + e^s\frac{t\lambda}{n})^n \leq e^{-t(s - \lambda e^s + \lambda)}
	\end{equation}
	Using $s=\log(1/\lambda)$, which minimises the bound \footnote{ Observe that as $s$ must be positive, $\lambda$ must be smaller than 1 for the argument to be true.}, we obtain
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| >t) \leq e^{-I_{\lambda}t}
	\end{equation}
\end{proof}
Now using this result we will obtain a logarithmic bound on the largest connected component.
\newline
For this we will use the random variable 
\begin{equation}
	\mathbb{Z}_{\geq k } = \sum_{v \in V} \mathbbm{1}_{|\mathcal{C}(v)| \geq k}
\end{equation}
Observing that $Z_{\geq k}$ is equal to 0 if $k$ is larger that the size of the greatest connected component \footnote{ we assume uniqueness of the greatest connected component as asymptotically it is almost surely unique } and we denote it by $\mathcal{C}_{\max}$. Hence we have
\begin{equation}
	|\mathcal{C}_{\max}| = \max\{k: \mathbb{Z}_{\geq k} \geq k\}
\end{equation}
And we obtain 
\begin{equation}\label{eq:nexplambda}
	\mathbb{E}_{\lambda}(\mathbb{Z}_{\geq k}) = n \mathbb{P}_{\lambda}(|\mathcal{C}(1)| \geq k)
\end{equation}
Applying the theorem above \ref{th:uplambda} we immediately have the following result.
\begin{lemma}\label{lemmalambdainf}
	$\mathbb{P}_{\lambda}(|\mathcal{C}_{\max}| > a \log n) = \mathcal{O}(n^{-\delta}) \quad, a\geq I_{\lambda}^{-1}, \quad \delta > 0$
\end{lemma}
We will now prove the next lemma that is similar to the previous one but gives an upper bound on the greatest connected component instead (in the sub-critical regime).
\begin{lemma}\label{lemmalambdasup}
	$\mathbb{P}_{\lambda}(|\mathcal{C}_{\max}| < a\log(n)) = \mathcal{O}(n^{-\delta})\quad, a\leq I_{\lambda}^{-1}, \quad \delta >0$
\end{lemma}
\begin{proof}
	This prove will be a little bit more technical as it uses the second moment methods.
	First of all we will need an estimate of the variance on $Z_{\geq}$, for this purpose we will use the following function
	\begin{equation}
		\chi_k(\lambda) = \mathbb{E}_{\lambda}(|\mathcal{C}(v)|\mathbbm{1}_{\{|\mathcal{C}(v)| \geq k\}})
	\end{equation}
	\begin{lemma}
		$\mathbb{V}_{\lambda}(Z_{\geq k}) \leq n\chi_k(\lambda)$
	\end{lemma}
	\begin{proof}
		By definition of the variance
		\begin{equation}
			\mathbb{V}_{\lambda}(Z_{\geq k}) = \sum_{i,j \in V}(\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k)
				-\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)
		\end{equation}
		And we can split those probabilities as components form an obvious partition of the vertex set as follow
		\begin{equation}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k) 
			= (\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, i \leftrightarrow j) 
			+ (\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k, i \not\leftrightarrow j)
		\end{equation}
		Furthermore, 
		\begin{align}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k) 
			&= \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l, |\mathcal{C}(j)| \geq k) \\
			&= \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l)\mathbb{P}( |\mathcal{C}(j)| \geq k | |\mathcal{C}(i)| = l) 
		\end{align}
		And simply observing that in $\mathcal{G}_{n,p}$, the event $\{|\mathcal{C}(j)| \geq k\}$ naturally increases with $n$, thus decreases if $l$ vertices are removed, we have in the event where $i$ and $j$ are not connected.
		\begin{align}\label{negval}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k | i \not\leftrightarrow j) 
			&\leq \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l)\mathbb{P}( |\mathcal{C}(j)| \geq k ) \\
			&\leq \mathbb{P}( |\mathcal{C}(j)| \geq k ) \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l) \\
			&\leq \mathbb{P}( |\mathcal{C}(j)| \geq k ) \mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k) 
		\end{align}
		And in the case where $i$ and $j$ are part of the same connected component then
		\begin{equation}\label{posval}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k | i \leftrightarrow j)
			=\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)
		\end{equation}
		Which gives, removing all negative terms
		\begin{equation}
			\mathbb{V}(Z_{\geq k}) \leq \sum_{i,j \in V} \mathbb{P}_{\lambda}(|\mathcal{C}(i)|\geq k, i \leftrightarrow j)
		\end{equation}
		And using properties of the expectation (linearity and behaviour in regard of indicator function ) we have
		\begin{align}
			\mathbb{V}(Z_{\geq k}) &\leq \sum_{i \in V}\sum_{j \in V} \mathbb{E}_{\lambda} ( \mathbbm{1}_{|\mathcal{C}(i)|\geq k} \mathbbm{1}_{j \in \mathcal{C}(i)})\\
				&\leq \sum_{i \in V} \mathbb{E}_{\lambda} ( \mathbbm{1}_{|\mathcal{C}(i)|\geq k} \sum_{j\in V} \mathbbm{1}_{j \in \mathcal{C}(i)}) \\
				&\leq \sum_{i \in V} \mathbb{E}_{\lambda} ( \mathbbm{1}_{|\mathcal{C}(i)|\geq k}  |\mathcal{C}(i)|) = n\chi_k(\lambda) 
		\end{align}
	\end{proof}
	As we want to prove that $\mathbb{P}_{\lambda}(Z_{\geq k_n} = 0)$ goes to 0 for $n$ sufficiently large using Bienaymé-Tchebychev inequality, we will give an upper bound on the variance and a lower bound on the expectation of $Z_{\geq k_n}$.
	TODO : CHECK THE FOLLOWING (NOT CLEAR WHY )
	\begin{equation}
		\chi_{k_n}(\lambda) = \sum_{t=k_n}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t) 
	\end{equation}
	And using \ref{th:uplambda}\footnote{The inequality being strict the -1 appears}  we obtain
	\begin{equation}
		\chi_{k_n}(\lambda) \leq \sum_{t=k_n}^{n} e^{-I_{\lambda}(t-1)} \leq \frac{e^{-(k_n -1)I_{\lambda}}}{1-e^{-I_{\lambda}}} = \mathcal{O}(n^{-a I_{\lambda}})
	\end{equation}
	And now we need to find a lower bound on the expectation of $Z_{\geq k}$ So we will make use of \ref{eq:nexplambda} to obtain the formula of the expectation, 
	\ref{th:lowbin} to obtain a lower bound and \ref{th:poibin} to convert our lower bound in the Poisson distribution.
	So we have
	\begin{equation}
		\mathbb{E} Z_{\geq k} = n \mathbb{P}_{\lambda}(|\mathcal{C}(1)| \geq k) \geq n \mathbb{P}_{n-k, p}(T \geq k) 
		= n(\mathbb{P}_{\lambda_n}^*(T^* \geq k) + o(1) )
	\end{equation}
	$T$ and $T^*$ being the total progeny of branching process ( Binomial and Poisson), and $\lambda_n = (n-k)p = \frac{n-k}{n}p$ from \ref{th:poibin}.
	\newline
	And we can compute the term on the RHS as follows
	\begin{equation}
		\mathbb{P}_{\lambda_n}^*(T^* \geq k)
		 = \sum_{t=k}^{\infty} \mathbb{P}_{\lambda_n}^*(T^* = k)
		 = \sum_{t=k}^{\infty} \frac{(\lambda_n t)^{t-1}}{t!} e^{-\lambda_n t}
	\end{equation}
	To simplify the writings we observe $\lambda_n = (1-o(1))\lambda$ and $I_{\lambda_n} = I_{\lambda} + o(1)$, then applying Stirling's formula we obtain
	\begin{equation}
		\frac{(\lambda_n t)^{t-1}}{t!} e^{-\lambda_n t}
		= \frac{\lambda^{t-1} (1-o(1))^{t-1}t^{t-1}}{t^t} \frac{e^t}{\sqrt{2\pi t}(1 + o(1))} e^{-(1-o(1))\lambda t}	
	\end{equation}
	Simplifying we get
	\begin{equation}
		\frac{(\lambda_n t)^{t-1}}{t!} e^{-\lambda_n t}
		= \frac{1}{\lambda t^{\frac{3}{2}}}(\lambda t t^{-\lambda})^t (1 + o(1))
		= \frac{e^{-I_{\lambda} t }}{\lambda t^{\frac{3}{2}}} (1 + o(1)) \geq \frac{1}{\lambda}e^{-I_{\lambda} t}
	\end{equation}
	Now, as the summand is decreasing we can bound it by the integral as follows.
	\begin{equation}
		n\mathbb{P}_{\lambda_n}^*(T^* \geq k)
		\geq \frac{n}{\lambda} \int_{k}^{\infty} e^{-I_{\lambda} t} \mathrm{d}t
		 = \frac{e^{-I_{\lambda} k}}{\lambda I_{\lambda}}
	\end{equation}
	With $k_n = a\log(n)$ we have 
	\begin{equation}
		\mathbb{E} Z_{k_n} \geq n\mathbb{P}_{\lambda_n}^*(T^* \geq k_n) \geq n^{-I_{\lambda} a + 1}
	\end{equation}
	And finally we have
	\begin{equation}
		\mathbb{P}(Z_{k_n} = 0) \leq \frac{\mathbb{V}Z_{k_n}}{(\mathbb{E}Z_{k_n})^2} \leq \frac{\mathcal{O}(n^{-I_{\lambda}a + 1})}{n^{-2I_{\lambda}a + 2} \lambda I_{\lambda}} 
			= \mathcal{O}(n^{I_{\lambda}a - 1})
	\end{equation}
	So, if $a<I_{\lambda}^{-1}$ there is no component larger than $a\log(n)$ asymptotically almost surely.
\end{proof}
Now, simply combining the previous results \ref{lemmalambdainf} and \ref{lemmalambdasup}  We have
\begin{theorem}
	If $\lambda < 1$
	\newline
	Then
	\begin{equation}
		\frac{|\mathcal{C}_{\max}|}{\log(n)} \longrightarrow_\mathbb{P} I_{\lambda}^{-1}
	\end{equation}
\end{theorem}
Hence, in $\mathcal{G}_{n, \frac{\lambda}{n}}$ with $\lambda < 1$ the connected components grow at a logarithmic speed. We will see in the next section that this is very different from the case where $\lambda > 1$ in which components grow linearly.
This change of speed in the growth rate is called the phase transition.

\section{The exploration process, Karp's new approach}
\section{The subcritical case : $\lambda < 1 $ }
\section{The supercritical case : $\lambda > 1 $ }
First we will show that that in the supercritical case there is a component of linear size (in $n$). In order to show that we will use that the existence if there exists a path of a linear size, then it is contained in a component of linear size too.
Here we will consider $p = \frac{1 + \epsilon}{n}$. 
For the beginning we will use the recent approach from Sudakov \cite{Sudakov} which makes use of an algorithm of graph exploration called the depth first search (DFS).
We made use of the breadth first search (BFS) which when exploring a vertex added all the set of neighbours to it's active stack.
Here the approach is quite different as instead of adding all of the neighbours, we check for existence of neighbours one by one, and if one is found then the algorithms moves to this new vertex. If no adjacent vertex can be found then it goes back to the previous vertex.
\newline
More formally we will use the same partition of vertex as in the BFS, $A_t$ the set of active vertices, $E_t$ the sorted vertices that we do not have to treat anymore and $U_t$ the vertices that have not yet been added to $A_t$.
We will describe the behaviour in the case of the exploration of a random graph so we say that we feed our DFS algorithm with $X_N = \{X_i\}_i^N$ a sequence of i.i.d. random values, one for each possible edge.
So the algorithm starts at some specified vertex. From there it checks for edges using each time one of the $X_i$, the number of evaluations is our time value.
If $X_t = 1$, then the new vertex under evaluation is moved from $U_t$ into $A_t$ and the same procedure repeats. 
In the case that all possible edges from a vertex have been tested and answered negatively, then the vertex is moved to set of explored vertices of corresponding time, so it is moved from $A_t$ to $E_t$.
The algorithm stops when $A_t$ is empty.
\newline
In order for the algorithm to be able to explore all components, when $A_t$ is empty, an edge is selected from $u_t$.
\newline
The proof will make an extensive use of the depth-first search algorithm, at each step of the algorithm, when it is searching for a neighbour it is simply following a Bernoulli random variable of parameter $p$.
So we consider $X_N = \{X_i\}_i^N$ our sequence of i.i.d. random variables where each $X_i$ follows a Bernoulli of parameter $p$.
Keeping the notation from the breadth first search algorithm we obtain the following inequality.
\begin{equation}
	|A_t \cup E_t| \geq \sum_{i=1}^t X_i
\end{equation}
Because in the event $X_i = 1$, then a vertex is simply moved from $A_i$ to $E_i$. 
And for the set of active vertices we have
\begin{equation}\label{eq:At}
	|A_t| \leq 1 + \sum_{i=1}^t X_i
\end{equation}
From these two inequalities we understand that having knowledge on $X$ will give us knowledge on the behaviour of our depth first search algorithm. And we might make use of this knowledge to obtain information on our connected component.
\newline
The following simple lemma will give us information on the behaviour of our binomial. It is the only probabilistic tool that we will use to show our theorem on the growth rate of the giant component.
\begin{lemma}\label{lemmaN0}
	Let $p = \frac{1+\epsilon}{n}$ and $N_0 = \frac{\epsilon n^2}{2}$. Then, 
	\begin{equation}
		|X_{N_0} - N_0 p| \leq n^{\frac{2}{3}} \quad, \text{a.a.s.}
	\end{equation}
\end{lemma}
\begin{proof}
	Let's observe that $\mathbb{E}X_{N_0} = N_0 p = \frac{\epsilon(1+\epsilon)}{2}n$.
	Simply using Chernoff inequality ( Chung and Lu modification for binomial variables ) TODO : ADD REF + CHECK IF TRUE + CHECK IF THERE IS SIMPLER ARGUMENT
	\begin{equation}
		\mathbb{P}(|X_{N_0} - \mathbb{E}X_{N_0}| > n^{\frac{2}{3}} ) \leq 2 e^{-\frac{n^{\frac{4}{3}}}{2n}}
	\end{equation}
	So asymptotically almost surely we have 
	\begin{equation}
		|X_{N_0} - \mathbb{E}X_{N_0}| \leq n^{\frac{2}{3}}
	\end{equation}
\end{proof}
Now we can state and prove the following theorem making use of the previous lemma and of our knowledge of the depth first search algorithm.
\begin{theorem}
	Let $p = \frac{1+\epsilon}{n}$ and $N_0 = \frac{\epsilon n^2}{5}$. Then, $G \sim \mathcal{G}_{n,p}$ contains a path of length at least $\frac{\epsilon^2n}{5}$
\end{theorem}
\begin{proof}
	The proof will be done by contradiction.
	\newline
	We consider $X_N$ with parameter $p = \frac{1+\epsilon}{n}$. We claim that if $N_0 = \frac{\epsilon n^2}{2}$ then $|A_{N_0}| \geq \frac{\epsilon^2 n}{5}$.
	\newline
	Let's first show that $E_{N_0} < \frac{n}{3}$.
	\newline
	If it was not the case, elements flowing in $E$ one by one, there would exist a $t$ such that $E_t = \frac{n}{3}$. Also from \ref{eq:At} and \ref{lemmaN0} we would have
	\begin{equation}
		|A_t| \leq 1 + \sum_{i=1}^{t} X_i < 1 + n^{\frac{2}{3}} < \frac{n}{3}.
	\end{equation}
	Then using the fact that the sets used in the depth first search do not intersect we have
	\begin{equation}
		|U_t| = n - |A_t| - |E_t| \geq \frac{n}{3}
	\end{equation}
	So, we obtain that the algorithm has tested all the $|E_t||U_t| \geq \frac{n^2}{9}$ possible pairs between the set of explored vertices and not seen vertices.
	But $\frac{n^2}{9} > \epsilon\frac{n^2}{2} = N_0$ \footnote{ $\epsilon$ is small enough} and as we assumed that we are at a step $t$ of the algorithm that is less than $N_0$ we have a contradiction. 
	\newline
	We are then sure from the previous argument that $|E_{N_0}| < \frac{n}{3}$ and we claim $A_{N_0}| < \frac{\epsilon^2 n}{5}$, then $U_{N_0} \neq \emptyset$. 
	Which means that there are still elements that can be added to the connected component.
	We are going to use the same arguments as previously.
	\newline
	By lemma \ref{lemmaN0}, the number of edges ( or vertices ) added is at least $\frac{\epsilon(1+\epsilon)n}{2} - n^{\frac{2}{3}}$.
	Which gives that the number of active and explored vertices is at least as follows
	\begin{equation}
		|A_{N_0} \cup E_{N_0}| \geq \frac{\epsilon(1+\epsilon)n}{2} - n^{\frac{2}{3}}
	\end{equation}
	So $|E_{N_0}| \geq \frac{\epsilon n}{2} + \frac{3\epsilon^2n}{10} - n^{\frac{2}{3}}$ and that would mean all of the pairs between $E_{N_0}$ and $A_{N_0}$ have been explored.
	So we obtain the following set of inequalities.
	\begin{align}
		N_0 = \frac{\epsilon n^2}{2} \geq |E_{N_0}||A_{N_0}| &\geq (\frac{\epsilon n}{2} + \frac{3\epsilon^2n}{10} - n^{\frac{2}{3}} )
									(n - \frac{\epsilon n}{2} - \frac{\epsilon^2 n}{2} + n^{\frac{2}{3}})\\
									&\geq \frac{\epsilon n^2}{2} + \frac{\epsilon ^2 n^2 }{20} - o(\epsilon ^3)n^2 
									
	\end{align}
	The last inequality being only with the dominating terms ( i.e. the $n^2$ ) and for $\epsilon < 1$ it is larger than $\frac{\epsilon n^2}{2}$ Which is a contradiction.
	Then $A_{N_0}$ must be larger or equal than $\frac{\epsilon ^2n}{5}$ and observing that $A_{N_0}$ must be a path we have the result.
\end{proof}

\section{Some words on the critical case}
