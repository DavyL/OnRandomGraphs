\section{Galton-Watson trees and Karp's exploration process}
A branching process is the simplest model that can be used to describe the evolution of a population over time.
Typically in a branching process we start with one individual, consider it will create a number of individuals through his lifetime. 
The distribution of this number is called the offspring distribution and we denote it by $\{p_i\}_0^{\infty}$ such as
\begin{equation}
	p_i = \mathbb{P}(\text{ having i children })
\end{equation}
When we say that a branching process is a "something" branching process, that means that the offspring distribution follows the "something" law (typically a Poisson branching process).
And we also denote by $Z_n$ the number of individuals in the $n$-th generation. 
Then if we consider that the offspring distribution doesn't depend on the generation of the individual considered we have
\begin{equation}
	Z_n = \sum_{i=1}^{Z_{n-1}}X_{n, i} \quad, \text{ with } X = \{X_{n,i}\}_{n,i} \text{,  i.i.d.}
\end{equation}
Observing this distribution, we observe that if for some generation $k_0$ we have $Z_{k_0} = 0$, then $Z_{k_0 + k} = 0$ for any $k$. We would say that that the population dies out at $k_0$ and one might be interested to study under which condition a population will die out.
It was in fact this question that was studied by Galton and Watson in 1874 regarding the fact that aristocratic surnames seemed to disappear.
We will refer to these branching processes as Galton-Watson processes or trees (GW).
Branching processes have not only been applied to genealogy but also for such objects as genes, neutrons or cosmic rays, see \cite{Harris64}.
\newline
We define $\mu = \mathbb{E} X_{1,1} < \infty$ which we assume to be finite and different from zero.
\begin{lemma}\label{martinGW}
	$M_n = \frac{1}{\mu^n}Z_n$ is a Martingale. 
	\footnote{We take $\mathcal{F}_n = \sigma(X_{i,j} , i,j \in \mathbb{N})$ as the filtration adapted to $M_n$ }
\end{lemma}
\begin{proof}
	Recall that a Martingale satisfies for all $n\geq0$, $\mathbb{E}(|M_n|) < \infty$ and $\mathbb{E}(M_{n+1} | \mathcal{F}_n) = M_n$.
	Let's first show the integrability property.
	\begin{align}
		\mathbb{E}Z_{n+1} 	&= \mathbb{E}(X_{n,1} + X_{n,2} + ... + X_{n, Z_n}) \\
					&= \sum_{k=0}^{\infty} \mathbb{P}(Z_n = k)\mathbb{E}(X_{n,1} + ... + X_{n,k})\\
					&= \sum_{k=0}^{\infty} \mathbb{P}(Z_n = k) \mu k = \mu \mathbb{E}Z_n \\
					&= \mu ^{n+1} \mathbb{E} Z_0 = \mu^{n+1}
	\end{align}
	Then we have that $\mathbb{E}M_{n+1} = 1$.
	\newline
	Now let's prove the property on the conditional expectation.
	\begin{align}
		\mathbb{E}(Z_{n+1} | \mathcal{F}_n) 	&= \mathbb{E}( X_{n,1} + X_{n,2} + ... + X_{n, Z_n} | \mathcal{F}_n ) \\
							&= \mathbb{E}X_{n, 1} \mathbb{E}(Z_n |\mathcal{F}_n ) = \mu Z_n
	\end{align}
	And we obtain,
	\begin{equation}
		\mathbb{E}(M_{n+1} | \mathcal{F}_n) = \mathbb{E}(\frac{Z_{n+1}}{\mu^{n+1}}) = \frac{Z_n}{\mu^n} = M_n
	\end{equation}
\end{proof}
We now define the following generating function
\begin{equation}
	\phi(s) = \sum_{k=0}^{\infty} p_k s^k \quad , |s| <1
\end{equation}
Then if we define the extinction probability $\zeta$ as $\zeta = \mathbb{P}(\lim_{n \to \infty} Z_n = 0)$
we have
\begin{theorem}\label{th:ProbExtinction}
	The extinction probability is a fixed point of $\phi$.
\end{theorem}
\begin{proof}
	For this proof we will rewrite the expression for $Z_{n+1}$ as a sum of independent Galton-Watson process. 
	In order to do so we simply consider that the progeny of the second generation is a sum of $Z_1$ GW process.
	For all of the $1 \leq j \leq Z_1$ children, we denote by $Z_n(j)$ as the number of descendants of $n$-th generation of $j$.
	\begin{equation}\label{indepGW}
		Z_{n+1} = \sum_{j=1}^{Z_1} Z_n(j)
	\end{equation}
	Each of these $Z_n(j)$ is a GW process independent from the others and for each of them we can then construct $M_n(j)$ as in \eqref{martinGW}.
	As these are Martingales, using the convergence theorem of Martingales
	\footnote{See Theorem 9.4.4 from Chung74 (TODO: ADD REF) for a proof.}
	we have that they converge to some random values that we denote by $M$ and $M(j)$.
	Now if we divide both sides in \eqref{indepGW} by $\mu^{n+1}$.
	\begin{equation}
		\frac{Z_{n+1}}{\mu^{n+1}} = M_{n+1} = \mu^{-1} \sum_{j=1}^{Z_1} M_n(j)
	\end{equation}
	And by taking limits we have
	\begin{equation}
		M = \frac{1}{\mu} \sum_{j=1}^{Z_1}M(j)
	\end{equation}
	Which gives that $\mu M$ is distributed as $\sum_{j=1}^{Z_1}M(j)$ and we can finally obtain
	\begin{align}
		\mathbb{P}(M = 0) 	&= \mathbb{P}(\mu M = 0) = \mathbb{P}(\sum_{j=1}^{Z_1}M_j = 0) \\
					&= \sum_{k=1}^{\infty} p_k \mathbb{P}(\sum_{j=1}^{k} M(j) = 0 | Z_1 = k) \\
					&= \sum_{k=1}^{\infty} p_k (\mathbb{P}(M=0))^k\\
					&= \phi(\mathbb{P}(M=0))
	\end{align}
\end{proof}
Using this result we can obtain the following result
\begin{theorem}
	The extinction probability of a Poisson branching process of parameter $\lambda$ is a solution of $x = e^{-\lambda(1-x)}$.
\end{theorem}
\begin{proof}
	\begin{align}
		\phi(s) &= \sum_{k=0}^{\infty} p_k s^k = e^{-\lambda} \sum_{k=0}^{\infty} \frac{\lambda ^k}{k!} s^k\\
			&= e^{-\lambda}e^{\lambda s} = e^{-\lambda(1 -s)}
	\end{align}
\end{proof}

We can obtain the following theorem
\begin{theorem}
	If $\mathbb{E}X \leq 1$ then the population dies out almost surely.
\end{theorem}
\begin{proof}
    If $\lambda \leq 1$
	\begin{equation}
	    \phi'(s) < \phi'(1) = \lambda \leq 1
	\end{equation}
	The only fixed point is 1. Hence, the probability of survival is 0.
\end{proof}
\begin{theorem}
	If $\mathbb{E}X > 1$ then the population survives with a non-zero probability.
\end{theorem}
\begin{proof}
	If $\mathbb{E}X > 1$ then $\phi(s) < s$ when $s$ is slightly less than 1, but $\phi(0) \geq 0$, hence there must have a solution to $s = \phi(s)$ in $[0,1)$.
	By convexity of $\phi$ and Rolle's theorem this solution is unique.
	Moreover, the iterates $\phi^k(0)$ are non decreasing, which gives that the probability of survival is $\lim_{k \to \infty} \phi^k(0)$ which is the smallest fixed point of $\phi$.
\end{proof}
We will now define an exploration process of such a branching process. We will use the model and notations from Bollob\'as and Riordan \cite{BollobRiordan12}, which is in fact an extension of the process introduced by Karp in \cite{Karp90}.
In this model we consider a graph with $n$ vertices and the exploration will take $n$ steps. 
For now we will consider the case where we are exploring a connected component.
With this process we think of vertices in three different positions, a vertex can be active, that means the algorithm knows the existence of the vertex and is evaluating it.
A vertex can be explored, in that case we can consider that the vertex has been completely evaluated and sorted, in some fashion we can forget about it.
Otherwise a vertex can be unseen, meaning that we still have no idea of what it is.
So in terms of set, we can consider that at time (step) $t$ we have :
\begin{align}
	A_t&: \text{ the set of active vertices at time $t$ } \\
	E_t&: \text{ the set of explored vertices at time $t$ } \\
	U_t&: \text{ the set of unseen vertices at time $t$ } 
\end{align}
The process starts as follows, at $t = 0$, we place one randomly chosen vertex $v_0$ in $A_0$, so $U_0 = V\backslash \{v_0\}$ and $A_0 = \{v_0\}, E_0=\emptyset $.
\newline
For the following steps, at time $t \geq 1$, the process is as follows: a vertex $v_t$ is picked at random in $A_{t-1}$. 
For convenience we will define the variable $\eta_t = |N(v_t) \cap U_{t-1}|$ the number of vertices not yet seen that are neighbours from $v_t$.
And then $A_t = (N(v_t)\cap U_{t-1} ) \cup A_{t-1} \backslash \{v_t\}$.
Finally, we move $v_t$ to $E_t$ and the process stops when all vertices in the connected component of order $n$ are explored $|E_t| = n$, equivalently $t=n$, equivalently $|A_t| = 0$.
\footnote{This process can be very easily extended to explore every vertices simply by picking a vertex not yet explored when the set of active vertices is empty}
\newline
This process is called \emph{Breadth First Search} and might be referred to as BFS and we may summarise it by the following expression:
\begin{align}
    A_t &= (N(v_t) \cap U_{t-1})\cup A_{t-1} \backslash \{v_t\} \\
    U_t &= U_{t-1}\backslash N(v_t)  \\
    E_t &= E_{t-1}\cup \{v_t\}
\end{align}
\begin{figure}
    \centering
    \includesvg[%
  width=15cm,height=6cm,%inkscapelatex=false
%  inkscapeformat=pdf,
%  inkscapelatex=false,
%  distort=true,
  angle=0,
%  extractdistort=false,
%  extractangle=inherit,
]{Breadth-first-tree}%
    \caption{Order in which nodes are explored using BFS}
    \label{fig:BFS}
\end{figure}
See Figure \ref{fig:BFS} for a visual representation of the BFS \footnote{This is a visual representation of an exploration as it would be done by a computer. In this study we consider that all neighbours are explored at once.}.
Notice that we have:
\begin{align}\label{activatedV}
	\left\{\begin{array}{rl}
			|A_0| &= 1\\
			|A_i| &= |A_{i-1}| + \eta_i - 1 = \eta_1 + ... + \eta_i - ( i - 1)
	 \end{array}
	\right.
\end{align}
In this case all the $\eta_i$ are independent and identically distributed random variables.
We can then define $T$ as the instant the population dies out.
Connecting this to a Galton-Watson tree, $\eta_t$ is the direct progeny of $v_t$ so $\eta_t$ follows the offspring distribution defined by $X_{1, 1}$ and we consider that the population dies out if and only if the algorithm finishes.
We can then define $T$ as the instant the population dies out.
\begin{equation}\label{eq:defT}
	T := \min\{t: A_t = 0\}
\end{equation}
Connecting this to a Galton-Watson tree, $\eta_t$ is the direct progeny of $v_t$ so $\eta_t$ follows the offspring distribution defined by $X_{1, 1}$ and we consider that the population dies out if and only if the algorithm finishes.
If $T = \infty$ then we say that the population survives. 
\newline
We have that $\{v_t\}_t$ is a random walk on the tree under exploration and we also get the following evolution equation 
\begin{equation}\label{eq:St}
	|A_t| = |A_{t-1}| + \eta_t - 1
\end{equation}
\newline
When we are considering the Erd\H{o}s Renyi model as a random graph process, we consider that in this random walk each vertex $v$ has a probability $p$ of turning active. 
Studying the random walk in that case we observe that the number of vertices to which we can connect the vertex $v$ is
\begin{equation}
	|U_{t-1}| = n - |E_{t-1}| - |A_{t-1}| = n - (t-1) - |A_{t-1}|
\end{equation}
So we have, conditionally on $|A_{t-1}|$,
\footnote{This is the conditional law on the number of neighbour of a vertex knowing $|A_{t-1}|$}
\begin{equation}
	\eta_t \sim \text{Bin}( n - (t-1) - |A_{t-1}|, p)
\end{equation}
and we observe comparing it to \eqref{activatedV} that our random values $\eta_i$ are no longer independently distributed. 
However we notice that if $A_{t-1}$ is "small enough" and $n$ "large enough" the r.v. are "almost independently distributed".
We also denote that the random walk we defined explores only a connected component so intuitively we want to say that if the connected components are small enough and sparse enough, then, they follow a Galton Watson process, hence our random graph would be made of Galton Watson trees.
This will be the point of this chapter, studying links between random graphs and branching process, so we will consider that our probability $p$ has to be small enough such that there is not a single connected component. From Theorem \ref{th:isolcomp} we will consider that $p=\frac{\lambda}{n}$.
\newline
As the Poisson law is more convenient to work with than the binomial, and also being its limit in distribution, let's observe through a few theorems the connecting between Poisson and binomial branching process.
\begin{theorem}\label{th:poibin}
For a branching process with a binomial offspring distribution with parameter $n$ and $p$ and a branching process with a Poisson offspring distribution with parameter $\lambda = np$
	\begin{equation}
		\mathbb{P}_{n,p}(T \geq k) = \mathbb{P}_{\lambda}^*(T^* \geq k) + e_k(n)\quad, \forall k \geq 1
	\end{equation}
	with $T$ (resp. $T^*$) the total progeny of the binomial (resp. Poisson) resulting branching process, and 
	\begin{equation}
		|e_k(n)| \leq \frac{2\lambda ^2}{n}\sum_{s=1}^{k-1} \mathbb{P}_{\lambda}^*(T^* \geq s) \leq \frac{2\lambda ^2k}{n}
	\end{equation}
\end{theorem}
\begin{proof}
	From the result of the appendix \ref{th:poiIneqCoup}, one can couple independent binomial random variables $X_i \sim \text{Bin}(n, \frac{\lambda}{n})$
	and independent Poisson random variables $X_i^* \sim \text{Poi}(\lambda)$ in such a way that
	\begin{equation}
		\mathbb{P}(X_i \neq X_i^*) \leq \frac{\lambda^2}{n}.
	\end{equation}
	Also, with $T$ (resp. $T^*$) defined as in \eqref{eq:defT} for a binomial (resp. Poisson) branching process.
	\begin{align}
		\mathbb{P}_{n,p}(T\leq k) &= \mathbb{P}(T\geq k, T^* \geq k) +\mathbb{P}(T\geq k, T^* < k)\\
		\mathbb{P}_{\lambda}^*(T^*\leq k) &= \mathbb{P}(T\geq k, T^* \geq k) +\mathbb{P}(T< k, T^* \geq k)
	\end{align}
	Which gives,
	\begin{align}
		|\mathbb{P}_{n, p}&(T \geq k) -\mathbb{P}_{\lambda}^*(T^* \geq k)|\\
				  &\leq  \mathbb{P}(T\geq k, T^* < k) +\mathbb{P}(T< k, T^* \geq k)
	\end{align}
	The following part, until the end of the proof, is valid if we exchange $T$ by $T^*$ and $X$ by $X^*$.
	\newline
	By construction of $T$, the event $\{T\geq k\}$ is only defined by the events $X_1, ...., X_{k-1}$
	Then we have $T\geq k$ and $T^*<k$ if there exists some $s$ such as $X_s \neq X_s^*$.
	Hence, 
	\begin{equation}
		\mathbb{P}(T\geq k, T^*<k) \leq \sum_{s=1}^{k-1}\mathbb{P}(T \geq K, X_i \neq X_i^*, \forall i \leq s-1, X_s \neq X_s^*)
	\end{equation}
	If we are in the event,$T \geq K, X_i \neq X_i^*, \forall i \leq s-1$ then
	\begin{equation}
		X_1^*+...+X_i^* \geq i, \forall i \leq s-1
	\end{equation}
	In particular,
	\begin{equation}
		X_1^*+....+X_s^* = T^* - 1 \geq s-1
	\end{equation}
	Then the event $\{T^* \geq s\}$ depends only on the $X_i^*, i\leq s-1$ thus it is independent of the event $X_s \neq X_s^*$.
	Combining these elements we obtain,
	\begin{align}
		\mathbb{P}(T\geq k, T^* < k) &\leq \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s, X_s \neq X_s^*) 	\\
					&\leq  \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s)\mathbb{P}(X_s \neq X_s^*) 	\\
					     &\leq  \frac{\lambda ^2}{n} \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s) 
	\end{align}
	The last inequality being obtained by the theorem on couplings \ref{th:poiIneqCoup}.
	Using the remark on the fact that this portion of the proof is valid for both the binomial and the Poisson case we obtain the following inequality which finishes the proof.
	\begin{equation}
		e_k(n) = |\mathbb{P}_{n, p}(T \geq k) -\mathbb{P}_{\lambda}^*(T^* \geq k)| \leq \frac{2\lambda ^2}{n}\sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s)
	\end{equation}
\end{proof}
The last theorem gives us some kind of "point wise" convergence between Poisson and binomial branching for trees to be larger than some fixed constant.
\newline
Now we want to investigate the typical size of a connected component in $\mathcal{G}_{n,p}$. We denote the connected comported of a vertex $v$ by $\mathcal{C}(v)$ and as a typical component we take $\mathcal{C}(1)$.
\begin{theorem}
	For all $k\geq1$,
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)| \geq k) \leq \mathbb{P}_{n,p}(T\geq k)
	\end{equation}
\end{theorem}
\begin{proof}
	Since conditionally on $|A_{t-1}|$, $\eta_t$ is binomial with parameters $n - (t-1) -|A_{t-1}| \leq n$ and $p$, 
	one can couple $(\eta_t)_{t \geq 1}$ with i.i.d. random variables $(\eta_t')_{t\geq 1}$ such that $\eta_t \sim \text{Bin}(n,p)$ and $\eta_t \leq \eta_t'$.
	\newline
	The size of the component of 1 is the first index $i$ such that $\eta_1 + \ldots + \eta_i - (i-1) \leq 0$, 
	and then it is at most the first index $i$ such that $\eta_1' + \ldots + \eta_i' - (i-1) \leq 0$.
	Hence, $|\mathcal{C}(1)|$ is smaller than the total progeny of a Galton-Watson tree with offspring distribution $\text{Bin}(n,p)$.
	\end{proof}
Another similar theorem is the following one that gives a lower bound on the size of a typical connected component.

\begin{theorem}\label{th:lowbin}
	We have
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)| \geq k) \geq \mathbb{P}_{n-k,p}(T\geq k),
	\end{equation}
	where $T$ is the total progeny of a binomial branching process with parameter $n-k$ and $p$.
\end{theorem}
\begin{proof}
	In this case, we couple $\eta_t$ with i.i.d. random variables $\eta_t''$ which are binomial with parameters $n-k$ and $p$ in such a way that $\eta_t \geq \eta_t''$ when $n-(t-1)-|A_{t-1}| \geq n-k$.
	On the event $T<k$ this condition is always satisfied since the number of unseen vertices in the graph is larger that $n-k$. 
	Since in this event, $\eta_i + \ldots + \eta_{i-1} - (i-1) \leq 0$, which implies that the total progeny of a Galton-Watson tree is smaller than $k$.
\end{proof}
We will now admit the following theorem which gives a way to obtain the probability that the total progeny of a Galton-Watson branching process is of a specified size.
\begin{theorem}[Hitting-time Theorem]
	Let $(Z_t)$ be a Galton-Watson branching process with total progeny $T$. Then,
	\begin{equation}
		\mathbb{P}(T = t) = \frac{1}{t} \mathbb{P}(X_1 + X_2 + \ldots + X_t = t-1),
	\end{equation}
	for all $t\geq 1$.
\end{theorem}
The proof is admitted as it needs to make use of Spetzner's combinatorial lemma which is out of the scope of this report. 
A complete proof can be found in Chapter 5 of \cite{Roch15}.
\newline
However we will use the hitting-time Theorem to obtain the following corollary,
\begin{corollary}\label{th:sizePoi}
	Let $(Z_t)$ be a Poisson branching process with offspring distribution $\text{Poi}(\lambda)$,
	\begin{equation}
		\mathbb{P}_{\lambda}(T^* = k) = e^{-\lambda t}\frac{(\lambda t)^{t-1}}{(t-1)!}.
	\end{equation}
\end{corollary}
To obtain the previous corollary simply notice that a sum of $t$ independent Poisson random variables of parameter $\lambda$ is a Poisson random variable of parameter $\lambda t$.
Before finishing this section, here is a theorem that gives the probability law of $|A_t|$ in a random graph branching process.
\begin{theorem}\label{th:Atlaw}
	\begin{equation}
		|A_t| + (t-1) \sim \text{Bin}(n-1, 1 - (1-p)^t)
	\end{equation}
\end{theorem}
\begin{proof}
	Let's first observe by symmetry that 
	\begin{equation}
		X \sim \text{Bin}(m, p) \iff Y = m-X \sim \text{Bin}(m, 1-p)
	\end{equation}
	So to prove the theorem we will prove the equivalent statement
	\begin{equation}
		n-t-|A_t| = |U_t| \sim \text{Bin}(n-1, (1-p)^t)
	\end{equation}
	Indeed, conditionally on $|A_{t-1}|$
	\begin{align}
		|U_t| &= n - t - |A_t| = n - t - |A_{t-1}| - |\eta_t| + 1 \\
		      &= n - (t-1) - A_{t-1} -\eta_t \\
		      &= n - (t-1) - A_{t-1} - \text{Bin}(n-(t-1)-|A_{t-1}|, p)\\
		      &=|U_{t-1}| - \text{Bin}(|U_{t-1}|, p) = \text{Bin}(|U_{t-1}|, 1-p)
	\end{align}
	Induction on the last result gives the expected result.
\end{proof}

\section{The subcritical case}
Now we will apply the results from the previous section in order to prove the following theorem, with $C_{\max}$ the size of the largest connected component of the graph $\mathcal{G}_{n, \frac{\lambda}{n}}$.
\begin{theorem}\label{th:subCritCV}
	If $\lambda < 1$
	\newline
	Then
	\begin{equation}
		\frac{C_{\max}}{\log(n)} \longrightarrow_\mathbb{P} I_{\lambda}^{-1}
	\end{equation}
\end{theorem}
\begin{theorem}\label{th:uplambda}
	$\mathbb{P}_{\lambda}(|\mathcal{C}(1)| >t) \leq e^{-I_{\lambda}t}$
\end{theorem}
In the previous theorem, $I_{\lambda}$ stands for the large deviation rate function corresponding to the Poisson random variables and is defined as follows.
\begin{equation}
	I_{\lambda} = \lambda - 1 - \log(\lambda)
\end{equation}
It is interesting to note that $I_\lambda$ is positive if $\lambda \neq 0$
\begin{proof}[Proof of Theorem \ref{th:uplambda}]
	This proof uses the fact that $|A_t| = 0$ means that the whole connected component has been explored after $t$ steps, so the connected component is of size less than $t$.
	Using Theorem \ref{th:Atlaw} we obtain that $|A_t| \sim \text{Bin}(n-1, 1-(1-p)^t) - (t-1)$, so 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq \mathbb{P}(|A_t|>0) \leq \mathbb{P}(\text{Bin}(n-1, 1-(1-p)^t) \geq t)
	\end{equation}
	Using Bernoulli's inequality \eqref{bernoulli} $ 1- (1-p)^t \leq tp$ and observing that for all $s$ positive the following is true
	\begin{equation}
		\mathbb{P}(\text{Bin}(n-1, 1-(1-p)^t) \geq t) \leq \mathbb{P}(e^{s\text{Bin}(n-1, tp)} \geq e^{st}),
	\end{equation}
	then we can apply Markov inequality which gives, $\forall s \geq 0$, 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq e^{-st}\mathbb{E}(e^{s\text{Bin}(n, \frac{t\lambda}{n})})
	\end{equation}
	Replacing the moment generating function of the binomial with its value \eqref{binMGF}, we obtain 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq e^{-st}(1 - \frac{t\lambda}{n} + e^s\frac{t\lambda}{n})^n \leq e^{-t(s - \lambda e^s + \lambda)}
	\end{equation}
	Using $s=\log(1/\lambda)$, which minimises the bound \footnote{ Observe that as $s$ must be positive, $\lambda$ must be smaller than 1 for the argument to be true.}, we obtain
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| >t) \leq e^{-I_{\lambda}t}
	\end{equation}
\end{proof}
Now using this result we will obtain a logarithmic bound on the largest connected component.
\newline
For this we will use the random variable 
\begin{equation}
	Z_{\geq k } = \sum_{v \in V} \mathbbm{1}_{|\mathcal{C}(v)| \geq k}
\end{equation}
Observing that $Z_{\geq k}$ is equal to 0 if $k$ is larger than $C_{\max}$, the size of the greatest connected components, and we denote it by $\mathcal{C}_{\max}$. Hence we have
\begin{equation}
	C_{\max} = \max\{k: Z_{\geq k} \geq k\}
\end{equation}
and we obtain 
\begin{equation}\label{eq:nexplambda}
	\mathbb{E}_{\lambda}(Z_{\geq k}) = n \mathbb{P}_{\lambda}(|\mathcal{C}(1)| \geq k)
\end{equation}
Applying theorem \ref{th:uplambda} we immediately have the following result.
\begin{lemma}\label{lemmalambdainf}
	For $a>I_{\lambda}^{-1}$, there exists $\delta > 0$ such that:
	\begin{equation}
		\mathbb{P}_{\lambda}(C_{\max} > a \log n) = \mathcal{O}(n^{-\delta}) 
	\end{equation}
\end{lemma}
We will now prove the next lemma that is similar to the previous one but gives an upper bound on the greatest connected component instead (in the sub-critical regime).
These two lemmas together imply Theorem \ref{th:subCritCV}.
\begin{lemma}\label{lemmalambdasup}
	For $a<I_{\lambda}^{-1}$, there exists $\delta >0$ such that
	\begin{equation}
		\mathbb{P}_{\lambda}(C_{\max} < a\log(n)) = \mathcal{O}(n^{-\delta})
	\end{equation}
\end{lemma}
\begin{proof}
	This proof will be a little bit more technical as it uses the second moment methods.
	First of all we will need an estimate of the variance on $Z_{\geq}$, for this purpose we will use the following function
	\begin{equation}
		\chi_k(\lambda) = \mathbb{E}_{\lambda}(|\mathcal{C}(1)|\mathbbm{1}_{\{|\mathcal{C}(1)| \geq k\}})
	\end{equation}
	\begin{lemma}
		$\mathbb{V}_{\lambda}(Z_{\geq k}) \leq n\chi_k(\lambda)$, where $\mathbb{V}$ denotes the variance.
	\end{lemma}
	\begin{proof}
		By definition of the variance
		\begin{equation}
			\mathbb{V}_{\lambda}(Z_{\geq k}) = \sum_{i,j \in V}(\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k)
				-\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)
		\end{equation}
		And we can split those probabilities as components form an obvious partition of the vertex set as follows
		\begin{equation}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k) 
			= (\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, i \leftrightarrow j) 
			+ (\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k, i \not\leftrightarrow j)
		\end{equation}
		where $i\leftrightarrow j$ means that $i$ and $j$ are in the same connected component of the graph.
		Furthermore, conditionally to $\mathcal{C}(i)$, in the event $i \not\leftrightarrow j$, the order of $\mathcal{C}(j)$ is stochastically smaller than the order of $\mathcal{C}(j)$ without conditioning.
		Hence,
		\begin{equation}
			\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k | \mathcal{C}(i), i \not\leftrightarrow j) \leq \mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k).
		\end{equation}
		Multiplying both sides by $\mathbbm{1}_{\mathcal{C}(i)| \geq k}$ we have
		\begin{align}
			\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k, |\mathcal{C}(j)| \geq k | \mathcal{C}(i), i \not\leftrightarrow j) 
			&\leq \mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)\mathbbm{1}_{|\mathcal{C}(i)| \geq k}\\
			&\leq \mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k).
		\end{align}
		We deduce:
		\begin{equation}
			\mathbb{V}(Z_{\geq k}) \leq \sum_{i,j \in V} \mathbb{P}_{\lambda}(|\mathcal{C}(i)|\geq k, i \leftrightarrow j)
		\end{equation}
		and then,
		\begin{align}
			\mathbb{V}(Z_{\geq k}) &\leq \sum_{i \in V}\sum_{j \in V} \mathbb{E}_{\lambda} ( \mathbbm{1}_{|\mathcal{C}(i)|\geq k} \mathbbm{1}_{j \in \mathcal{C}(i)})\\
				&\leq \sum_{i \in V} \mathbb{E}_{\lambda} ( \mathbbm{1}_{|\mathcal{C}(i)|\geq k} \sum_{j\in V} \mathbbm{1}_{j \in \mathcal{C}(i)}) \\
				&\leq \sum_{i \in V} \mathbb{E}_{\lambda} ( \mathbbm{1}_{|\mathcal{C}(i)|\geq k}  |\mathcal{C}(i)|) = n\chi_k(\lambda) 
		\end{align}
	\end{proof}
	As we want to prove for $k_n = \lceil a\log n\rceil$ that $\mathbb{P}_{\lambda}(Z_{\geq k_n} = 0)$ goes to 0 for $n$ going to infinity using Bienaymé-Tchebychev inequality, we use the previous upper bound on the variance and a lower bound on the expectation of $Z_{\geq k_n}$.
	\newline
	We have,
	\begin{equation}
		\chi_{k_n}(\lambda) = k_n \mathbb{P}_{\lambda}(|\mathcal{C}(1)| \geq k_n) + \sum_{t=k_n+1}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t) 
	\end{equation}
	And using Theorem \ref{th:uplambda} we obtain
	\begin{equation}
		\chi_{k_n}(\lambda) \leq k_ne^{-I_{\lambda}(k_n - 1)} +\sum_{t=k_n +1 }^{n} e^{-I_{\lambda}(t-1)} \leq \frac{e^{-(k_n -1)I_{\lambda}}}{1-e^{-I_{\lambda}}} = \mathcal{O}(n^{-u})
	\end{equation}
	for all $u < aI_{\lambda}$.
	And now we need to find a lower bound on the expectation of $Z_{\geq k}$.
	In the following inequality we make use of \eqref{eq:nexplambda}, then lemmas \ref{th:lowbin} and \ref{th:poibin}.
	So we have
	\begin{equation}
		\mathbb{E} Z_{\geq k} = n \mathbb{P}_{\lambda}(|\mathcal{C}(1)| \geq k) \geq n \mathbb{P}_{n-k, p}(T \geq k) 
		= n(\mathbb{P}_{\lambda_n}^*(T^* \geq k) + o(1) )
	\end{equation}
	$T$ and $T^*$ being the total progeny of branching process ( Binomial and Poisson), and $\lambda_n = (n-k)p = \frac{n-k}{n}p$ from \eqref{th:poibin}.
	\newline
	And we can compute the term on the RHS using the corollary \ref{th:sizePoi} as follows:
	\begin{equation}
		\mathbb{P}_{\lambda_n}^*(T^* \geq k)
		 = \sum_{t=k}^{\infty} \mathbb{P}_{\lambda_n}^*(T^* = k)
		 = \sum_{t=k}^{\infty} \frac{(\lambda_n t)^{t-1}}{t!} e^{-\lambda_n t}.
	\end{equation}
	To simplify the writings we observe $\lambda_n = (1-o(1))\lambda$ and $I_{\lambda_n} = I_{\lambda} + o(1)$, then applying Stirling's formula we obtain
	\begin{equation}
		\frac{(\lambda_n t)^{t-1}}{t!} e^{-\lambda_n t}
		= \frac{\lambda^{t-1} (1-o(1))^{t-1}t^{t-1}}{t^t} \frac{e^t}{\sqrt{2\pi t}(1 + o(1))} e^{-(1-o(1))\lambda t}.	
	\end{equation}
	Simplifying we get
	\begin{equation}
		\frac{(\lambda_n t)^{t-1}}{t!} e^{-\lambda_n t}
		= \frac{1}{\lambda t^{\frac{3}{2}}}(\lambda t t^{-\lambda})^t (1 + o(1))
		= \frac{e^{-I_{\lambda} t }}{\lambda t^{\frac{3}{2}}} (1 + o(1)) \geq \frac{1}{\lambda}e^{-I_{\lambda} t}.
	\end{equation}
	Now, as the summand is decreasing we can bound it by the integral as follows.
	\begin{equation}
		n\mathbb{P}_{\lambda_n}^*(T^* \geq k)
		\geq \frac{n}{\lambda} \int_{k}^{\infty} e^{-I_{\lambda} t} \mathrm{d}t
		 = \frac{e^{-I_{\lambda} k}}{\lambda I_{\lambda}}
	\end{equation}
	With $k_n = a\log(n)$ we have 
	\begin{equation}
		\mathbb{E} Z_{k_n} \geq n\mathbb{P}_{\lambda_n}^*(T^* \geq k_n) \geq n^{-I_{\lambda} a + 1}
	\end{equation}
	and finally we have
	\begin{equation}
		\mathbb{P}(Z_{k_n} = 0) \leq \frac{\mathbb{V}Z_{k_n}}{(\mathbb{E}Z_{k_n})^2} \leq \frac{\mathcal{O}(n^{ 1 - u})}{n^{-2I_{\lambda}a + 2} \lambda I_{\lambda}} 	
		= \mathcal{O}(n^{ -\delta})
	\end{equation}
	When $u$ is close enough to $aI_{\lambda}$ and $\delta >0$ small enough.
	\newline
	So, if $a<I_{\lambda}^{-1}$ there is no component larger than $a\log(n)$ with probability going to one.
\end{proof}
Now, simply combining the previous results \eqref{lemmalambdainf} and \eqref{lemmalambdasup} we have our proof of our main theorem in this section \eqref{th:subCritCV}.
Hence, in $\mathcal{G}_{n, \frac{\lambda}{n}}$ with $\lambda < 1$ the connected components grow at a logarithmic speed. We will see in the next section that this is very different from the case where $\lambda > 1$ in which components grow linearly.
This change of speed in the growth rate is called a phase transition.

\section{The supercritical case : $\lambda > 1 $ }
First we will show that that in the supercritical case there is a component of linear size (in $n$). In order to show this result we will use the fact that if there exists a path of a linear size, then it is contained in a component of linear size too. Then we will make this result a little bit better with a very similar proof.
\newline
In fact there exists stronger results that use the same method but make a use of martingales in order to get a really sharp result on the asymptotic size of the greatest connected component, see Bollob\'as and Riordan \cite{BollobRiordan12}.
\newline
Here we will consider $p = \frac{1 + \epsilon}{n}$. 
For the beginning we will use the recent approach from Sudakov \cite{Sudakov} which makes use of an algorithm of graph exploration called the \emph{depth first search} (DFS).
\begin{figure}
    \centering
    \includesvg[%
  width=15cm,height=6cm,%inkscapelatex=false
%  inkscapeformat=pdf,
%  inkscapelatex=false,
%  distort=true,
  angle=0,
%  extractdistort=false,
%  extractangle=inherit,
]{Depth-first-tree}%
    \caption{Order in which nodes are explored using DFS}
    \label{fig:DFS}
\end{figure}
We made use of the breadth first search (BFS) which when exploring a vertex added all the set of neighbours to its active stack.
Here the approach is quite different as instead of adding all of the neighbours, we check for existence of neighbours one by one, and if one is found then the algorithms moves to this new vertex. If no adjacent vertex can be found then it goes back to the previous vertex.
\newline
More formally we will use the same partition of vertex as in the BFS, $A_t$ the \emph{Last-In/First-Out stack}
\footnote{A LIFO stack is a set in which elements are ordered according to the time they were added in the stack. 
Only the last element added can be extracted from a LIFO stack. See Knuth TAOCP I (TODO: ADD REF ) for a detailed review of stacks, lists, ....}
of active vertices, $E_t$ the sorted vertices that we do not have to treat anymore and $U_t$ the vertices that have not yet been added to $A_t$.
The interest of using DFS here is that $A_t$ is by construction always a path.
\newline
We will describe the behaviour in the case of the exploration of a random graph so we say that we feed our DFS algorithm with $X = \{X_i\}_i^N$ a sequence of i.i.d. random values, one for each possible edge, recall that $N = \frac{n(n-1)}{2}$.
So the algorithm starts at some specified vertex. From there it checks for edges using each time one of the $X_i$, the number of evaluations is what we refer to as time.
If $X_t = 1$, then the new vertex under evaluation is moved from $U_t$ into $A_t$ and the same procedure repeats. 
In the case that all possible edges from a vertex have been tested and answered negatively, then the vertex is moved to set of explored vertices of corresponding time, so it is moved from $A_t$ to $E_t$.
The algorithm stops when $U_t$ is empty.
\newline
In order for the algorithm to be able to explore all components, when $A_t$ is empty, a vertex is selected from $U_t$.
\newline
The proof will make an extensive use of the depth-first search algorithm, at each step of the algorithm, when it is searching for a neighbour it is simply following a Bernoulli random variable of parameter $p$.
So we consider $X = \{X_i\}_i^N$ our sequence of i.i.d. random variables where each $X_i$ follows a Bernoulli of parameter $p$ in order to get an Erd\"os-Renyi random graph process.
So we obtain the following inequality, as in the event $X_i = 1$, then a vertex is simply moved from $U_i$ to $A_i$. And if there is a sequence of $X_i = 0$ then the vertices might only move from $A$ to $E$.  
\begin{equation}
	|A_t \cup E_t| \geq \sum_{i=1}^t X_i
\end{equation} 
And for the set of active vertices we have
\begin{equation}\label{eq:At}
	|A_t| \leq 1 + \sum_{i=1}^t X_i
\end{equation}
From these two inequalities we understand that having knowledge on $X$ will give us knowledge on the behaviour of our depth first search algorithm. And we might make use of this knowledge to obtain information on our connected component.
\newline
The following simple lemma will give us information on the behaviour of binomial random variables. It is the only probabilistic tool that we will use to show our theorem on the growth rate of the giant component.
\begin{lemma}\label{lemmaN0}
	Let $p = \frac{1+\epsilon}{n}$ and $N_0 = \lceil\frac{\epsilon n^2}{2}\rceil$. Then, 
	\begin{equation}
	|\sum_{i=1}^{N_0} X_{i} - N_0 p| \leq n^{\frac{2}{3}} \quad \text{with probability tending to 1 when $n\to \infty$.}
	\end{equation}
\end{lemma}
\begin{proof}
	Let's observe that $\mathbb{E}X_{N_0} = N_0 p = \frac{\epsilon(1+\epsilon)}{2}n$.
	Simply using Chernoff inequality \eqref{chernoff3}.
	\begin{equation}
		\mathbb{P}(|X_{N_0} - \mathbb{E}X_{N_0}| > n^{\frac{2}{3}} ) \leq 2 e^{-\frac{n^{\frac{4}{3}}}{2n}}
	\end{equation}
	So with high probability we have 
	\begin{equation}
		|X_{N_0} - \mathbb{E}X_{N_0}| \leq n^{\frac{2}{3}}
	\end{equation}
\end{proof}
Now we can state and prove the following theorem making use of the previous lemma and of our knowledge of the depth first search algorithm.
\begin{theorem}
	Let $p = \frac{1+\epsilon}{n}$. Then, $\mathbb{G}_{n,p}$ contains a path of length at least $\frac{\epsilon^2n}{5}$.
\end{theorem}
\begin{proof}
	The proof will be done by contradiction.
	\newline
	We consider $X_N$ with parameter $p = \frac{1+\epsilon}{n}$. We claim that if $N_0 = \frac{\epsilon n^2}{2}$ then $|A_{N_0}| \geq \frac{\epsilon^2 n}{5}$.
	\newline
	Let's first show that $|E_{N_0}| < \frac{n}{3}$.
	\newline
	If it was not the case, elements flowing in $E$ one by one, there would exist a $t$ such that $|E_t| = \lfloor\frac{n}{3}\rfloor$. Also from \eqref{eq:At} and Lemma \ref{lemmaN0} we would have with high probability for $n$ large enough,
	\begin{equation}
		|A_t| \leq 1 + \sum_{i=1}^{t} X_i < 1 + n^{\frac{2}{3}} < \frac{n}{3}.
	\end{equation}
	Then using the fact that the sets used in the depth first search do not intersect we have
	\begin{equation}
		|U_t| = n - |A_t| - |E_t| \geq \frac{n}{3}
	\end{equation}
	So, we obtain that the algorithm has tested all the $|E_t||U_t| \geq \frac{n^2}{9}$ possible pairs between the set of explored vertices and not seen vertices.
	But $\frac{n^2}{9} > \epsilon\frac{n^2}{2} = N_0$ \footnote{ $\epsilon$ is small enough} and as we assumed that we are at a step $t$ of the algorithm that is less than $N_0$ we have a contradiction. 
	\newline
	We are then sure from the previous argument that $|E_{N_0}| < \frac{n}{3}$ and we claim $|A_{N_0}| < \frac{\epsilon^2 n}{5}$, then $U_{N_0} \neq \emptyset$. 
	Which means that there are still elements that can be added to the connected component.
	We are going to use the same arguments as previously.
	\newline
	By lemma \eqref{lemmaN0}, the number of edges ( or vertices ) added is at least $\frac{\epsilon(1+\epsilon)n}{2} - n^{\frac{2}{3}}$.
	Which gives that the number of active and explored vertices is at least as follows
	\begin{equation}
		|A_{N_0} \cup E_{N_0}| \geq \frac{\epsilon(1+\epsilon)n}{2} - n^{\frac{2}{3}}
	\end{equation}
	So $|E_{N_0}| \geq \frac{\epsilon n}{2} + \frac{3\epsilon^2n}{10} - n^{\frac{2}{3}}$ and that would mean all of the pairs between $E_{N_0}$ and $A_{N_0}$ have been explored.
	So we obtain the following set of inequalities.
	\begin{align}
		N_0 = \frac{\epsilon n^2}{2} \geq |E_{N_0}||A_{N_0}| &\geq (\frac{\epsilon n}{2} + \frac{3\epsilon^2n}{10} - n^{\frac{2}{3}} )
									(n - \frac{\epsilon n}{2} - \frac{\epsilon^2 n}{2} + n^{\frac{2}{3}})\\
									&\geq \frac{\epsilon n^2}{2} + \frac{\epsilon ^2 n^2 }{20} - o(\epsilon ^3)n^2 			
	\end{align}
	The last inequality being only with the dominating terms ( i.e. the $n^2$ ) and for $\epsilon < 1$ it is larger than $\frac{\epsilon n^2}{2}$ Which is a contradiction.
	Then $A_{N_0}$ must be larger or equal than $\frac{\epsilon ^2n}{5}$ and observing that $A_{N_0}$ must be a path we have the result.
\end{proof}
In fact, we can refine this result to the size of the connected component and not simply the size of a walk with the following theorem.
\begin{theorem}
	Let $p = \frac{1+\epsilon}{n}$ and $N_0 = \frac{\epsilon n^2}{5}$. Then, $G \sim \mathcal{G}_{n,p}$ contains a connected component of size at least $\frac{\epsilon n}{2}$
\end{theorem}
This theorem can be proved using the same method as in the previous proof.
However, a much more precise result will be proved later in this report on the size of the largest component in the supercritical phase, see Theorem \ref{th:NWS}.
\paragraph{Some words on the critical phase}
We have in the two previous sections investigated the different behaviours of $\mathcal{G}_{n, \frac{\lambda}{n}}$ for fixed values of $\lambda \neq 1$.
We have seen that if $\lambda < 1$ then the largest component is growing in size as a logarithm of $n$ and the components will be trees or isolated vertices.
Moreover, if $\lambda > 1$ then many components will have merged compared to $\lambda <1$ and the largest component will be growing as a linear function of $n$.
The transition from tree components to a giant component will be made through the creation of \emph{complex} components
\footnote{A $l$-complex component is a component of $k$ vertices with $k+l$ edges for all $l \geq 1$.}
in the critical window $\lambda(n) = 1 + \theta n^{-\frac{1}{3}}$ for some fixed $\theta \in \mathbb{R}$.
In this critical window there will be with positive probability there is never more than one complex component (Janson, Knuth, Luczak and Pittel \cite{JansonKnuth}, proved that this probability is exactly $\frac{5\pi}{18}$), and the largest component will be of size about $n^{2/3}$.
A brief introductory review on the phase transition of Erd\"os-Renyi random graphs and other settings in which a similar phase transition can be observed (bond percolation, $d$-regular random graphs, specified degree sequence, ...) can be found in \cite{Spencer09}.


