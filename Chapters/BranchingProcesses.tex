\section{Galton-Watson trees}
A branching process is the simplest model that can be used to describe the evolution of a population over time.
Typically in a branching process we start with one individual, consider it will create a number of individuals through his lifetime. 
This number following a distribution, we call it the offspring distribution and denote it by $\{p_i\}_0^{\infty}$ such as
\begin{equation}
	p_i = \mathbb{P}(\text{ having i child })
\end{equation}
When we say that a branching process is a "something" branching process, that means that the offspring distribution follows the "something" law (typically a Poisson branching process).
And we also denote by $Z_n$ the number of individuals in the n-th generation. 
Then if we consider that the offspring distribution doesn't depend on the generation of the individual considered we have
\begin{equation}
	Z_n = \sum_{i=1}^{Z_{n-1}}X_{n, i} \quad, \text{ with } X = \{X_{n,i}\}_{n,i} \text{,  i.i.d.}
\end{equation}
Observing this distribution, we observe that if for some generation $k_0$ we have $Z_{k_0} = 0$, then $Z_{k_0 + k} = 0$ for any $k$. We would say that that the population dies out at $k_0$ and one might be interested to study under which condition a population will die out.
It was in fact this question that was studied by Galton and Watson (TODO : CHECK AND ADD HISTORICAL DETAILS ) that led to the study of branching processes. 
Hence, we might refer to these branching proces as Galton-Watson processes or trees (GW).
We can obtain the following theorem
\begin{theorem}
	If $\mathbb{E}X \leq 1$ then the population dies out almost surely.
\end{theorem}
\begin{proof}
	TODO : ADD PROOF
\end{proof}
\begin{theorem}
	If $\mathbb{E}X > 1$ then the population survives with a non-zero probability.
\end{theorem}
\begin{proof}
	TODO : ADD PROOF
\end{proof}
We will now define an exploration process of such a branching process. We will use the model of Bollobas and Riordan ( TODO : ADD REF ).
In this model we consider a graph with $n$ vertices and the exploration will take $n$ steps. 
For now we will consider the case where we are exploring a connected component.
With this process we think of vertices in three different positions, a vertex can be active, that means the algorithm knows the existence of the vertex and is evaluating it.
A vertex can be explored, in that case we can consider that the vertex has been completely evaluated and sorted, in some fashion we can forget about it.
Otherwise a vertex can be unseen, meaning that we still have no idea of what it is.
So in terms of set, we can consider that at time (step) $t$ we have :
\begin{align}
	A_t&: \text{ the set of active vertices at time $t$ } \\
	E_t&: \text{ the set of explored vertices at time $t$ } \\
	U_t&: \text{ the set of unseen vertices at time $t$ } 
\end{align}
The process starts as follow, at $t = 0$, no vertex has been seen yet, so $U_0 = V$ and the process has not started yet then $A_0 = \emptyset, E_0=\emptyset$.
In the case of a branching process we consider a rooted tree and we denote the root as $r$, so at $t = 1$, $A_1 = r, U_1 = V\backslash\{r\}, E_1 = r$.
The two-steps initialisation might seem redundant but we will use it in the future when we will extend this process to study any kind of graph. 
\newline
For the following steps, at time $t > 1$, the process is as follow, a vertex $v_t$ is picked \footnote{We use "picked" as follow, if an element $x$ is picked in  $X_{t}$ then $x \not\in X_{t+1}$.} at random in $A_{t-1}$. 
For convenience we will add the variable $\eta_t = |N_(v_t) \cap U_{t-1}|$ the number of vertices not yet seen that are neighbours from $v_t$.
And then $A_t = (N(v_t)\cap U_{t-1} ) \cup A_{t-1} \backslash \{v_t\}$
Finally, we move $v_t$ to $E_t$ and the process stops when all vertices are explored $|E_t|$ = n, equivalently $t=n$, equivalently $|U_t| = 0$.
\newline
Connecting this to a Galton-Watson tree, $\eta_t$ is the direct progeny of $v_t$ so $\eta_t$ follows the offspring distribution defined by $X_{1, 1}$ and we consider that the population dies out only if the algorithm finishes.
\begin{align}\label{activatedV}
	\left\{\begin{array}{rl}
			|A_0| &= 1\\
			|A_i| &= |A_{i-1}| + \eta_i - 1 = \eta_1 + ... + \eta_i - ( i - 1)
	 \end{array}
	\right.
\end{align}
With all the $\eta_i$ independent and identically distributed random variables.
We can then define $T$ as the instant the population dies out.
\begin{equation}
	T := \min\{t: A_t = 0\}
\end{equation}
If $T = \infty$ then we say that the population survives. 
\newline
We have that $\{A_t\}_t$ is a random walk on the tree and we also get the following evolution equation 
\begin{equation}\label{eq:St}
	|S_t| = |S_{t-1}| + \eta_t - 1
\end{equation}
\newline
When we are considering the Erdos Renyi model, in the Markovian paradigm, we consider that in this random walk each vertex $v$ has a probability $p$ of turning active. 
Studying the random walk in that case we observe that the number of vertices to which we can connect the vertex we $v$ is
\begin{equation}
	|U_t| = n - |E_{t-1}| - |A_{t-1}| = n - (t-1) - |A_{t-1}|
\end{equation}
So we have
\begin{equation}
	\eta_t ~ \text{Bin}( n - (t-1) - |A_{t-1}|, p)
\end{equation}
and we observe comparing it to \ref{activatedV} that our random values $\eta_i$ are no longer independently distributed. 
However we notice that if $A_{t-1}$ is "small enough" and $n$ "large enough" the r.v. are almost independently distributed.
We also denote that the random walk we defined explores only a connected component so intuitively we want to say that if the connected components are small enough and sparse enough, then, they follow a Galton Watson process, hence our random graph would be made of Galton Watson trees.
This will be the point of this chapter, studying links between random graphs and branching process, so we will consider that our probability $p$ has to be small enough such that there is not a single connected component. From \ref{th:isolcomp} we will consider that $p=\frac{\lambda}{n}$.
\newline
As the Poisson law is more convenient to work with than the Bernoulli, and also being it's limit in distribution, let's observe through a few theorems the connecting between Poisson and Bernoulli branching process.
\begin{theorem}
For a branching process with a binomial offspring distribution with parameter $n$ and $p$ and a branching process with a Poisson offspring distribution with parameter $\lambda = np$
	\begin{equation}
		\mathbb{P}_{n,p}(T \geq k) = \mathbb{P}_{\lambda}^*(T^* \geq k) + e_k(n)\quad, \forall k \geq 1
	\end{equation}
	With $T$ (resp. $T^*$) the size of the binomial (resp. Poisson) resulting branching process.
	And 
	\begin{equation}
		|e_k(n)| \leq \frac{2\lambda ^2}{n}\sum{s=1}^{k-1} \mathbb{P}_{lambda}^*(T^* \geq s) \leq \frac{2\lambda ^2k}{n}
	\end{equation}
\end{theorem}
\begin{proof}
	The proof makes use of couplings, see Annex (TODO : WRITE ON COUPLINGS IN THE ANNEX), they are a way to define distinct random values on a common probability space giving information on the intertwinning of both.
	\begin{equation}
		X_i~\text{Bin}(n, \frac{\lambda}{n})\quad,\quad X_i^* ~\text{Poi}(\lambda)
	\end{equation}
	If we denote by $\mathbb{P}$ the joint probability distribution of $X_i$ and $X_i^*$n then according to the theorem (TODO : ADD THE THEOREM IN ANNEX + PROVE IT) then
	\begin{equation}
		\mathbb{P}(X_i \neq X_i^*) \leq \frac{\lambda^2}{n}
	\end{equation}
	Also, 
	\begin{align}
		\mathbb{P}_{n,p}(t\leq k) &= \mathbb{P}(T\geq k, T^* \geq k) +\mathbb{P}(T\geq k, T^* < k)\\
		\mathbb{P}_{\lambda}^*(t\leq k) &= \mathbb{P}(T\geq k, T^* \geq k) +\mathbb{P}(T< k, T^* \geq k)
	\end{align}
	Which gives,
	\begin{align}
		|\mathbb{P}_{n, p}&(T \geq k) -\mathbb{P}_{\lambda}^*(T^* \geq k)|\\
				  &\leq  \mathbb{P}(T\geq k, T^* < k) +\mathbb{P}(T< k, T^* \geq k)
	\end{align}
	The following part, until the end of the proof, is valid if we exchange $T$ by $T^*$ and $X$ by $X^*$.
	\newline
	By construction of $T$, the event $\{T\geq k\}$ is only defined by the events $X_1, ...., X_{k-1}$
	Then we have $T\geq k$ and $T^*<k$ if there exists some $s$ such as $X_s \neq X_s^*$.
	Hence, 
	\begin{equation}
		\mathbb{P}(T\geq k, T^*<k) \leq \sum_{s=1}^{k-1}\mathbb{P}(T \geq K, X_i \neq X_i^*, \forall i \leq s-1, X_s \neq X_s^*)
	\end{equation}
	If we are in the event,$T \geq K, X_i \neq X_i^*, \forall i \leq s-1$ then
	\begin{equation}
		X_1^*+...+X_i^* \geq i, \forall i \leq s-1
	\end{equation}
	In particular,
	\begin{equation}
		X_1^*+....+X_s^* = T^* - 1 \geq s-1
	\end{equation}
	Then the event $\{T^* \geq s\}$ depends only on the $X_i^*, i\leq s-1$ thus it is independent of the event $X_s \neq X_s^*$.
	Combining these elements we obtain,
	\begin{align}
		\mathbb{P}(T\geq k, T^* < k) &\leq \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s, X_s \neq X_s^*) 	\\
					     &\leq  \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s)(X_s \neq X_s^*) 	\\
					     &\leq  \frac{\lambda ^2}{n} \sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s) 
	\end{align}
	The last inequality being obtained by the theorem on couplings (TODO : ADD REFERENCE ).
	Using the remark on the fact that this portion of the proof is valid for both the binomial and the Poisson case we obtain the following inequality that finishes the proof.
	\begin{equation}
		e_k(n) = |\mathbb{P}_{n, p}(T \geq k) -\mathbb{P}_{\lambda}^*(T^* \geq k)| \leq \frac{2\lambda ^2}{n}\sum_{s=1}^{k-1} \mathbb{P}(T^* \geq s)
	\end{equation}
\end{proof}
The last theorem gives us some kind of "point wise" convergence between Poisson and binomial branching for trees to be larger than some fixed constant.
\newline
Now we want to investigate the typical size of a connected component in $\mathcal{G}_{n,p}$. We denote the connected comported of a vertex $v$ by $\mathcal{C}(v)$ and as a typical component we take $\mathcal{C}(1)$.
\begin{theorem}
	For all $k\geq1$,
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)| \geq k) \leq \mathbb{P}_{n,p}(T\geq k)
	\end{equation}
\end{theorem}
\begin{proof}
	We consider $|U_i| = n - i - |A_i|$ the number of vertice at step $t = i$ and the random variable $X_i$ that is the offspring distribution on vertices that make the branching process stay a tree.
	And $Y_i$ the converse of $X_i$ that is the offspring distribution for elements that have already appeared in our branching process. We then have
	\begin{equation}
		X_i ~ \text{Bin}(|U_{i-1}|, p) \quad, \quad Y_i ~ \text{Bin}(n-|U_{i-1}|, p)
	\end{equation}
	And we construct the random variable $X_i^{\geq} = X_i + Y_i$ Then we have that $X_i{\geq} ~ \text{Bin}(n, p)$ if $X_i$ and $Y_i$ are independent, so it is true if we consider $X_i{\geq}$ conditionnally on $\{X_j\}_{j=1}^{i-1}$. Which gives that they are i.i.d. variables.
	\newline
	Also, as $Y_i \geq 0$, then $X_j{\geq} \geq X_i$.
	And if we denote by 
	\begin{equation}
		A_i^{\geq} = X_1 + ... + X_i^{\geq} - (i-1)
	\end{equation}
	Then, 
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)|\geq k) = \mathbb{P}(|A_t| > 0, \forall t \leq k-1) 
		\leq \mathbb{P}(|A_t^{\geq}| > 0, \forall t \leq k-1) = \mathbb{P}(T\geq k)
	\end{equation}
\end{proof}
Another similar theorem ( TODO : PROVE IT CORRECTLY  ?? ) is the following one that gives a lower bound on the size of a typical connected component although it is less useful than the previous because it is less fit for asymptotical evaluations.

\begin{theorem}
	\begin{equation}
		\mathbb{P}_{n,p}(|\mathcal{C}(1)| \geq k) \geq \mathbb{P}_{n-k,p}(T\geq k)
	\end{equation}
	Here $T$ is the total progeny of a branching process with parameter $n-k$ and $p$.
\end{theorem}
Here is a sketch of the proof, the following changes shall just be plugged in the previous proof and the result will appear.
\begin{proof}
	The proof is very similar to the previous one but the following random variables are used
	\begin{equation}
		Y_i \sim \text{Bin}(N_{i-1} - (n-k), p)
	\end{equation}
	then 
	\begin{equation}
		X_i = X_i^{\leq}+Y_i
	\end{equation}
\end{proof}
Before finishing this section, here is a theorem that gives the probability law of $|A_t$ in a random graph branching process.
\begin{theorem}\label{th:Atlaw}
	\begin{equation}
		|A_t| + (t-1) \sim \text{Bin}(n-1, 1 - (1-p)^t)
	\end{equation}
\end{theorem}
\begin{proof}
	Let's first observe by symmetry that 
	\begin{equation}
		X \sim \text{Bin}(m, p) \iff Y = m-X \sim \text{Bin}(m, 1-p)
	\end{equation}
	So to prove the theorem we will prove the equivalent statement
	\begin{equation}
		n-t-|A_t| = |U_t| \sim \text{Bin}(n-1, (1-p)^t)
	\end{equation}
	Indeed, 
	\begin{align}
		|U_t| &= n - t - |A_t| = n - t - |A_{t-1}| - |\eta_t| + 1 \\
		      &= n - (t-1) - A_{t-1} -\eta_t \\
		      &= n - (t-1) - A_{t-1} - \text{Bin}(n-(t-1)-|A_{t-1}|, p)\\
		      &=|U_{t-1}| - \text{Bin}(|U_{t-1}|, p) = \text{Bin}(|U_{t-1}|, 1-p)
	\end{align}
	Simply applying a recursion on the last result gives the expected result.
\end{proof}
Now we will apply this result to prove the following theorem.
\begin{theorem}\label{th:uplambda}
	$\mathbb{P}_{\lambda}(|\mathcal{C}(1)| >t) \leq e^{-I_{\lambda}t}$
\end{theorem}
In the previous theorem, $I_{\lambda}$ stands for the large deviation rate function and is defined as follow.
\begin{equation}
	I_{\lambda} = \lambda - 1 - \log(\lambda)
\end{equation}
It is interesting to note that $I_\lambda$ is positive if $\lambda \neq 0$
\begin{proof}
	This proof uses the fact that $|A_t| = 0$ means that the whole connected component has been explored after $t$ steps, so the connected component is of size less than $t$.
	Using \ref{th:Atlaw} we obtain that $|A_t| \sim \text{Bin}(n-1, 1-(1-p)^t) - (t-1)$, so 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq \mathbb{P}(|A_t|>0) \leq \mathbb{P}_{\lambda}(\text{Bin}(n-1, 1-(1-p)^t) \leq t)
	\end{equation}
	Using Bernoulli's inequality ( TODO : PROVE IT SOMEWHERE ) $ 1- (1-p)^t \leq tp$ and observing that for all $s$ positive the following is true
	\begin{equation}
		\mathbb{P}_{\lambda}(e^{s\text{Bin}(n-1, tp)} \leq e^{st}) 
	\end{equation}
	Then we can apply Markov inequality which gives 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq e^{-st}\mathbb{E}_{\lambda}(e^{s\text{Bin}(n, \frac{t\lambda}{n})})
	\end{equation}
	Replacing the moment generating function of the binomial with it's value (TODO : COMPUTE IT SOMEWHERE AND ADD REF ), we obtain 
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| > t ) \leq e^{-st}(1 - \frac{t\lambda}{n} + e^s(\frac{t\lambda}{n})^n) \leq e^{-t(s - \lambda e^s - \lambda)}
	\end{equation}
	Using $s=\log(1/\lambda)$, which minimizes the bound, we obtain
	\begin{equation}
		\mathbb{P}_{\lambda}(|\mathcal{C}(1)| >t) \leq e^{-I_{\lambda}t}
	\end{equation}
\end{proof}
From this theorem we can obtain that a typical component will be smaller than any (finite) composition of lograithm (TODO : CHECK THIS ) on $n$ almost surely.Now using this result we will obtain a logarithmic bound on the largest connected component.
\newline
For this we will use the random variable 
\begin{equation}
	\mathbb{Z}_{\leq k } = \sum_{v \in V} \mathbbm{1}{|\mathcal{C}(v)| \leq k}
\end{equation}
Observing that $\mathcal{Z}_{\leq k}$ is equal to 0 if $k$ is larger that the size of the greatest connected component \footnote{ we assume unicity on the greatest connected component as asymptotically it is almost surely unique } and we denote it by $\mathcal{C}_{\max}$. Hence we have
\begin{equation}
	|\mathcal{C}_{\max}| = \max\{k: \mathbb{Z}_{\leq k} \leq k\}
\end{equation}
And we obtain 
\begin{equation}
	\mathbb{E}_{\lambda}(\mathbb{Z}_{\leq k}) = n \mathbb{P}_{\lambda}(|\mathcal{C}(1)| \leq k)
\end{equation}
Applying the theorem above \ref{th:uplambda} we immediately have the following result.
\begin{lemma}
	$\mathbb{P}_{\lambda}(|\mathcal{C}_{\max}| \geq a \log n) \longrightarrow_{n \to \infty} 0 \quad, a>I_{\lambda}^{-1}$
\end{lemma}
We will now prove the next lemma that is similar to the previous one but gives an upper bound on the greatest connected component instead (in the subcritical regime).
\begin{lemma}
	$\mathbb{P}_{\lambda}(|\mathcal{C}_{\max}| \leq a\log(n)) \leq \mathcal{O}(n^{-\delta})$
\end{lemma}
\begin{proof}
	This prove will be a little bit more technical as it uses the second moment methods.
	First of all we will need an estimate of the variance on $Z_{\geq}$, for this purpose we will use the following function
	\begin{equation}
		\chi_k(\lambda) = \mathbb{E}_{\lambda}(|\mathcal{C}(v)|\mathbbm{1}_{\{|\mathcal{C}(v)| \geq k\}})
	\end{equation}
	\begin{lemma}
		$\mathbb{V}_{\lambda}(Z_{\geq k}) \leq n\chi_k(\lambda)$
	\end{lemma}
	\begin{proof}
		By definition of the variance
		\begin{equation}
			\mathbb{V}_{\lambda}(Z_{\geq k}) = \sum_{i,j \in V}(\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k)
				-\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)
		\end{equation}
		And we can split those probabilities as components form an obvious partition of the vertex set as follow
		\begin{equation}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k) 
			= (\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, i \leftrightarrow j) 
			+ (\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k, i \not\leftrightarrow j)
		\end{equation}
		Furthermore, 
		\begin{align}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k) 
			&= \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l, |\mathcal{C}(j)| \geq k) \\
			&= \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l)\mathbb{P}( |\mathcal{C}(j)| \geq k | |\mathcal{C}(i)| = l) 
		\end{align}
		And simply observing that in $\mathcal{G}_{n,p}$, the event $\{|\mathcal{C}(j)| \geq k\}$ naturally increases with $n$, thus decreases if $l$ vertices are removed, we have in the event where $i$ and $j$ are not connected.
		\begin{align}\label{negval}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k | i \not\leftrightarrow j) 
			&\leq \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l)\mathbb{P}( |\mathcal{C}(j)| \geq k ) \\
			&\leq \mathbb{P}( |\mathcal{C}(j)| \geq k ) \sum_{l=k}^{n} \mathbb{P}_{\lambda}(|\mathcal{C}(i)| = l) \\
			&\leq \mathbb{P}( |\mathcal{C}(j)| \geq k ) \mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k) 
		\end{align}
		And in the case where $i$ and $j$ are part of the same connected component then
		\begin{equation}\label{posval}
			\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k | i \leftrightarrrow j)
			=\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)
		\end{equation}
		So, if we denote by $\Gamma$ the partition in connected components of $V$. Then we have the following
		\begin{align}
			\mathbb{V} \mathbb{Z}_{\geq k} &=  \sum_{C \in \Gamma} \sum_{i \in C} (\sum_{j \in C}(\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k)
			-\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)) \\
			&+\sum_{j \not \in C}(\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k, |\mathcal{C}(j)| \geq k)))
			-\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)\mathbb{P}_{\lambda}(|\mathcal{C}(j)| \geq k)) \\
			&\leq \sum_{C \in \Gamma} \sum_{i \in C} (\sum_{j \in C}\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)
			-\mathbb{P}_{\lambda}(|\mathcal{C}(i)| \geq k)^2) \\	
			&\leq \sum_{C \in \Gamma} (\mathbb{P}_{\lambda}(|C| \geq k)
			-\mathbb{P}_{\lambda}(|C| \geq k)^2) \sum_{i \in C} \sum_{j \in C} 1\\	
			&\leq \sum_{C \in \Gamma} (\mathbb{P}_{\lambda}(|C| \geq k)
			-\mathbb{P}_{\lambda}(|C| \geq k)^2) |C|^2 \\
			&\leq \sum_{C \in \Gamma} \mathbb{P}_{\lambda}(|C| \geq k) n |C|
			- \sum_{C\in\Gamma} \mathbb{P}_{\lambda}(|C| \geq k)^2 |C|^2 \\
			&\leq n \chi_k(\lambda) - \sum_{C \in \Gamma} \mathbb{P}_{\lambda}(|C| \geq k)^2 |C|^2 		
		\end{align}
		A way of seeing this proof is to consider the covariance matrix, of the random variable $\mathbbm{1}_{|\mathcal{C}(i)|\leq k}(i)$ for all vertex $i$.
		So this matrix is of the size than the adjacency matrix of the graph.
		\newline
		Observing that it is possible to permute the columns of the adjacency matrix to obtain a block decomposition of the matrix, with each block representing a connected component.
		To make it clear, in each block line or column there is at least a non-zero value. And each block containing for instance the vertex $i$ is of size $|\mathcal{C}(i)| \times |\mathcal{C}(i)|$. Outside of these blocks that we place on the diagonal all values are zero.
		\newline
		Now, if we compute the covariance matrix on this block matrix of adjacency, in each connected block the values are positive (from \ref{posval} and outside these blocks negative ( from \ref{negval} ). 
		\newline
		Then the first inequality above corresponds to taking the positive values of this matrix by taking it's trace and the third one comes from observing that in each block the values are constant from \ref{posval}. Hence we observe that $\chi_k(\lambda)$ is the trace of the covariance matrix.			
	\end{proof}
\end{proof}


\section{The exploration process, Karp's new approach}
\section{The subcritical case : $\lambda < 1 $ }
\section{The supercritical case : $\lambda > 1 $ }
\section{Some words on the critical case}
