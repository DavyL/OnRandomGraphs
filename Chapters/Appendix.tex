\section{Common inequalities and simple probabilistic results}
This section presents inequalities that are used in this report and proves most of them.
\begin{theorem}[The moment generating function of a binomial]\label{binMGF}
	If $X \sim \text{Bin}(n, p)$, then 
	\begin{equation}
		\mathbb{E}(e^{tX}) = (1 - p + e^t p)^n
	\end{equation}
\end{theorem}
\begin{proof}
	\begin{align}
		\mathbb{E}(e^{tX}) 	&= \sum_{k=1}^n e^{tk} \mathbb{P}(X =k) \\
					&= \sum_{k=1}^n \binom{n}{k} e^{tk}p^k(1-p)^k  \\
					&= \sum_{k=1}^n \binom{n}{k} (e^tp)^k(1-p)^k  \\
					&=(1 - p + e^t p)^n
	\end{align}
\end{proof}
From this theorem we can obtain the moment generating function of $Y$ following a Poisson distribution of parameter $\lambda = \lim_{n\to \infty} np$ as
\begin{equation}
	\mathbb{E}(e^{tY}) = e^{\lambda(1-e^t)}
\end{equation}
simply by taking the limit in the previous theorem.
\begin{theorem}[Convergence of factorial moments implies convergence in distribution]
	Let $X$ be a random variable with a distribution that is determined by its moments. If $X_1, X_2, ...$ are random variables with finite moments such that
	$\mathbb{E}_k(X_n) \longrightarrow \mathbb{E}_k(X)$ when $ n \to \infty$ for every integer $k \geq 1$ then
	\begin{equation}
		X_n \longrightarrow_d X
	\end{equation}
\end{theorem}
See Theorem 2.3 from \cite{Hofstad16} for a proof.
\begin{theorem}[Factorial moments of a Poisson ]\label{th:factPois}
	If $X \sim \text{Poi}(\lambda)$ then
	\begin{equation}
		\mathbb{E}_r(X) = \lambda^r
	\end{equation}
\end{theorem}
\begin{proof}
	\begin{align}
		\mathbb{E}_r(X) &= \mathbb{E}(X(X-1)\ldots(X-r+1)) \\
		&= \sum_{k=0}^{\infty} (k)_r e^{-\lambda} \frac{\lambda^k}{k!} \\
	\end{align}
	With $(k)_r = k(k-1)\ldots(k-r+1)$ and $(k)_r = 0$ if $k<r$, then,
	\begin{equation}
		\mathbb{E}_r(X) = e^{-\lambda}\sum_{j=0}^{\infty} \lambda^r \frac{\lambda^j}{j!} = \lambda^r
	\end{equation}
\end{proof}
\begin{theorem}[Bernoulli's inequality]\label{bernoulli}
	\begin{equation}
		1 - (1-p)^t \leq tp
	\end{equation}
\end{theorem}

\begin{theorem}[Stirling's formula]\label{stirling} 
	We have for all integer $n\geq 1$:
	\begin{equation}
		n! \geq (\frac{n}{e})^n ,\quad	n! \sim (\frac{n}{e})^n \sqrt{2\pi n}
	\end{equation}
\end{theorem}

\begin{corollary}[$n$ choose $k$ approximation]\label{th:NChooseK}
	\begin{equation}
		\binom{n}{k} \leq \frac{n^k}{k!} \leq (\frac{ne}{k})^k
	\end{equation}
\end{corollary}
\begin{proof}
	\begin{equation}
		\binom{n}{k} 	= \frac{n!}{k!(n-k)!} = \frac{n(n-1)\ldots(n-k+1)}{k!} \leq \frac{n^k}{k!}  
	\end{equation}
	Using $\frac{1}{k!} \leq (\frac{e}{k})^k$ we can conclude the proof.
\end{proof}
	\begin{theorem}[Markov's inequality]\label{markov}
	\begin{equation}
		\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}X}{a}
	\end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
	    \mathbb{E}X \geq \mathbb{E}(a\mathbbm{1}_{X\geq a} ) = a \mathbb{P}(X \geq a)
    \end{equation}
\end{proof}
\begin{corollary}[Chebyshev's inequality]\label{cheby} 
	For all positive random variables X, and all $a>0$,
	\begin{equation}
		\mathbb{P}(|X - \mathbb{E}X| \geq k \mathbb{V}X ) \leq \frac{1}{k^2}
	\end{equation}
\end{corollary}
\begin{proof}
    Apply \eqref{markov} using the random variable $(X-\mathbb{E}X)^2$
\end{proof}
\begin{theorem}\label{chernoff3}
	Let $X_1, \ldots, X_n$ be independent random variables (not necessarily with the same distribution).
	Assume $0 \leq X_i \leq 1$ for each $i$.
	Let $X = X_1 + \ldots X_n$ and $\mu = \mathbb{E}(X)$,
	then for any $0 \leq \epsilon \leq 1$,
	\begin{equation}
		\mathbb{P}( |X - \mathbb{E}(X)| \geq \epsilon \mu ) \leq 2\exp(-\frac{\epsilon^2}{3}\mu).
	\end{equation}
\end{theorem}
This inequality can be obtained in many ways, for instance it can be derived Chernoff's bound or Hoeffding-Azuma's inequality.
We refer the reader to Section A.1. of \cite{Alon16} for a formal proof.
\section{Couplings}
Two random variables, for instance $X$ and $Y$, are coupled if they are defined on the same probability space.
If we are given two random variables $X$ and $Y$ and we want to couple them, we define the random pair $(\hat{X}, \hat{Y})$ with a joint probability $\mathbb{P}$ 
such that the marginal distribution of $\hat{X}$ (resp. $\hat{Y}$) coincides with the distribution of $X$ (resp. $Y$).
We consider $P$ and $Q$ the probability measures associated with $X$ and $Y$.
$P$ and $Q$ are defined on a $\sigma$-algebra $\mathcal{F}$ of subsets of the countable sample space $\Omega$.
We define the \emph{total variation distance} as:
\begin{equation}
	d_{TV} = \sup_{A \in \mathcal{F}} |P(A) - Q(A)|
\end{equation}
First of all, let's observe that the absolute value can be removed as $P(A^c) - Q(A^c) = -(P(A) - Q(A))$ for any $A \in \mathcal{F}$.
using the fact that $\Omega$ is countable we can construct the set reaching the supremum as $A = \{x\in \Omega: P(x) \geq Q(x)\}$.
We have $P(A) - Q(A) = \sum_{x \in A} |P(x) - Q(x)|$
and we obtain the simple formula:
\begin{equation}
	d_{TV}(P, Q) = \frac{1}{2} \sum_{x \in \Omega} |P(x) - Q(x)|
\end{equation}
simply using the "complementary" remark above which makes that we need the one half factor otherwise we would obtain a value two times larger.
We can now obtain the following lemma,
\begin{lemma}[Coupling inequality]\label{th:coupIneq}
	With all the definitions and notations above, we have,
	\begin{equation}
		d_{TV}(P, Q) \leq \mathbb{P}(\hat{X} \neq \hat{Y}).
	\end{equation}
\end{lemma}
\begin{proof}
	For $A \in \mathcal{F}$, 
	\begin{align}
		P(A) - Q(A) &= \mathbb{E}(\mathbbm{1}(X\in A)- \mathbbm{1}(Y\in A)) \\
				   &= \mathbb{E}((\mathbbm{1}(X\in A)- \mathbbm{1}(Y\in A))\mathbbm{1}(X \neq Y))\\ 
		     &\leq \mathbb{E}(\mathbbm{1}(X \neq Y)).
	\end{align}
\end{proof}
Now let's prove the following Theorem from \cite{Hofstad16} on couplings:
\begin{theorem}
	Let $\{I_i\}_{i=1}^n$ be independent with $I_i \sim \text{Be}(p_i)$, and let $\lambda = \sum_{i=1}^n p_i$.
	Let $X = \sum_{i=1}^n I_i$ and let $Y$ be a Poisson random variable with parameter $\lambda$.
	Then there exists a coupling $(\hat{X}, \hat{Y})$ of $(X, Y)$ such that
	\begin{equation}
		\mathbb{P}(\hat{X} \neq \hat{Y}) \leq \sum_{i=1}^n p_i^2.
	\end{equation}
\end{theorem}
\begin{proof}
	Throughout this proof we will consider for $i\leq n$,  $I_i \sim \text{Be}(p_i)$ are independent, and we let $J_i \sim \text{Poi}(p_i)$ also independent.
	We define
	\begin{equation}
		p_{i,1} = \mathbb{P}(I_i = 1) = 1-p_{i,0}, \quad q_{i,x} = \mathbb{P}(J_i = x) = e^{-p_i}\frac{p_i^x}{x!}
	\end{equation}
	which are the Bernoulli and Poisson probability mass functions\footnote{The probability mass function of $X$ is the probability density function of $X$ with respect to the counting measure.}.
	\newline
	For each pair $I_i, J_i$, we consider the (maximal) coupling $(\hat{I}_i, \hat{J}_i)$:
	\begin{equation}
		\mathbb{P}(\hat{I}_i = \hat{J}_i =x) = \min\{p_{1,x}, q_{1, x}\} = \left\{	\begin{array}{rl}
												1-p_i  &\text{for $x = 0$,}\\
												p_ie^{-p_i} &\text{for $x= 1$,}\\
												0 &\text{for $x\geq 2$}\\
											\end{array}
										\right.
	\end{equation}
	Thus we have,
	\begin{equation}
		\mathbb{P}(\hat{I}_i \neq \hat{J}_i) = 1 - \mathbb{P}(\hat{I}_i = \hat{J}_i) = 1 - (1-p_i) - p_ie^{-p_i} = p_i(1-e^{-p_i}) \leq p_i^2.
	\end{equation}
	Using $\hat{X} = \sum_{i=1}^n \hat{I}_i$ and $\hat{Y} = \sum_{i=1}^n \hat{J}_i$ the proof can be easily completed.
\end{proof}
We can obtain the following result as a corollary,
\begin{corollary}\label{th:poiIneqCoup}
	For every $\lambda > 0$ and $n\in \mathbb{N}$, there exists a coupling $(\hat{X}, \hat{Y})$ where $\hat{X} \sim \text{Bin}(n, \frac{\lambda}{n})$ and 
	$\hat{Y} \sim \text{Poi}(\lambda)$ there exists a coupling $(\hat{X}, \hat{Y})$ such that 
	\begin{equation}
		\mathbb{P}(\hat{X} \neq \hat{Y}) \leq \frac{\lambda^2}{n},
	\end{equation}
which in turn implies, for every $i\in \mathbb{N}$
	\begin{equation}
		|\mathbb{P}(X = i) - \mathbb{P}(Y=i)| \leq \frac{\lambda^2}{n}.
	\end{equation}
\end{corollary}
The first claim should be clear using the previous theorem. The second claim comes from the coupling inequality \ref{th:coupIneq}.
