\section{Common inequalities and simple probabilistic results}
This section presents inequalities that are used in this report and proves most of them. (TODO : PROVE THEM)
\begin{theorem}[The moment generating function of a binomial]\label{binMGF}
	If $X \sim \text{Bin}(n, p)$, then 
	\begin{equation}
		\mathbb{E}(e^{tX}) = (1 - p + e^t p)^n
	\end{equation}
\end{theorem}
\begin{proof}
	\begin{align}
		\mathbb{E}(e^{tX}) 	&= \sum_{k=1}^n e^{tk} \mathbb{P}(X =k) \\
					&= \sum_{k=1}^n \binom{n}{k} e^{tk}p^k(1-p)^k  \\
					&= \sum_{k=1}^n \binom{n}{k} (e^tp)^k(1-p)^k  \\
					&=(1 - p + e^t p)^n
	\end{align}
\end{proof}
\begin{theorem}[Convergence of factorial moments implies convergence in distribution]
	Let $X$ be a random variable with a distribution that is determined by its moments. If $X_1, X_2, ...$ are random variables with finite moments such that
	$\mathbb{E}_k(X_n) \longrightarrow \mathbb{E}_k(X)$ when $ n \to \infty$ for every integer $k \geq 1$ then
	\begin{equation}
		X_n \longrightarrow_d X
	\end{equation}
\end{theorem}
See Theorem 2.3 from \cite{Hofstad16} for a proof.
\begin{theorem}[Factorial moments of a Poisson ]\label{th:factPois}
	If $X \sim \text{Poi}(\lambda)$ then
	\begin{equation}
		\mathbb{E}_r(X) = \lambda^r
	\end{equation}
\end{theorem}
\begin{proof}
	\begin{align}
		\mathbb{E}_r(X) &= \mathbb{E}(X(X-1)\ldots(X-r+1)) \\
		&= \sum_{k=0}^{\infty} (k)_r e^{-\lambda} \frac{\lambda^k}{k!} \\
	\end{align}
	With $(k)_r = k(k-1)\ldots(k-r+1)$ and $(k)_r = 0$ if $k<r$, then,
	\begin{equation}
		\mathbb{E}_r(X) = e^{-\lambda}\sum_{j=0}^{\infty} \lambda^r \frac{\lambda^j}{j!} = \lambda^r
	\end{equation}
\end{proof}
\begin{theorem}[Bernoulli's inequality]\label{bernoulli}
	\begin{equation}
		1 - (1-p)^t \leq tp
	\end{equation}
\end{theorem}

\begin{theorem}[Stirling's formula]\label{stirling} 
	We have for all integer $n\geq 1$:
	\begin{equation}
		n! \geq (\frac{n}{e})^n ,\quad	n! \sim (\frac{n}{e})^n \sqrt{2\pi n}
	\end{equation}
\end{theorem}

\begin{corollary}[n choose k approximation]\label{th:NChooseK}
	\begin{equation}
		\binom{n}{k} \leq \frac{n^k}{k!} \leq (\frac{ne}{k})^k
	\end{equation}
\end{corollary}
\begin{proof}
	\begin{equation}
		\binom{n}{k} 	= \frac{n!}{k!(n-k)!} = \frac{n(n-1)\ldots(n-k+1)}{k!} \leq \frac{n^k}{k!}  
	\end{equation}
	Using $\frac{1}{k!} \leq (\frac{e}{k})^k$ we can conclude the proof.
\end{proof}
	\begin{theorem}[Markov's inequality]\label{markov}
	\begin{equation}
		\mathbb{P}(X \geq a) \leq \frac{\mathbb{E}X}{a}
	\end{equation}
\end{theorem}
\begin{proof}
    \begin{equation}
	    \mathbb{E}X \geq \mathbb{E}(a\mathbbm{1}_{X\geq a} ) = a \mathbb{P}(X \geq a)
    \end{equation}
\end{proof}
\begin{corollary}[Chebyshev's inequality]\label{cheby} 
	For all positive random variables X, and all $a>0$,
	\begin{equation}
		\mathbb{P}(|X - \mathbb{E}X| \geq k \mathbb{V}X ) \leq \frac{1}{k^2}
	\end{equation}
\end{corollary}
\begin{proof}
    Apply \eqref{markov} using the random variable $(X-\mathbb{E}X)^2$
\end{proof}
Now let's prove the following Theorem on couplings:
\begin{theorem}
	Let $\{I_i\}_{i=1}^n$ be independent with $I_i \sim \text{Be}(p_i)$, and let $\lambda = \sum_{i=1}^n p_i$.
Let $X = \sum_{i=1}^n I_i$ and let $Y$ be a Poisson random variable with parameter $\lambda$.
Then there exists a coupling $(\hat{X}, \hat{Y})$ of $(X, Y)$ such that
\begin{equation}
	\mathbb{P}(\hat{X} \neq \hat{Y}) \leq \sum_{i=1}^n p_i^2.
\end{equation}
\end{theorem}
\begin{proof}
	Throughout thhis proof we will consider $I_i \sim \text{Be}(p_i)$
	See Van Der Hofstadt page 35.
\end{proof}
\section{Tail inequalities}
The following inequalities are designated as Chernoff inequalities.
\begin{theorem}[Chernoff bound]\label{chernoff1}
	If $X \sim \text{Bin}(n, p)$, and $\lambda = np$ then $\forall t \geq 0$
	\begin{equation}
		\mathbb{P}( X \geq \mathbb{E}X + t) \leq e^{-\frac{t^2}{2(\lambda + t/3)}}
	\end{equation}
	\begin{equation}
		\mathbb{P}( X \geq \mathbb{E}X + t) \leq e^{-\frac{t^2}{2\lambda}}
	\end{equation}
\end{theorem}
\begin{corollary}\label{chernoff2}
	If $X \sim \text{Bin}(n, p)$, and $\lambda = np$ then $\forall t \geq 0$
	\begin{equation}
		\mathbb{P}( |X - \mathbb{E}X| \geq t) \leq 2e^{-\frac{t^2}{2(\lambda + t/3)}}
	\end{equation}
\end{corollary}
\begin{corollary}\label{chernoff3}
	If $X \sim \text{Bin}(n, p)$, and $0<\epsilon \leq 3/2$ then
	\begin{equation}
		\mathbb{P}( |X - \mathbb{E}X| \geq \epsilon \mathbb{E}X) \leq 2e^{-\frac{\epsilon^2}{3}\mathbb{E}X }
	\end{equation}
\end{corollary}
\begin{theorem}[Binomial tail]\label{binomTail}

\end{theorem}

\section{Markov chains}
\section{Martingales}
