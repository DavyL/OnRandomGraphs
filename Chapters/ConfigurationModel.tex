\section{Generalized Binomial Graph}
We are now interested in a generalization of the ( binomial ) Erd\"os-R\'enyi model which was first considered by Kovalenko in (TODO : ADD REF ).
In this model, often referred to as \emph{inhomogeneous}, the probability of appearance of the edge $(i,j)$, that we call $p_{ij}$, is not necessarily the same for all pairs of vertices $i,j$.
\newline
From this definition it is very natural to write those edge probabilities in an $n\times n$ matrix, which we denote by
\begin{equation}
	\bold{P} = [p_{ij}]
\end{equation}
As previously we consider graphs without loops, so $p_{ii} = 0$ for all $i \in V$, we also consider that the graph is not directed so the probability that $i$ connects to $j$ has to be the same as the probability that $j$ connects to $i$ (i.e. $p_{ij} = p_{ji}$), hence $\bold{P}$ is symmetric.
In order to shorten the writing, we put $q_{ij} = 1 - p_{ij}$.
\newline
We denote the probability space of this generalised model as $\mathcal{G}_{n,\bold{P}}$ and as before we denote a random variable in this space as $\mathbb{G}_{n,\bold{P}}$.
\newline
We now define
\begin{equation}
	Q_i = \prod_{j=1}^n q_{ij}, \quad \lambda_n = \sum_{i=1}^nQ_i
\end{equation}
and we observe that $Q_i$ is the probability that vertex $i$ is isolated, hence $\lambda_n$ is the expected number of isolated vertices.
\newline
We also need to define
\begin{equation}
	R_{ik} = \min_{1\leq j_1<j_2< \ldots <j_k\leq n} q_{ij_1}q_{ij_2}\ldots q_{ij_k}
\end{equation}
In the following, we suppose that the edge probabilities $p_{ij}$ are chosen in such a way that the following equations are simultaneously satisfied as $n\to \infty$.
\begin{align}
	\max_{1\leq i \leq n} Q_i &\to 0 \label{eq:C1}\\
	\lim_{n \to \infty} \lambda_n &= \lambda = \text{constant} \label{eq:C2} \\
	\lim_{n\to \infty} \sum_{k=1}^{n/2} \frac{1}{k!}(\sum_{i=1}^n \frac{Q_i}{R_{ik}})^k &= e^{\lambda} - 1 \label{eq:C3}
\end{align}
In this section we are interested in proving the following theorem.
\begin{theorem}\label{th:CVIsolPoi}
	Let $X_0$ denote the number of isolated vertices in $\mathbb{G}_{n, \bold{P}}$. If \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} are satisfied, 
	then the number of isolated vertices is asymptotically Poisson distributed with mean $\lambda$.
	\newline
	i.e. for $k \in \mathbb{N}$, 
	\begin{equation}
		\lim_{n \to \infty} \mathbb{P}(X_0 = k) = \frac{\lambda^k}{k!}e^{-\lambda}
	\end{equation}
\end{theorem}
Let's first show as a corollary that this theorem allows us to conclude the proof of \ref{th:connect}.
\begin{corollary}\label{th:proofPoissonP}
	If $p(n) = \frac{\log n + c}{n}$, then \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} are satisfied, 
	and the number of isolated vertices is asymptotically Poisson distributed with mean $e^{-c}$.
\end{corollary}
\begin{proof}[Proof of \ref{th:proofPoissonP}]
	We observe that $\mathcal{G}_{n,p}$ and $\mathcal{G}_{n, \bold{P}}$ when $p_{ij} = p$ for all $i \neq j$.
	\newline
	We observe that $Q_i = Q = q^{n-1}$ (and \eqref{eq:C1} is obviously satisfied) and $R_{ik} = R_k = q^k$. 
	From this we get
	\begin{equation}
		\lambda_n = n Q = nq^{n-1} = n(1-p)^{n-1}
	\end{equation}
	and we have $\lim \lambda_n = e^{-c} = \lambda$, so \eqref{eq:C2} is also satisfied.
	\newline
	Let's observe that it is enough to show $\sum_{i=1}^n \frac{Q_i}{R_{ik}}$ converges uniformly to $\lambda = e^{-c}$ for \eqref{eq:C3} to be satisfied.
	\begin{equation}
		\sum_{i=1}^n \frac{Q_i}{R_{ik}} = \sum_{i=1}^n \frac{Q}{R_k} = n \frac{Q}{R_k} = nq^{n-1-k}
	\end{equation}
	And $\lim_{n \to \infty}  nq^{n-1-k} = e^{-c}$ finally proves the corollary.
\end{proof}
\begin{proof}{Proof of \ref{th:CVIsolPoi}}
	Let $Y_{ij}$ a random variable following a Bernoulli of parameter $p_{ij}$.
	\footnote{ Recall that a Bernoulli of parameter $p$ is 1 with prob. $p$ and 0 with prob. $1-p$.}
	We denote by $Y_i$ the indicator of the event that vertex $i$ is isolated, i.e. $Y_i = \mathbbm{1}_{\sum_{j=1}^n Y_{ij} = 0}$.
	In order to show the convergence of $X_0$ in distribution to the Poisson random variable we will use the method of factorial moments.
	So, we want to show that for any natural number $k$ we have
	\begin{equation}\label{eq:EIsolMom}
		\mathbb{E}(\sum_{1\leq i_1<i_2< \ldots < i_k \leq n} X_{i_1}X_{i_2}\ldots X_{i_k}) \to_{n \to \infty} \frac{\lambda^k}{k!}
	\end{equation}
	Let's observe that the LHS of \eqref{eq:EIsolMom} is the sum of $\mathbb{E}(X_{i_1}X_{i_2}\ldots X_{i_k})$ over all $i_1 < i_2 < \ldots i_k$.
	\begin{align}
		\mathbb{E}(X_{i_1}X_{i_2}\ldots X_{i_k}) 	&= \prod_{r=1}^k \mathbb{P}(X_{i_r} = 1 | X_{i_1} = \ldots = X_{i_{r-1}} = 1) \\
							&= \prod_{r=1}^k\frac{ \prod_{j=1}^n q_{i_r j} }{ \prod_{s=1}^{r-1}q_{i_r i_s}}
	\end{align}
	From which we get,
	\begin{equation}
		Q_{i_r} \leq \mathbb{P}(X_{i_r} = 1 | X_{i_1} = \ldots = X_{i_{r-1}} = 1) \leq \frac{Q_{i_r}}{R_{i_r, r-1}} \leq \frac{Q_{i_r}}{R_{i_r k}} 
	\end{equation}
	Hence,
	\begin{equation}
		Q_{i_1} \ldots Q_{i_k} \leq \mathbb{E}(X_{i_1}\ldots X_{i_k}) \leq \frac{Q_{i_1}}{R_{i_1 k}} \ldots \frac{Q_{i_k}}{R_{i_k k}}
	\end{equation}
 	\begin{align}\label{eq:CVDOWN}
		&\sum_{1\leq i_1 < \ldots < i_k \leq n} Q_{i_1} \ldots Q_{i_k} = \frac{1}{k!}\sum_{1 \leq i_1 \neq \ldots \neq i_r \leq n} Q_{i_1} \ldots Q_{i_k} \geq\\
	     & \frac{1}{k!} \sum_{1\leq i_1, \ldots, i_k \leq n} Q_{i_1}\ldots Q_{i_k} 
		- \frac{k}{k!}\sum_{i=1}^n Q_i^2(\sum_{1 \leq i_1, \ldots, i_{k-2} \leq n} Q_{i_1} \ldots Q_{i_{k-2}})\\
		&\geq \frac{\lambda_n^k}{k!} - (\max_i Q_i)\lambda_n^{k-1} \longrightarrow \frac{\lambda^k}{k!}
 	\end{align}
 	Now, by definition of $R_{ik}$, we have,
 	\begin{equation}
	\sum_{i=1}^n \frac{Q_i}{R_{ik}} \geq \sum_{i=1}^n Q_i = \lambda_n
	\end{equation}
	If we suppose that $\limsup_{n\to \infty} \sum_{i=1}^n \frac{Q_i}{R_{ik}} > \lambda$, 
	this would imply that $\limsup_{n\to \infty} \sum_{k=1}^{\frac{n}{2}} \frac{1}{k!} (\sum_{i=1}^n \frac{Q_i}{R_{ik}})^k > e^{\lambda} - 1$
	which contradicts \eqref{eq:C3}.
	It follows that 
	\begin{equation}
	\lim_{n\to\infty} \sum_{i=1}^n \frac{Q_i}{R_{ik}} = \lambda
	\end{equation}
	We obtain, 
	\begin{equation}\label{eq:CVUP}
	\sum_{1 \leq i_1 < \ldots < i_k \leq n} Q_{i_1} \ldots Q_{i_k} \leq \frac{1}{k!} (\sum_{i=1}^n \frac{Q_i}{R_{ik}})^k \longrightarrow \frac{\lambda^k}{k!}
	\end{equation}
	Combining \eqref{eq:CVUP} and \eqref{eq:CVDOWN} we have $\lim_{n\to\infty} \sum_{1 \leq i_1 < \ldots <i_k \leq n} Q_{i_1} \ldots Q_{i_k} = \frac{\lambda^k}{k!}$, which is the result we wanted to prove.
\end{proof}
It is interesting to note through the following theorem, which we will not prove, that under certain conditions, a similar behaviour appears between the binomial model $\mathcal{G}_{n,p}$ and the generalised binomial $\mathcal{G}_{n, \bold{P}}$ at the connectivity threshold.
\begin{theorem}
	If \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} are satisfied, then,
	\begin{equation}
		\lim_{n\to \infty} \mathbb{P}(\mathbb{G}_{n, \bold{P}} \text{ is connected}) = e^{-\lambda}
	\end{equation}
\end{theorem}

Above we have seen a generalised binomial model which changed the probability distribution on the set of all edges, now we will describe models in which we change the edge set.
\paragraph{Bipartite random graphs}
Bipartite graphs are formally defined as $K_{n,m} = G([n] \times [m], E)$ with $E=\{\{x,y\} : x\in[n], y\in[m]\}$.
They are an appropriate way to describe many networks, for instance if you consider a graph in which you consider a graph on the set of movie actors and you draw an edge linking two actors if they have collaborated in a movie.
The resulting graph looks very convoluted, with many cliques and doesn't seem to give much information on what we might be interested for.
However, is you consider that the set of vertices is the set of actors \emph{and} the set of movies, and there is an edge only between an actor and a movie if the actor played in the movie.
This graph would give much more information on the nature of this network.
\newline
Hence, an important part of research has been studying random bipartite graph that we can naturally describe the generalised model as follows:
consider the edges matrix of probabilities $\bold{P}_n$ and $\bold{P}_m$ respectively defined for $\mathcal{G}_{n, \bold{P_n}}$ and $\mathcal{G}_{m, \bold{P_m}}$,
then the associated random bipartite graph is defined as $\mathcal{G}_{n+m, \bold{P_n}\bigotimes \bold{P_m}}$.
(TODO : ADD A FIGURE OF DIRECT SUM)
From this it is possible to obtain a relationship between $\mathcal{G}_{n,m,p}$ and $\mathcal{G}_{n+m,pI_n\bigotimes I_m}$.
This raises interest in studying the random generalised binomial model through the point of view of the generalised model.
For instance we will prove the following theorem for connectivity in $\mathcal{G}_{n,m,p}$:
\begin{theorem}
	Let $c\in \mathbb{R}$ and $p=\frac{\log(n) + c}{n}$, then the number of isolated vertices follows a Poisson of mean $2e^{-c}$.
\end{theorem}
\begin{proof}
	Let $n\geq m$, $\bold{P} = pI_n\bigotimes I_m$, then in the proof of corollary (TODO : ADD REF), we have:
	\begin{equation}
		Q_i = q^{q_i}, \text{ with } q_i = q^n \text{ if } i\leq n, .... (TODO : WRITE WITH ARRAY)
	\end{equation}
	Then, we can bound $Q$ as follows:
	\begin{equation}
		Q \leq q^{m},
	\end{equation}
	then,
	\begin{align}
		\lambda_{n+m} = (n+m)Q = (n+m)q^{n+m} 
	\end{align}
	And we have,
	\begin{equation}
		\lim_{n,m \to \infty} \lambda_{n+m} = 2e^{-c}
	\end{equation}
	The convergence of the sum also follows which concludes the proof.
\end{proof}
\section{An arbitrary degree sequence - the Newman-Watts-Strogatz model}
As seen in the previous section, an Erd\"os-R\'enyi random graph has degrees following a Poisson distribution. 
However, in real world networks are more frequently observed power law distribution (TODO: ADD REF).
Hence, the fraction of vertices of degree $k$ is $p_k = Ck^{-\beta}$, with $C$ the normalisation constant and typically most of the observed networks which can be represented by a power-law distribution have $\beta$ between 2 and 3.
We are here interested in constructing and investigating graphs following a specified degree distribution.
\newline
First of all, let's consider a finite case in which we have a sequence of degrees $d_1, d_2, \ldots, d_n$ such that $d_i = d(i)$.
We need for this definition to be correct to make sure that the sum of the degrees is an even number.
In order to build the graph, we will assign to each vertex $i$, $d_i$ "half-edges", and now connecting each of these edges will give a graph with the appropriate degree sequence.
The way these "half-edges" are paired is not unique and we will here consider that the pairings are made at random, hence, loops and multi-edges might appear.
We will investigate in the following the proportion of these loops and multi-edges and how they affect the model. 
A realistic model having often no loops or multi-edge (for instance in a social network, one is not often friend with himself or friend several times with someone else).
\newline
The approach from Newman, Strogatz and Watts makes an extensive use of generating functions defining the degree sequence, we will here only prove some simple results on the average cluster size.
In the following we will make use of generating functions in order to obtain several distributions of interest in a general setting. 
First of all, let's consider that $p_k$ is the probability that a vertex is of degree $k$, then we naturally define the associated generating function, for $x<1$:
\begin{equation}\label{eq:G0def}
	G_0(x) = \sum_k p_k x^k
\end{equation}
We may here observe that \eqref{eq:G0def} satisfies a condition of normalisation with $G_0(1) = 1$.
As in the following we will base our approach on this generating function it is interesting to note that this normalisation condition is the only we require.
Hence, it is very convenient to work with if the only knowledge we have on our random graph is the degree of each vertex, which is often the case.
For instance, if we consider that we have $n_k$ vertices of degree $k$, we naturally define $G_0$ as:
\begin{equation}
	G_0(x) = \frac{\sum_k n_k x^k}{\sum_k n_k},
\end{equation}
which satisfies the normalisation condition. 
As an example, suppose that in a network of 100 nodes, you observe that there are 20 nodes of degree 0, 10 nodes of degree 1, 30 nodes of degree 3 and 40 nodes of degree 5, then the generating function is defined as:
\begin{equation}
	G_0(x) = \frac{ 20 + 10x + 30 x^3 + 40x^5}{100}
\end{equation}
Returning to the general case, the average degree of a vertex is:
\begin{equation}
	z = \sum_k kp_k = G_0'(1),
\end{equation}
so once we can compute a generating function, we can also calculate the mean of the probability distribution it generates.
It is in fact clear from the distribution that when $G$ is a generating function of a probability distribution, then $G'(1)$ is the expectation of the probability distribution it defines.
In this section we will investigate the probability distribution of the cluster (connected component) size.
\newline
Let's remark that if we take an edge chosen at random, then it arrives at a vertex with a probability that is proportional to its degree.
Hence, such a vertex has a probability distribution on its degree proportional to $kp_k$. 
Which give that the normalised generating function of the degree of vertices that we arrive at by a randomly chosen edge is:
\begin{equation}\label{eq:kpk}
	\frac{\sum_k kp_kx^k}{\sum_k kp_k} = x\frac{G_0'(x)}{G_0'(1)}.
\end{equation}
Using the equation above, we obtain that the number of outgoing edges generated by this function has the generating function
\begin{equation}
	G_1(x) = \frac{G_0'(x)}{G_0'(1)} = \frac{1}{z}G_0'(x)
\end{equation}
which is \eqref{eq:kpk} once we have removed a power of $x$, which is the edge used to reach the vertex.
Here we may observe that if the degrees follow a Poisson distribution of parameter $z$
\footnote{This is also true for the limiting distribution in $n$ of a binomial with parameter $n$ and $z/n$},
then the moment generating function of a Poisson distribution being\footnote{It can be deduced from theorem \ref{binMGF} in appendix}
\begin{equation}
	G_0(x) = e^{z(x-1)},
\end{equation}
we obtain $G_0'(x) = zG_0(x)$ and
\begin{equation}
	G_1(x) = G_0(x).
\end{equation}
This means that a random graph following a Poisson distribution on its degree has the same distribution of outgoing edges at a vertex whether we arrived by a randomly chosen edge or if we picked a vertex at random.
This being not true in general makes that studying an Erd\"os R\'enyi graph is much easier than most of the other models.
\newline
Let's now investigate the distribution of the number of neighbours.
In order to reach neighbours, one can think that we start from a randomly chosen vertex, count the number of outgoing edges and for each vertex at the end of those edges we count the number of outgoing edges.
This gives the following distribution for the number of second neighbours of a vertex:
\begin{equation}
	\sum_k p_k(G_1(x))^k = G_0(G_1(x)).
\end{equation}
And we can now compute the expected number of second neighbours as:
\begin{equation}
	z_2 = (\frac{d}{dx} G_0(G_1(x)))_{x=1} = G_0'(1)G_1'(1).
\end{equation}
As a matter of fact, the generating function for the distribution of the number of $n$-th neighbours can be obtained as follows
\begin{equation}
	G_0(G_1^{n-1}(x))
\end{equation}
with $G_1^{n-1}$ denoting the $n-1$ composed of $G_1$.
\newline
We now define $H_1$ as the generating function for the distribution of the size of the connected component reached by an edge chosen at random.
\paragraph{No giant component}
We consider for the moment that there is no giant component with the degree distribution we investigate, removing this component growing linearly in $n$ allows us to set a normalisation condition on $H_1$ such that $H_1(1) = 1$.
Note that the following results are also formally true ( in the ring of formal power series ?? (TODO: ASK)) if there is a giant component as long as we do not consider that $H_1$ is not normalised.
Defining $q_k$ as the probability that the initial site has $k$-edges coming out, excluding the edge from which we came along, then
\begin{align}\label{eq:H1}
	H_1(x) 	&= xq_0 + xq_1H_1(x) + xq_2 (H_1(x))^2 + \ldots\\
		&= x \sum_k q_k (H_1(x))^k = xG_1(H_1(x)).
\end{align}
And similarly we obtain that if we started at a randomly chosen vertex instead of an edge, we can define $H_0$ as
\begin{equation}\label{eq:defH0}
	H_0(x) = x G_0(H_1(x)).
\end{equation}
Denoting by $s$ the expected cluster size, then,
\begin{align}\label{eq:sH0}
	s &= H_0'(1) =  \sum_k p_k H_1(1)^k + \sum_k x k p_k H_1'(1) H_1(1)^{k-1} \\
		 &= G_0(H_1(1)) + H_1'(1)G_0'(1)
\end{align}
With $H_1(1) = 1$ we have
\begin{equation}\label{eq:sSub}
	s = 1 + H_1'(1)G_0'(1).
\end{equation}
\newline
Similarly for $H_1'$ we can get from \eqref{eq:H1}
\begin{equation}\label{eq:sH1}
	H_1'(1) = G_1(H_1(1)) + G_1'(1)H_1'(1)
\end{equation}
which gives when $H_1(1) = 1$,
\begin{equation}
	H_1'(1) = \frac{1}{1- G_1'(1)}.
\end{equation}
Replacing the above in \eqref{eq:sSub} we obtain
\begin{equation}\label{eq:ExpCluster}
	s = 1 + \frac{G_0'(1)}{1-G_1'(1)} = 1 + \frac{z_1^2}{z_1 - z_2}.
\end{equation}
The above formula \eqref{eq:ExpCluster} gives the expected cluster size in the "subcritical" case $G_1'(1) < 1$, as we could have expected from the study we lead on the phase transition, it is finite.
It is interesting to note that the expression above diverges when $G_1'(1) \geq 1$ which corresponds to the result we found investigating branching processes through other methods.
We also observe the equivalent condition in the rightmost part which gives the divergence when $z_2 \geq z_1$ which translated in usual languages states that the phase transition between guaranteed extinction and possible survival happens when the expected number of neighbours at distance 2 is larger than the expected number of neighbours at distance 1.
This formulation should not be too surprising.
\newline
\paragraph{Above the phase transition}
As what we did above would not have worked in the event $G_1'(1) > 1$ we will here get an answer using the fact that an analysis of \eqref{eq:H1} gives that $H_1(1)$ is a fixed point of $G_1$.
We will denote it by $\rho_1 = G_1(\rho_1) = H_1(1)$.
Using \eqref{eq:sH1} we obtain,
\begin{equation}
	H_1'(1) = \rho_1 + H_1'(x)G_1'(\rho_1)
\end{equation}
from which we get
\begin{equation}\label{eq:HPrime}
	H_1'(1) = \frac{\rho_1}{1 - G_1'(\rho_1)}.
\end{equation}
Now using the definition of $H_0$ in \eqref{eq:defH0} we have $G_0(H_1(1)) = H_0(1)$.
Replacing that in \eqref{eq:sH0}, dividing by $H_0(1)$ and using \eqref{eq:HPrime} we have
\begin{align}
	\frac{H_0'(1)}{H_0(1)} &= 1 + \frac{H_1'(1)G_0'(\rho_1)}{H_0(1)} \\
		&= 1 +\frac{G_0'(\rho_1)H_1(1)}{H_0(1)(1-G_1'(\rho_1))}.
\end{align}


