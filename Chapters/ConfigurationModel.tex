\section{Generalized Binomial Graph}
We are now interested in a generalization of the ( binomial ) Erd\"os-R\'enyi model which was first considered by Kovalenko in \cite{Kovalenko71}.
In this model, often referred to as \emph{inhomogeneous}, the probability of appearance of the edge $(i,j)$, that we call $p_{ij}$, is not necessarily the same for all pairs of vertices $i,j$.
\newline
From this definition it is very natural to write those edge probabilities in an $n\times n$ matrix, which we denote by
\begin{equation}
	\bold{P} = [p_{ij}]_{1\leq i,j \leq n}
\end{equation}
As previously we consider graphs without loops, so $p_{ii} = 0$ for all $i \in V$, we also consider that the graph is not directed so the probability that $i$ connects to $j$ has to be the same as the probability that $j$ connects to $i$ (i.e. $p_{ij} = p_{ji}$), hence $\bold{P}$ is symmetric.
In order to shorten the writing, we put $q_{ij} = 1 - p_{ij}$.
\newline
We denote the probability space of this generalised model as $\mathcal{G}_{n,\bold{P}}$ and as before we denote a random variable in this space as $\mathbb{G}_{n,\bold{P}}$.
\newline
We now define
\begin{equation}
	Q_i = \prod_{j=1}^n q_{ij}, \quad \lambda_n = \sum_{i=1}^nQ_i
\end{equation}
and we observe that $Q_i$ is the probability that vertex $i$ is isolated, hence $\lambda_n$ is the expected number of isolated vertices.
\newline
We also need to define
\begin{equation}
	R_{ik} = \min_{1\leq j_1<j_2< \ldots <j_k\leq n} q_{ij_1}q_{ij_2}\ldots q_{ij_k}
\end{equation}
In the following, we suppose that the edge probabilities $p_{ij}$ are chosen in such a way that the following equations are simultaneously satisfied as $n\to \infty$.
\begin{align}
	\max_{1\leq i \leq n} Q_i &\to 0 \label{eq:C1}\\
	\lim_{n \to \infty} \lambda_n &= \lambda = \text{constant} \label{eq:C2} \\
	\lim_{n\to \infty} \sum_{k=1}^{\lfloor n/2 \rfloor} \frac{1}{k!}(\sum_{i=1}^n \frac{Q_i}{R_{ik}})^k &= e^{\lambda} - 1 \label{eq:C3}
\end{align}
In this section we are interested in proving the following theorem.
\begin{theorem}\label{th:CVIsolPoi}
	Let $X_0$ denote the number of isolated vertices in $\mathbb{G}_{n, \bold{P}}$. If \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} are satisfied, 
	then the number of isolated vertices is asymptotically Poisson distributed with mean $\lambda$.
	\newline
	i.e. for $k \in \mathbb{N}$, 
	\begin{equation}
		\lim_{n \to \infty} \mathbb{P}(X_0 = k) = \frac{\lambda^k}{k!}e^{-\lambda}
	\end{equation}
\end{theorem}
Let's first show as a corollary that this theorem allows us to conclude the proof of \ref{th:connect}.
\begin{corollary}\label{th:proofPoissonP}
	If $p(n) = \frac{\log n + c}{n}$, then \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} are satisfied, 
	and the number of isolated vertices is asymptotically Poisson distributed with mean $e^{-c}$.
\end{corollary}
\begin{proof}[Proof of Corollary \ref{th:proofPoissonP}]
	We observe that $\mathcal{G}_{n,p}$ and $\mathcal{G}_{n, \bold{P}}$ when $p_{ij} = p$ for all $i \neq j$.
	\newline
	We observe that $Q_i = Q = q^{n-1}$ (and \eqref{eq:C1} is obviously satisfied) and $R_{ik} = R_k = q^k$. 
	From this we get
	\begin{equation}
		\lambda_n = n Q = nq^{n-1} = n(1-p)^{n-1}
	\end{equation}
	and we have $\lim \lambda_n = e^{-c} = \lambda$, so \eqref{eq:C2} is also satisfied.
	\newline
	Let's observe that it is enough to show $\sum_{i=1}^n \frac{Q_i}{R_{ik}}$ converges uniformly to $\lambda = e^{-c}$ for \eqref{eq:C3} to be satisfied.
	\begin{equation}
		\sum_{i=1}^n \frac{Q_i}{R_{ik}} = \sum_{i=1}^n \frac{Q}{R_k} = n \frac{Q}{R_k} = nq^{n-1-k}
	\end{equation}
	And $\lim_{n \to \infty}  nq^{n-1-k} = e^{-c}$ finally proves the corollary.
\end{proof}
\begin{proof}[Proof of Theorem \ref{th:CVIsolPoi}]
	Let $Y_{ij}$ a random variable following a Bernoulli of parameter $p_{ij}$.
	\footnote{ Recall that a Bernoulli of parameter $p$ is 1 with prob. $p$ and 0 with prob. $1-p$.}
	We denote by $Y_i$ the indicator of the event that vertex $i$ is isolated, i.e. $Y_i = \mathbbm{1}_{\sum_{j=1}^n Y_{ij} = 0}$.
	In order to show the convergence of $X_0$ in distribution to the Poisson random variable we will use the method of factorial moments.
	So, we want to show that for any natural number $k$ we have
	\begin{equation}\label{eq:EIsolMom}
		\mathbb{E}(\sum_{1\leq i_1<i_2< \ldots < i_k \leq n} X_{i_1}X_{i_2}\ldots X_{i_k}) \to_{n \to \infty} \frac{\lambda^k}{k!}
	\end{equation}
	Let's observe that the LHS of \eqref{eq:EIsolMom} is the sum of $\mathbb{E}(X_{i_1}X_{i_2}\ldots X_{i_k})$ over all $i_1 < i_2 < \ldots i_k$.
	\begin{align}
		\mathbb{E}(X_{i_1}X_{i_2}\ldots X_{i_k}) 	&= \prod_{r=1}^k \mathbb{P}(X_{i_r} = 1 | X_{i_1} = \ldots = X_{i_{r-1}} = 1) \\
							&= \prod_{r=1}^k\frac{ \prod_{j=1}^n q_{i_r j} }{ \prod_{s=1}^{r-1}q_{i_r i_s}}
	\end{align}
	From which we get,
	\begin{equation}
		Q_{i_r} \leq \mathbb{P}(X_{i_r} = 1 | X_{i_1} = \ldots = X_{i_{r-1}} = 1) \leq \frac{Q_{i_r}}{R_{i_r, r-1}} \leq \frac{Q_{i_r}}{R_{i_r k}} 
	\end{equation}
	Hence,
	\begin{equation}
		Q_{i_1} \ldots Q_{i_k} \leq \mathbb{E}(X_{i_1}\ldots X_{i_k}) \leq \frac{Q_{i_1}}{R_{i_1 k}} \ldots \frac{Q_{i_k}}{R_{i_k k}}
	\end{equation}
 	\begin{align}\label{eq:CVDOWN}
		&\sum_{1\leq i_1 < \ldots < i_k \leq n} Q_{i_1} \ldots Q_{i_k} = \frac{1}{k!}\sum_{1 \leq i_1 \neq \ldots \neq i_r \leq n} Q_{i_1} \ldots Q_{i_k} \geq\\
	     & \frac{1}{k!} \sum_{1\leq i_1, \ldots, i_k \leq n} Q_{i_1}\ldots Q_{i_k} 
		- \frac{k}{k!}\sum_{i=1}^n Q_i^2(\sum_{1 \leq i_1, \ldots, i_{k-2} \leq n} Q_{i_1} \ldots Q_{i_{k-2}})\\
		&\geq \frac{\lambda_n^k}{k!} - (\max_i Q_i)\lambda_n^{k-1} \longrightarrow \frac{\lambda^k}{k!}
 	\end{align}
 	Now, by definition of $R_{ik}$, we have,
 	\begin{equation}
	\sum_{i=1}^n \frac{Q_i}{R_{ik}} \geq \sum_{i=1}^n Q_i = \lambda_n
	\end{equation}
	If we suppose that $\limsup_{n\to \infty} \sum_{i=1}^n \frac{Q_i}{R_{ik}} > \lambda$, 
	this would imply that
	\begin{equation}
		\limsup_{n\to \infty} \sum_{k=1}^{\floor \frac{n}{2}\rfloor} \frac{1}{k!} (\sum_{i=1}^n \frac{Q_i}{R_{ik}})^k > e^{\lambda} - 1
	\end{equation}
	which contradicts \eqref{eq:C3}.
	It follows that 
	\begin{equation}
		\lim_{n\to\infty} \sum_{i=1}^n \frac{Q_i}{R_{ik}} = \lambda
	\end{equation}
	We obtain, 
	\begin{equation}\label{eq:CVUP}
	\sum_{1 \leq i_1 < \ldots < i_k \leq n} Q_{i_1} \ldots Q_{i_k} \leq \frac{1}{k!} (\sum_{i=1}^n \frac{Q_i}{R_{ik}})^k \longrightarrow \frac{\lambda^k}{k!}
	\end{equation}
	Combining \eqref{eq:CVUP} and \eqref{eq:CVDOWN} we have $\lim_{n\to\infty} \sum_{1 \leq i_1 < \ldots <i_k \leq n} Q_{i_1} \ldots Q_{i_k} = \frac{\lambda^k}{k!}$, which is the result we wanted to prove.
\end{proof}
It is interesting to note through the following theorem, which we will not prove, that under certain conditions, a similar behaviour appears between the binomial model $\mathcal{G}_{n,p}$ and the generalised binomial $\mathcal{G}_{n, \bold{P}}$ at the connectivity threshold.
\begin{theorem}
	If \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} are satisfied, then,
	\begin{equation}
		\lim_{n\to \infty} \mathbb{P}(\mathbb{G}_{n, \bold{P}} \text{ is connected}) = e^{-\lambda}
	\end{equation}
\end{theorem}

Above we have seen a generalised binomial model which changed the probability distribution on the set of all edges, now we will describe models in which we change the edge set.
\paragraph{Bipartite random graphs}
Complete Bipartite graphs are formally defined as $K_{n,m} = G([n+m], E)$ with $E=\{\{x,y\} : x\in[n], y\in n+[m]\}$.
In the above we have studied graphs as subgraphs of the complete graph, now we restrict ourselves to graphs which are subgraphs of the complete bipartite graph.
They are an appropriate way to describe many networks, for instance if you consider a graph in which you consider a graph on the set of movie actors and you draw an edge linking two actors if they have collaborated in a movie.
The resulting graph looks very convoluted, with many cliques and doesn't seem to give much information on what we might be interested for.
However, if you consider that the set of vertices is the set of actors \emph{and} the set of movies, and there is an edge only between an actor and a movie if the actor played in the movie.
This graph would give much more information on the nature of this network.
\newline
Hence, an important part of research has been studying random bipartite graph and we can very naturally define $\mathcal{G}_{n,n, p}$ with the generalised binomial model.
Indeed, a $(n,n)$-bipartite graph represented through its adjacency matrix has the following block representation:
\begin{equation}
    \left[
\begin{array}{c|c}
        0 & A \\
         \hline
        A & 0
        \end{array}
\right]
\end{equation}
with $A$ a $n$ by $n$ matrix containing only zeros and ones.
Hence, if we consider $1_n$ the $n$ by $n$ matrix made only of 1, then we have $\mathcal{G}_{n,n,p} = \mathcal{G}_{2n,\bold{p}}$ with
\begin{equation}\label{eq:pMat}
    \bold{p} = p\left[
\begin{array}{c|c}
        0 & 1_n \\
         \hline
        1_n & 0
        \end{array}
\right]
\end{equation}
This raises interest in studying random bipartite graphs through the point of view of the generalised model.
For instance we will prove the following theorem for connectivity in $\mathcal{G}_{n,n,p}$:
\begin{theorem}
	Let $c\in \mathbb{R}$ and $p=\frac{\log(n) + c}{n}$, then the number of isolated vertices of $\mathcal{G}_{n,n,p}$  follows a Poisson of mean $2e^{-c}$ for $n$ large enough.
\end{theorem}
\begin{proof}
	Let $\bold{p}$ as in \ref{eq:pMat}, and let's show that the conditions \eqref{eq:C1}, \eqref{eq:C2}, \eqref{eq:C3} of Theorem \ref{th:CVIsolPoi} are satisfied. 
	\begin{equation}
		Q_i = Q = q^{n}
	\end{equation}
	And,
	\begin{equation}
		\lambda_n = 2n q^{n}= 2n(1-p)^n \longrightarrow 2 e^{-c},
	\end{equation}
	and noticing 
	\begin{align}
		R_{ik} = R_k = \left\{	\begin{array}{rl}
						q^k & \text{if } k\leq n,\\
						q^{k-n} & \text{if } k > n.
					 \end{array}
				\right.
	\end{align}
	And as the sum in \eqref{eq:C3} only goes to $n$ in the sum we evaluate we have $R_k = q^k$, and as in the corollary we simply evaluate the following:
	\begin{equation}
		\sum_{i=1}^{2n} \frac{Q}{R_k} =2n q^{n-k} \longrightarrow_{n\to\infty} 2e^{-c} = \lambda.
	\end{equation}
	Hence as a consequence of Theorem \ref{th:CVIsolPoi}, the number of isolated vertices follows a Poisson of mean $2e^{-c}$.
\end{proof}
As shown in Bollobas 2001 \cite{Bollob01}, there is the same hitting time for connectivity and $\delta \geq 1$ with high probability, as a consequence of the previous Theorem one could prove that the probability of connectivity of $\mathcal{G}_{n,n, p}$ is $e^{-2e^{-c}}$. 

\section{An arbitrary degree sequence}
As seen in the previous section, an Erd\"os-R\'enyi random graph has degrees following a Poisson distribution. 
However, in practice networks usually have a degree sequence following a power-law, these are usually referred to as scale free networks.
Recent interest in scale free models started with \cite{Barabasi99} by Barabasi and Albert in which they exhibited that the world wide web and several social networks were scale free.
Hence, the fraction of vertices of degree $k$ is $p_k = Ck^{-\beta}$, with $C$ the normalisation constant and typically most of the observed networks which can be represented by a power-law distribution have $\beta$ between 2 and 3.
We are here interested in constructing and investigating graphs following a specified degree distribution.
\newline
First of all, let's consider a finite case in which we have a sequence of degrees $d_1, d_2, \ldots, d_n$ such that $d_i = d(i)$.
We need for this definition to be correct to make sure that the sum of the degrees is an even number.
In order to build the graph, we will assign to each vertex $i$, $d_i$ "half-edges", and now connecting each of these edges will give a graph with the appropriate degree sequence.
The way these "half-edges" are paired is not unique and we will here consider that the pairings are made at random, hence, loops and multi-edges might appear.
We will investigate in the following the proportion of these loops and multi-edges and how they affect the model. 
A realistic model having often no loops or multi-edge (for instance in a social network, one is not often friend with himself or friend several times with someone else).
Here we are not interested in the way such graphs are constructed, see the following works cited in which there is a formal construction of the model, however it is important to note that it is possible to prove that there is a positive probability of obtaining a simple graph using this construction.
The fact that the probability is positive ensures that results which are true with high probability are also true with high probability on simple graphs which follow the specified degree sequence.
\newline
The approach from Newman, Strogatz and Watts \cite{Newman01} makes an extensive use of generating functions defining the degree sequence, we will here only prove some simple results on the average cluster size. We may refer the reader to \cite{Hofstad16} and \cite{Durrett06} for a similar approach as Newman, Watts and Strogatz
or to \cite{Molloy95} and\cite{frieze15} for a more probabilistic approach. 
In the following we will make use of generating functions in order to obtain several distributions of interest in a general setting. 
First of all, let's consider that $p_k$ is the probability that a vertex is of degree $k$, then we naturally define the associated generating function
\footnote{See \cite{Wilf06} for a study of generating functions and their use in discrete mathematics.}
, for $|x|<1$:
\begin{equation}\label{eq:G0def}
	G_0(x) = \sum_k p_k x^k
\end{equation}
We may here observe that \eqref{eq:G0def} satisfies a condition of normalisation with $G_0(1) = 1$.
As in the following we will base our approach on this generating function it is interesting to note that this normalisation condition is the only we require.
Hence, it is very convenient to work with if the only knowledge we have on our random graph is the degree of each vertex, which is often the case.
For instance, if we consider that we have $n_k$ vertices of degree $k$, we naturally define $G_0$ as:
\begin{equation}
	G_0(x) = \frac{\sum_k n_k x^k}{\sum_k n_k},
\end{equation}
which satisfies the normalisation condition. 
As an example, suppose that in a network of 100 nodes, you observe that there are 20 nodes of degree 0, 10 nodes of degree 1, 30 nodes of degree 3 and 40 nodes of degree 5, then the generating function is defined as:
\begin{equation}
	G_0(x) = \frac{ 20 + 10x + 30 x^3 + 40x^5}{100}
\end{equation}
Returning to the general case, the average degree of a vertex is:
\begin{equation}
	z = \sum_k kp_k = G_0'(1),
\end{equation}
so once we can compute a generating function, we can also calculate the mean of the probability distribution it generates.
It is in fact clear from the distribution that when $G$ is a generating function of a probability distribution, then $G'(1)$ is the expectation of the probability distribution it defines.
In this section we will investigate the probability distribution of the cluster (connected component) size.
\newline
Let's remark that if we take an edge chosen at random, then it arrives at a vertex with a probability that is proportional to its degree.
Hence, such a vertex has a probability distribution on its degree proportional to $kp_k$. 
Which give that the normalised generating function of the degree of vertices that we arrive at by a randomly chosen edge is:
\begin{equation}\label{eq:kpk}
	\frac{\sum_k kp_kx^k}{\sum_k kp_k} = x\frac{G_0'(x)}{G_0'(1)}.
\end{equation}
Using the equation above, we obtain that the number of outgoing edges generated by this function has the generating function
\begin{equation}
	G_1(x) = \frac{G_0'(x)}{G_0'(1)} = \frac{1}{z}G_0'(x)
\end{equation}
which is \eqref{eq:kpk} once we have removed a power of $x$, which is the edge used to reach the vertex.
Here we may observe that if the degrees follow a Poisson distribution of parameter $z$
\footnote{This is also true for the limiting distribution in $n$ of a binomial with parameter $n$ and $z/n$},
then the moment generating function of a Poisson distribution being\footnote{It can be deduced from theorem \ref{binMGF} in appendix}
\begin{equation}
	G_0(x) = e^{z(x-1)},
\end{equation}
we obtain $G_0'(x) = zG_0(x)$ and
\begin{equation}\label{eq:PoiGen}
	G_1(x) = G_0(x).
\end{equation}
This means that a random graph following a Poisson distribution on its degree has the same distribution of outgoing edges at a vertex whether we arrived by a randomly chosen edge or if we picked a vertex at random.
This being not true in general makes that studying an Erd\"os R\'enyi graph is much easier than most of the other models.
\newline
Let's now investigate the distribution of the number of neighbours.
In order to reach neighbours, one can think that we start from a randomly chosen vertex, count the number of outgoing edges and for each vertex at the end of those edges we count the number of outgoing edges.
This gives the following distribution for the number of second neighbours of a vertex:
\begin{equation}
	\sum_k p_k(G_1(x))^k = G_0(G_1(x)).
\end{equation}
And we can now compute the expected number of second neighbours as:
\begin{equation}
	z_2 = (\frac{d}{dx} G_0(G_1(x)))_{x=1} = G_0'(1)G_1'(1).
\end{equation}
As a matter of fact, the generating function for the distribution of the number of $n$-th neighbours can be obtained as follows
\begin{equation}
	G_0(G_1^{n-1}(x))
\end{equation}
with $G_1^{n-1}$ denoting the $n-1$ composed of $G_1$.
\newline
We now define $H_1$ as the generating function for the distribution of the size of the connected component reached by an edge chosen at random.
\paragraph{No giant component}
We consider for the moment that there is no giant component with the degree distribution we investigate, removing this component growing linearly in $n$ allows us to set a normalisation condition on $H_1$ such that $H_1(1) = 1$.
Note that the following results are also formally true if there is a giant component as long as we do not consider that $H_1$ is not normalised.
Defining $q_k$ as the probability that the initial site has $k$-edges coming out, excluding the edge from which we came along, then
\begin{align}\label{eq:H1}
	H_1(x) 	&= xq_0 + xq_1H_1(x) + xq_2 (H_1(x))^2 + \ldots\\
		&= x \sum_k q_k (H_1(x))^k = xG_1(H_1(x)).
\end{align}
And similarly we obtain that if we started at a randomly chosen vertex instead of an edge, we can define $H_0$ as
\begin{equation}\label{eq:defH0}
	H_0(x) = x G_0(H_1(x)).
\end{equation}
Denoting by $s$ the expected cluster size, then,
\begin{align}\label{eq:sH0}
	s &= H_0'(1) =  \sum_k p_k H_1(1)^k + \sum_k x k p_k H_1'(1) H_1(1)^{k-1} \\
		 &= G_0(H_1(1)) + H_1'(1)G_0'(1)
\end{align}
With $H_1(1) = 1$ we have
\begin{equation}\label{eq:sSub}
	s = 1 + H_1'(1)G_0'(1).
\end{equation}
\newline
Similarly for $H_1'$ we can get from \eqref{eq:H1}
\begin{equation}\label{eq:sH1}
	H_1'(1) = G_1(H_1(1)) + G_1'(1)H_1'(1)
\end{equation}
which gives when $H_1(1) = 1$,
\begin{equation}
	H_1'(1) = \frac{1}{1- G_1'(1)}.
\end{equation}
Replacing the above in \eqref{eq:sSub} we obtain
\begin{equation}\label{eq:ExpCluster}
	s = 1 + \frac{G_0'(1)}{1-G_1'(1)} = 1 + \frac{z_1^2}{z_1 - z_2}.
\end{equation}
The above formula \eqref{eq:ExpCluster} gives the expected cluster size in the "subcritical" case $G_1'(1) < 1$, as we could have expected from the study we lead on the phase transition, it is finite.
It is interesting to note that the expression above diverges when $G_1'(1) \geq 1$ which corresponds to the result we found investigating branching processes through other methods.
We also observe the equivalent condition in the rightmost part which gives the divergence when $z_2 \geq z_1$ which translated in usual languages states that the phase transition between guaranteed extinction and possible survival happens when the expected number of neighbours at distance 2 is larger than the expected number of neighbours at distance 1.
This formulation should not be too surprising.
\newline
Now that we have defined those generating functions and variables, we main state the theorem from this section which includes the cases treated above without a giant component and the event of the existence of a giant component.
\begin{theorem}\label{th:NWS}
	\begin{enumerate}
		\item  $G_1'(1) < 1$:
			\begin{itemize}
				\item	There is with high probability no giant component.
				\item	The average cluster size is : $H_0'(1) = 1 + \frac{G_0'(1)}{1 - G_1'(1)}$
			\end{itemize}
		\item	$G_1'(1) > 1$:
			\begin{itemize}
				\item	There is with high probability a giant component.
				\item	The fraction of vertices in the giant component is asymptotically: $1-G_0(\rho_1)$, with $\rho_1$ the smallest fixed point of $G_1$,
				\item	i.e. $\frac{C_{max}}{n} \longrightarrow 1 - G_0(\rho_1)$.
			\end{itemize}
	\end{enumerate}
\end{theorem}
\paragraph{Above the phase transition}
We consider the two steps branching process starting from a half edge which has a probability distribution defined by $G_1$.
Using Theorem \ref{th:ProbExtinction} on general branching processes, we know that the probability of extinction starting from an edge is the smallest fixed point of $G_1$ which we denote by $\rho_1$.
So the probability that $k$ edges will be extinct if we wait long enough is $\rho_1^k$ as each branching process is independent from the others.
From which we get the following formula on the probability of survival:
\begin{equation}
	\sum_k p_k(1-\rho_1^k) 
\end{equation}
which is equal to
\begin{equation}
	1 - G_0(\rho_1).
\end{equation}
The probability of survival being asymptotically equivalent to the ratio of infinite components compared to the whole vertex set we can write
\begin{equation}
	\frac{C_{max}}{n} \longrightarrow 1 - G_0(\rho_1).
\end{equation}
Hence, $C_{max}$ is growing linearly which means that $C_{max}$ is a giant component as long as $G_0(\rho_1)$ is different from 1.
One may observe that what we proved here doesn't depend on the value of $G_1'(1)$, however in that case we would have that $\rho_1 = 1$ and then using the fact that $G_0(1) = 1$ we have that 
\begin{equation}
	\frac{C_{max}}{n} \longrightarrow 0, \quad \text{if } G_1'(1) < 1
\end{equation}
which concludes the proof of \ref{th:NWS}.

\paragraph{The Poisson Case}
As seen above if the degree sequence follows a Poisson distribution as it is the case in the Erd\"os-R\'enyi model, as observed in \eqref{eq:PoiGen} then $G_0 = G_1$, hence they have the same fixed points.
From this observation and considering that $np \longrightarrow \lambda$, we can apply Theorem \ref{th:NWS}.
So we obtain that the average cluster size if $\lambda = 1-\epsilon$ for any $\epsilon >0$ in $\mathcal{G}_{n,\frac{1-\epsilon}{n}}$ is
\begin{equation}
	1 + \frac{\lambda}{1-\lambda} = \frac{1}{\epsilon}.
\end{equation}
In the event $\lambda > 1$ and $\rho_1$ is the solution of the equation $x=e^{\lambda(x-1)}$ then we have
\begin{equation}
	C_{max} \longrightarrow (1-\rho_1)n.
\end{equation}


